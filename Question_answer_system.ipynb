{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_answer_system.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fdcd71058314a328f6c7da876f8b1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd5cd681ee1940628a45d8a2e4689277",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93389c64f4534d08b464fa85efd48202",
              "IPY_MODEL_9e55cd4b4d734ee1b319931737f8345f",
              "IPY_MODEL_3457f80099c44a63a037a93b375c896d"
            ]
          }
        },
        "bd5cd681ee1940628a45d8a2e4689277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93389c64f4534d08b464fa85efd48202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0583d1fa83e4a16be64dbb9342773d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0879d8f923f418c864b5451fdaec4c4"
          }
        },
        "9e55cd4b4d734ee1b319931737f8345f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d493fde6764459dab0f405734dc8112",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 384,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 384,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f99ecc86e0a44c7aa064eccd0cb4ec10"
          }
        },
        "3457f80099c44a63a037a93b375c896d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c01fd139b45c46569db5199ccc9b38d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 384/384 [00:00&lt;00:00, 12.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df56001cd51940a8b2ba8fb54690c6a6"
          }
        },
        "b0583d1fa83e4a16be64dbb9342773d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0879d8f923f418c864b5451fdaec4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d493fde6764459dab0f405734dc8112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f99ecc86e0a44c7aa064eccd0cb4ec10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c01fd139b45c46569db5199ccc9b38d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df56001cd51940a8b2ba8fb54690c6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "719a22c2df2e4545ad69a839bce176eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c45cdc6c5ca24092ace5adb7ab2a7716",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48ea9d815370499eb4ace34150ccce2b",
              "IPY_MODEL_5e7361d8a7d84d709874b34b1fa9b837",
              "IPY_MODEL_487b6dea9d3d47e0851f09cbdbde5e94"
            ]
          }
        },
        "c45cdc6c5ca24092ace5adb7ab2a7716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48ea9d815370499eb4ace34150ccce2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6950bad1e5a6427796167e94f8511503",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52040de888534c26afdbd6ad7361447f"
          }
        },
        "5e7361d8a7d84d709874b34b1fa9b837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05ca204097ff4e029e991ace803630e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_156d1f106b26493a900e7411398c135d"
          }
        },
        "487b6dea9d3d47e0851f09cbdbde5e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92b63afbaee3488e828669f0761ee344",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.86MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a825a1b788f4e12bfd2bb8193c7f4c8"
          }
        },
        "6950bad1e5a6427796167e94f8511503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52040de888534c26afdbd6ad7361447f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05ca204097ff4e029e991ace803630e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "156d1f106b26493a900e7411398c135d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92b63afbaee3488e828669f0761ee344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a825a1b788f4e12bfd2bb8193c7f4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43c70d14fb8546d8ad4d595862b108b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f39c3ebcc4948ea8dab2f707763911d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2189b9ad8163417fa0dcdb573df04c97",
              "IPY_MODEL_f90459e368414f1cbcd723daf885ded1",
              "IPY_MODEL_b7005af3f1d64329be500cf07a1d0fc7"
            ]
          }
        },
        "5f39c3ebcc4948ea8dab2f707763911d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2189b9ad8163417fa0dcdb573df04c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ffb71a4a90e044e388283d32e1bab8b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd9bac43833b46478caade2fab4d7f33"
          }
        },
        "f90459e368414f1cbcd723daf885ded1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6a31aa70b1a4d70bdf15410f4102513",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 327051810,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 327051810,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e7ae04312354d8c9c400a6d415ab9e1"
          }
        },
        "b7005af3f1d64329be500cf07a1d0fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d82e91ac7df416fbf529d48590677c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 327M/327M [00:11&lt;00:00, 36.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ecf4f52fff36452a8d9ecf4381ab3c48"
          }
        },
        "ffb71a4a90e044e388283d32e1bab8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd9bac43833b46478caade2fab4d7f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6a31aa70b1a4d70bdf15410f4102513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e7ae04312354d8c9c400a6d415ab9e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d82e91ac7df416fbf529d48590677c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ecf4f52fff36452a8d9ecf4381ab3c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55034a1da1ee40ec8bf6dfafe242a081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0dcfdf0f39854a7dab73a8bfa7001ed1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0879349f65644185a9b5d43b2a4a2b4a",
              "IPY_MODEL_c30a9eb5ebbc409ebaa088b09f2ada99",
              "IPY_MODEL_f550d454136344f39819617661d8fada"
            ]
          }
        },
        "0dcfdf0f39854a7dab73a8bfa7001ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0879349f65644185a9b5d43b2a4a2b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_870fbbcfc27d46da9358a8d4b34ab861",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97e51ebeec5149dd8f2de8d2f787f171"
          }
        },
        "c30a9eb5ebbc409ebaa088b09f2ada99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01c8f3f9208e45888e93070e6849575c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2d38c2a264a4affbb860676af871b3d"
          }
        },
        "f550d454136344f39819617661d8fada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0fb4164e98bf444aa65a67fb2049efb0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26.0/26.0 [00:00&lt;00:00, 793B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_530fd9c6ca8143b0ad858d82a5a62552"
          }
        },
        "870fbbcfc27d46da9358a8d4b34ab861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97e51ebeec5149dd8f2de8d2f787f171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01c8f3f9208e45888e93070e6849575c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2d38c2a264a4affbb860676af871b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fb4164e98bf444aa65a67fb2049efb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "530fd9c6ca8143b0ad858d82a5a62552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85377cd8f3cb465e8ef05a5a0d019df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1152638e41884d3a81ff8d244157e2b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da701940089841429114e220e18330a6",
              "IPY_MODEL_6534b4cf4302415d83311e914252a360",
              "IPY_MODEL_5135c8397ee54f36839d5e320be7b8fc"
            ]
          }
        },
        "1152638e41884d3a81ff8d244157e2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da701940089841429114e220e18330a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_669a132ed4fe46fa8863a9180fed635c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cdd002c61c9485ebe165d97bfdc544a"
          }
        },
        "6534b4cf4302415d83311e914252a360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef62e63548024d6c9826bc7192543d8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1600,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1600,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2446f93867d2467fa879bd75946b406c"
          }
        },
        "5135c8397ee54f36839d5e320be7b8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96a42b3d91a642e09d86c89d75b6d19a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.60k/1.60k [00:00&lt;00:00, 53.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b2ce628d9034eb3b6848b5e387d90b9"
          }
        },
        "669a132ed4fe46fa8863a9180fed635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cdd002c61c9485ebe165d97bfdc544a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef62e63548024d6c9826bc7192543d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2446f93867d2467fa879bd75946b406c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96a42b3d91a642e09d86c89d75b6d19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b2ce628d9034eb3b6848b5e387d90b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54256f9f1b164d4f882680651aaa59ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a72f265e30d54f2284b93956c2be0ba0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f91f299390f45c8b1cd910574d7af49",
              "IPY_MODEL_2fc33c228b614fc5b7f6447128d04c8d",
              "IPY_MODEL_a3613e24a7764ea591a500615f5aeb32"
            ]
          }
        },
        "a72f265e30d54f2284b93956c2be0ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f91f299390f45c8b1cd910574d7af49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e61ca02144642e0bce2a071df631503",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b55f51977f2443188a57c5cd7751ae2a"
          }
        },
        "2fc33c228b614fc5b7f6447128d04c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_be65781737bf40e68703d9146242a4dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f28470f80354d3d8ed255d608a41ade"
          }
        },
        "a3613e24a7764ea591a500615f5aeb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c284aa157f814c97b8ed244207f77d45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.99MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78e545b0aa0f4a1ca4f4e462d70da5ef"
          }
        },
        "1e61ca02144642e0bce2a071df631503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b55f51977f2443188a57c5cd7751ae2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be65781737bf40e68703d9146242a4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f28470f80354d3d8ed255d608a41ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c284aa157f814c97b8ed244207f77d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78e545b0aa0f4a1ca4f4e462d70da5ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "395781d292634fa38e67772c5485a3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6628d2a0ad3047f8b6c3088c3a04cf86",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e556c25d663424284888abbc05002cb",
              "IPY_MODEL_c57d7861a26a41329dcc557c2033c9b8",
              "IPY_MODEL_d627189a6b2945dab1fda1f42981b417"
            ]
          }
        },
        "6628d2a0ad3047f8b6c3088c3a04cf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e556c25d663424284888abbc05002cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7e5d8a84fe647b8bf6c5673a4623674",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a952fc453489496bb6ee450e2abacf3e"
          }
        },
        "c57d7861a26a41329dcc557c2033c9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8072c8f2682443d692930a124e6c17eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1b0e6ed0a4f4478b51125171eb006e4"
          }
        },
        "d627189a6b2945dab1fda1f42981b417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_966305a134b44cb695f9ebe69ea11780",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 2.90MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cbda97731ce34b47af3f8cbeb842f41c"
          }
        },
        "c7e5d8a84fe647b8bf6c5673a4623674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a952fc453489496bb6ee450e2abacf3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8072c8f2682443d692930a124e6c17eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1b0e6ed0a4f4478b51125171eb006e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "966305a134b44cb695f9ebe69ea11780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cbda97731ce34b47af3f8cbeb842f41c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2549c554948340c08b2f3716678ec2e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84883d04f64a48cb99989046a8b8368f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5bf14c40a47463c8e190167b23bef41",
              "IPY_MODEL_6c2fbcf402d947bba2738e22ae8953fa",
              "IPY_MODEL_a4f4679f0b03403d9f1a12f0bb22282b"
            ]
          }
        },
        "84883d04f64a48cb99989046a8b8368f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5bf14c40a47463c8e190167b23bef41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_954ac031e75d437a88da1f5e676b0810",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_704aba92165249edb38754ea25aa7653"
          }
        },
        "6c2fbcf402d947bba2738e22ae8953fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7fba7cb2843c4b64bdcae32e2e78614f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_153bbe985822450fb48c233d47ac5be1"
          }
        },
        "a4f4679f0b03403d9f1a12f0bb22282b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e59cbd57e11843dd9154d1a53eee1c91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.63MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e148ca1a345e4adb89467129068242ed"
          }
        },
        "954ac031e75d437a88da1f5e676b0810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "704aba92165249edb38754ea25aa7653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fba7cb2843c4b64bdcae32e2e78614f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "153bbe985822450fb48c233d47ac5be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e59cbd57e11843dd9154d1a53eee1c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e148ca1a345e4adb89467129068242ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b3fce1413214b3cabcda7ddca71103f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dc20e5b2ea874ca5bfd40a0791b2d478",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e253f1bc04f1431687194a0351515240",
              "IPY_MODEL_9a4eca95103d4ce3a95723622a467d23",
              "IPY_MODEL_9a0844f236ba4895841e27f2dd101072"
            ]
          }
        },
        "dc20e5b2ea874ca5bfd40a0791b2d478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e253f1bc04f1431687194a0351515240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e870539c5b645e588f0dd16c5f4bb64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b278a1f09ce6403bab2b1edf0072db58"
          }
        },
        "9a4eca95103d4ce3a95723622a467d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3fe44030c170457bbf7eadae1e4ac713",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1018571383,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1018571383,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a0cb712e92d430ba62cd7b9c151ed44"
          }
        },
        "9a0844f236ba4895841e27f2dd101072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b841c76859b4a3eb19cd8e3e2edc70c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.02G/1.02G [00:26&lt;00:00, 36.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ef580c2a6c746eeb60028b6e205fce3"
          }
        },
        "7e870539c5b645e588f0dd16c5f4bb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b278a1f09ce6403bab2b1edf0072db58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fe44030c170457bbf7eadae1e4ac713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a0cb712e92d430ba62cd7b9c151ed44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b841c76859b4a3eb19cd8e3e2edc70c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ef580c2a6c746eeb60028b6e205fce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ6oIhPQExDl"
      },
      "source": [
        "Industries are increasingly turning to automated chat assistants to handle customer support interactions. But, these tools can only successfully answer questions they were trained on, exposing a growing challenge for enterprise question answering (QA) techniques today. To address this, we are trying to build an intelligent question answering system that doesn’t just return documents related to the question, but extracts relevant information within the documents and puts forth the detailed answer, like one that a human would have come up with. \n",
        "\n",
        "In this notebook, we implement a domain-specific question-answering engine that answers any questions related to PyTorch. This model benefits students and working professionals who are beginners in PyTorch. The model is developed with datasets collected from Stack Overflow, PyTorch Github issues, PyTorch documentation, PyTorch discussion forum, and Youtube videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqWP9ITEwE4"
      },
      "source": [
        "### 1- Preliminaries\n",
        "The implementation presented here relies on the Hugging Face 🤗transformers and 🤗nlp libraries. Wikipedia indexing relies on faiss for the dense version. You can get all of these by running:\n",
        "\n",
        "<!-- pip install elasticsearch -->\n",
        "pip install faiss_gpu\n",
        "pip install nlp\n",
        "pip install transformers\n",
        "<!-- \n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz\n",
        "tar -xzvf elasticsearch-7.7.1-linux-x86_64.tar.gz -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAsRpBK4DhS_",
        "outputId": "f289e3a6-8d11-4136-f478-846bb3648b97"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 19 03:00:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u14eRYn09Kp",
        "outputId": "4652ac85-ff51-4e7a-a179-3e3126349736"
      },
      "source": [
        "!pip install faiss_gpu nlp transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss_gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7 MB 9.4 kB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 53.7 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 56.9 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.62.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, nlp, faiss-gpu\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed faiss-gpu-1.7.1.post2 huggingface-hub-0.0.17 nlp-0.4.0 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VgSm5-serJa"
      },
      "source": [
        "import functools\n",
        "import math\n",
        "import os  # noqa: F401\n",
        "from random import choice, randint\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import faiss  # noqa: F401\n",
        "import nlp  # noqa: F401\n",
        "import pandas as pd\n",
        "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddm2PdNBlkvZ",
        "outputId": "976b0df1-a43f-41f5-a862-cb6f8e2f5b88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xS7XbaolnKg"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TSAI/Capstone_1/data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EgAlp0clpW5"
      },
      "source": [
        "# from lfqa_utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpLMu5SeE5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3130cc-9489-4a9f-f662-0aac24eb5332"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "path = '/content/drive/MyDrive/TSAI/Capstone_1/data'\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB1FubpEnOWG",
        "outputId": "8915dcc3-a589-4250-ac92-12de107ed05d"
      },
      "source": [
        "folder = \"retriever_models\"\n",
        "# os.chdir(path)\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"retriever_models directory exists\")\n",
        "else:\n",
        "    print(\"retriever_models directory Doesn't exists, creating one\")\n",
        "    os.mkdir(folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current dir is: /content/drive/MyDrive/TSAI/Capstone_1/data\n",
            "retriever_models directory Doesn't exists, creating one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faCLqxJImdZU",
        "outputId": "ffad6fa2-a85c-4e14-fd7e-4898ae06bc47"
      },
      "source": [
        "folder = \"seq2seq_models\"\n",
        "# os.chdir(path)\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"seq2seq_models directory exists\")\n",
        "else:\n",
        "    print(\"seq2seq_models directory Doesn't exists, creating one\")\n",
        "    os.mkdir(folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current dir is: /content/drive/MyDrive/TSAI/Capstone_1/data\n",
            "seq2seq_models directory Doesn't exists, creating one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGAmr8IqmNNU"
      },
      "source": [
        "## Data Set Preparation\n",
        "One of the major tasks after collecting data is to clean the datasets and bring all five sets into a common json format.\n",
        "\n",
        "An example from the final question-answer pair training data set looks like follows :\n",
        "\n",
        "- Example :\n",
        "\n",
        "- - *x: Question: Why is PyTorch better than Keras?*\n",
        "  - *z: Answer Document: In my opinion, this is a very personal question. Every developer/person would have his/her comfort level while deciding on which platform suits them. I will share my opinion. I think PyTorch is better because it is pythonic, (literally like python and you do not need to learn anything new), has higher performance, strong community support, most new papers are written using PyTorch, and has dynamic computation graphs. But again, I might be biased towards it, and maybe you may like something that is better in Keras than Pytorch and might decide to go ahead with it.*\n",
        "  - *y: Exact Answer: PyTorch is better because it is pythonic, has higher performance,* has*strong community support, most new papers are written using PyTorch, and has dynamic computation graphs.*\n",
        "\n",
        "We will save all the supporting documents (answer documents) from train and test sets separately in another json for the ease of training. It's because one question might have been addressed in other supporting documents as well (other than the oblivious answer document ). Saving all supporting documents separately benefits us to take advantage of all relevant documents to answer that particular question (We'll see how this works in the following sections)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v6YJLUleqY"
      },
      "source": [
        "with open(path+'/train_data.json') as f:\n",
        "        train = json.load(f)\n",
        "with open(path+'/test_data.json') as f:\n",
        "        test = json.load(f)\n",
        "with open(path+'/context_master.json') as f:\n",
        "        passage_snippets = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrV7bVHQeyfg",
        "outputId": "f6dc40ae-0120-413f-f324-9d3d07ad66d7"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'x': 'Maxout Layer',\n",
              " 'y': 'The Maxout layer can be implemented as follows \\npython\\nclass Maxout(nn.Module):\\n def __init__(self, d_in, d_out, pool_size):\\n super().__init__()\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\n def forward(self, inputs):\\n shape = list(inputs.size())\\n shape[-1] = self.d_out\\n shape.append(self.pool_size)\\n max_dim = len(shape) - 1\\n out = self.lin(inputs)\\n m, i = out.view(*shape).max(max_dim)\\n return m\\n',\n",
              " 'z': 'For ones who need Maxout, I changed the above code to make it work. \\npython\\nclass Maxout(nn.Module):\\n def __init__(self, d_in, d_out, pool_size):\\n super().__init__()\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\n def forward(self, inputs):\\n shape = list(inputs.size())\\n shape[-1] = self.d_out\\n shape.append(self.pool_size)\\n max_dim = len(shape) - 1\\n out = self.lin(inputs)\\n m, i = out.view(*shape).max(max_dim)\\n return m\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKLzwpWfQNQ",
        "outputId": "8411e1d4-5a78-4492-8555-3fce907dc136"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9140"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVg6eAz9e6aP",
        "outputId": "a9b4ce6d-c038-4499-c9ab-f50617d31c72"
      },
      "source": [
        "test[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 101,\n",
              " 'x': 'What do Variable(tensor, requires_grad) return instead of Variables?',\n",
              " 'y': 'Tensors',\n",
              " 'z': 'The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected, but they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors with the same method names.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZUuSCe1fSy_",
        "outputId": "995d845f-9245-47d0-b386-a4c45f59df7b"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2286"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspIob65hT14"
      },
      "source": [
        "## Retrieving Support Documents with an Dense retriever Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io61B76Fh7ro"
      },
      "source": [
        "Before generating answers for the given query, our QA system needs to find supporting documents. The Retriever module’s job is to find the best candidate documents by calculating the similarity between query and document vectors. We can use either a sparse retriever or a dense retriever to automatically find relevant document snippets for a question.\n",
        "\n",
        "The sparse retriever works by finding passages that feature the words from the query. However, it has no way to know *a priori* which of these words are more important in context and seems to struggle with understanding the central theme of the query.\n",
        "\n",
        "Thankfully, some recent works have taken advantage of advances in pre-trained contextual word representations to solve this problem. Models such as [DPR](https://arxiv.org/abs/2004.04906) or [REALM](https://arxiv.org/abs/2002.08909) for example learn to compute a vector representation of the query, as well as vector representations of passages in such a way that the passages that best answer a question maximize the dot product between the two representations. Retrieval is then reduced to a Maximum Inner Product Search, which can be executed efficiently using systems like [FAISS](https://github.com/facebookresearch/faiss).\n",
        "\n",
        "These successes are very encouraging for our Open-Domain QA application. However, our setup does not quite meet the requirements of either of these approaches. On the one hand, the [DPR](https://arxiv.org/abs/2004.04906) system is trained using gold passage annotations. Unfortunately, we do not have such annotations for our data set. On the other hand, while [REALM](https://arxiv.org/abs/2002.08909) is trained without passage supervision, it requires a pretty expensive pre-training step with an [Inverse Cloze Task](https://arxiv.org/abs/1906.00300) (100,000 steps with batch size 4096), and the ability to re-compute the embeddings of all passages regularly during training.\n",
        "\n",
        "To train a similar dense retrieval system at reduced cost without having access to gold passage annotation, we will have to **take advantage of another unique feature of our dataset**, **namely the fact that our answers are quite similar in style to the document snippets we want to index. Our answers are summarized, cleaned versions of corresponding document snippets.** Our hypothesis then is that if we train a system to embed the questions and answers in our dataset in a way that allows us to easily match questions to answers, then using the answer embedder on document snippets should allow us to similarly match questions to supporting evidence from document corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJa0ofK7f65J"
      },
      "source": [
        "###############\n",
        "# retriever training\n",
        "###############\n",
        "class DatasetQARetriver(Dataset):\n",
        "    def __init__(self, examples_array, num_rows, extra_answer_threshold=2, min_answer_length=1, training=True, n_samples=None):\n",
        "        self.data = examples_array\n",
        "        self.answer_thres = extra_answer_threshold\n",
        "        self.min_length = min_answer_length\n",
        "        self.training = training\n",
        "        self.n_samples = num_rows if n_samples is None else n_samples\n",
        "        self.num_rows = num_rows\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        example = self.data[idx]\n",
        "        question = example[\"x\"]\n",
        "        answer = example[\"y\"]\n",
        "        return (question, answer)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx % self.num_rows)\n",
        "\n",
        "\n",
        "class RetrievalQAEmbedder(torch.nn.Module):\n",
        "    def __init__(self, sent_encoder, dim):\n",
        "        super(RetrievalQAEmbedder, self).__init__()\n",
        "        self.sent_encoder = sent_encoder\n",
        "        self.output_dim = 128\n",
        "        self.project_q = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.project_a = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.ce_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def embed_sentences_checkpointed(self, input_ids, attention_mask, checkpoint_batch_size=-1):\n",
        "        # reproduces BERT forward pass with checkpointing\n",
        "        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
        "            return self.sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
        "        else:\n",
        "            # prepare implicit variables\n",
        "            device = input_ids.device\n",
        "            input_shape = input_ids.size()\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "            head_mask = [None] * self.sent_encoder.config.num_hidden_layers\n",
        "            extended_attention_mask: torch.Tensor = self.sent_encoder.get_extended_attention_mask(\n",
        "                attention_mask, input_shape, device\n",
        "            )\n",
        "\n",
        "            # define function for checkpointing\n",
        "            def partial_encode(*inputs):\n",
        "                encoder_outputs = self.sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask,)\n",
        "                sequence_output = encoder_outputs[0]\n",
        "                pooled_output = self.sent_encoder.pooler(sequence_output)\n",
        "                return pooled_output\n",
        "\n",
        "            # run embedding layer on everything at once\n",
        "            embedding_output = self.sent_encoder.embeddings(\n",
        "                input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
        "            )\n",
        "            # run encoding and pooling on one mini-batch at a time\n",
        "            pooled_output_list = []\n",
        "            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
        "                b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
        "                pooled_output_list.append(pooled_output)\n",
        "            return torch.cat(pooled_output_list, dim=0)\n",
        "\n",
        "    def embed_questions(self, q_ids, q_mask, checkpoint_batch_size=-1):\n",
        "        q_reps = self.embed_sentences_checkpointed(q_ids, q_mask, checkpoint_batch_size)\n",
        "        return self.project_q(q_reps)\n",
        "\n",
        "    def embed_answers(self, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        a_reps = self.embed_sentences_checkpointed(a_ids, a_mask, checkpoint_batch_size)\n",
        "        return self.project_a(a_reps)\n",
        "\n",
        "    def forward(self, q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        device = q_ids.device\n",
        "        q_reps = self.embed_questions(q_ids, q_mask, checkpoint_batch_size)\n",
        "        a_reps = self.embed_answers(a_ids, a_mask, checkpoint_batch_size)\n",
        "        compare_scores = torch.mm(q_reps, a_reps.t())#cosine similarity\n",
        "        loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))#cross entrophy loss\n",
        "        loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
        "        loss = (loss_qa + loss_aq) / 2\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_qa_retriever_model(model_name=\"google/bert_uncased_L-8_H-512_A-8\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    # run bert_model on a dummy batch to get output dimension\n",
        "    d_ids = torch.LongTensor(\n",
        "        [[bert_model.config.bos_token_id if bert_model.config.bos_token_id is not None else 1]]\n",
        "    ).to(device)\n",
        "    d_mask = torch.LongTensor([[1]]).to(device)\n",
        "    sent_dim = bert_model(d_ids, attention_mask=d_mask)[1].shape[-1]\n",
        "    qa_embedder = RetrievalQAEmbedder(bert_model, sent_dim).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        qa_embedder.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, qa_embedder\n",
        "\n",
        "\n",
        "def make_qa_retriever_batch(qa_list, tokenizer, max_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        " \n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    \n",
        "    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),torch.LongTensor(q_toks[\"attention_mask\"]).to(device),)\n",
        "    # print(len(a_ls))\n",
        "\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    # TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
        "    # print(a_toks)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "\n",
        "    return (q_ids, q_mask, a_ids, a_mask)\n",
        "\n",
        "\n",
        "def train_qa_retriever_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    \n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # print(next(iter(data_loader)).shape)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        # print(\"q_ids\",q_ids.shape)\n",
        "        # print(\" q_mask,\", q_mask.shape)\n",
        "        # print(\"A_id\", a_ids.shape)\n",
        "        q_ids, q_mask, a_ids, a_mask = batch\n",
        "        pre_loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
        "        loss = pre_loss.sum()\n",
        "        # optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def evaluate_qa_retriever(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=eval_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    tot_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            q_ids, q_mask, a_ids, a_mask = batch\n",
        "            loss = model(q_ids, q_mask, a_ids, a_mask)\n",
        "            tot_loss += loss.item()\n",
        "        return tot_loss / (step + 1)\n",
        "\n",
        "\n",
        "def train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args):\n",
        "    qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
        "    qar_scheduler = get_linear_schedule_with_warmup(\n",
        "        qar_optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=(qar_args.num_epochs + 1) * math.ceil(len(qar_train_dset) / qar_args.batch_size),\n",
        "    )\n",
        "    for e in range(qar_args.num_epochs):\n",
        "        train_qa_retriever_epoch(qar_model, qar_train_dset, qar_tokenizer, qar_optimizer, qar_scheduler, qar_args, e)\n",
        "        m_save_dict = {\n",
        "            \"model\": qar_model.state_dict(),\n",
        "            \"optimizer\": qar_optimizer.state_dict(),\n",
        "            \"scheduler\": qar_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(qar_args.model_save_name))\n",
        "        # torch.save(m_save_dict, \"{}_{}.pth\".format(qar_args.model_save_name, e))\n",
        "        eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
        "        print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906,
          "referenced_widgets": [
            "5fdcd71058314a328f6c7da876f8b1b5",
            "bd5cd681ee1940628a45d8a2e4689277",
            "93389c64f4534d08b464fa85efd48202",
            "9e55cd4b4d734ee1b319931737f8345f",
            "3457f80099c44a63a037a93b375c896d",
            "b0583d1fa83e4a16be64dbb9342773d6",
            "f0879d8f923f418c864b5451fdaec4c4",
            "5d493fde6764459dab0f405734dc8112",
            "f99ecc86e0a44c7aa064eccd0cb4ec10",
            "c01fd139b45c46569db5199ccc9b38d0",
            "df56001cd51940a8b2ba8fb54690c6a6",
            "719a22c2df2e4545ad69a839bce176eb",
            "c45cdc6c5ca24092ace5adb7ab2a7716",
            "48ea9d815370499eb4ace34150ccce2b",
            "5e7361d8a7d84d709874b34b1fa9b837",
            "487b6dea9d3d47e0851f09cbdbde5e94",
            "6950bad1e5a6427796167e94f8511503",
            "52040de888534c26afdbd6ad7361447f",
            "05ca204097ff4e029e991ace803630e1",
            "156d1f106b26493a900e7411398c135d",
            "92b63afbaee3488e828669f0761ee344",
            "6a825a1b788f4e12bfd2bb8193c7f4c8",
            "43c70d14fb8546d8ad4d595862b108b5",
            "5f39c3ebcc4948ea8dab2f707763911d",
            "2189b9ad8163417fa0dcdb573df04c97",
            "f90459e368414f1cbcd723daf885ded1",
            "b7005af3f1d64329be500cf07a1d0fc7",
            "ffb71a4a90e044e388283d32e1bab8b8",
            "cd9bac43833b46478caade2fab4d7f33",
            "a6a31aa70b1a4d70bdf15410f4102513",
            "4e7ae04312354d8c9c400a6d415ab9e1",
            "1d82e91ac7df416fbf529d48590677c1",
            "ecf4f52fff36452a8d9ecf4381ab3c48"
          ]
        },
        "id": "O30WJrHHhVcS",
        "outputId": "6c47905e-08f2-4e71-85de-c8f9c80f4d03"
      },
      "source": [
        "# training arguments\n",
        "class ArgumentsQAR():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512\n",
        "        self.max_length = 128\n",
        "        self.checkpoint_batch_size = 32\n",
        "        self.print_freq = 100\n",
        "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-768_A-12\"\n",
        "        self.model_save_name = \"retriever_model_l-8_h-768_b-512-512\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs =10\n",
        "\n",
        "qar_args = ArgumentsQAR()\n",
        "\n",
        "# prepare torch Dataset objects\n",
        "qar_train_dset = DatasetQARetriver(train,num_rows=len(train), training=True)\n",
        "qar_valid_dset = DatasetQARetriver(test,num_rows=len(test), training=False)\n",
        "\n",
        "# load pre-trained BERT and make model\n",
        "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
        "        model_name=qar_args.pretrained_model_name,\n",
        "        from_file=None,\n",
        "        device=\"cuda\"\n",
        ")\n",
        "\n",
        "# train the model\n",
        "train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fdcd71058314a328f6c7da876f8b1b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "719a22c2df2e4545ad69a839bce176eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43c70d14fb8546d8ad4d595862b108b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/327M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-8_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of    17 \t L: 6.387 \t -- 10.484\n",
            " 0     1 of    17 \t L: 6.375 \t -- 20.875\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    0: 5.888\n",
            " 1     0 of    17 \t L: 5.996 \t -- 10.389\n",
            " 1     1 of    17 \t L: 5.982 \t -- 20.901\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    1: 5.510\n",
            " 2     0 of    17 \t L: 5.566 \t -- 10.400\n",
            " 2     1 of    17 \t L: 5.530 \t -- 20.779\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    2: 5.087\n",
            " 3     0 of    17 \t L: 5.060 \t -- 10.392\n",
            " 3     1 of    17 \t L: 5.112 \t -- 20.781\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    3: 4.851\n",
            " 4     0 of    17 \t L: 4.527 \t -- 10.381\n",
            " 4     1 of    17 \t L: 4.361 \t -- 20.765\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    4: 4.590\n",
            " 5     0 of    17 \t L: 3.759 \t -- 10.395\n",
            " 5     1 of    17 \t L: 3.641 \t -- 20.770\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    5: 4.770\n",
            " 6     0 of    17 \t L: 2.935 \t -- 10.380\n",
            " 6     1 of    17 \t L: 2.869 \t -- 20.761\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    6: 5.034\n",
            " 7     0 of    17 \t L: 2.120 \t -- 10.392\n",
            " 7     1 of    17 \t L: 2.154 \t -- 20.780\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    7: 5.551\n",
            " 8     0 of    17 \t L: 1.497 \t -- 10.384\n",
            " 8     1 of    17 \t L: 1.487 \t -- 20.771\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    8: 6.198\n",
            " 9     0 of    17 \t L: 1.157 \t -- 10.388\n",
            " 9     1 of    17 \t L: 1.048 \t -- 20.776\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    9: 6.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nICkWOlSaSbN"
      },
      "source": [
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGh7wbrQunpO"
      },
      "source": [
        "Once the model is trained, it can be used to compute passage embeddings for all document corpus. The make_qa_dense_index method takes advantage of numpy memory-mapping, so embeddings are written directly to disk. Again with a single GPU, computing the full set of passage embeddings should take about1 hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yK4na7_YUEW"
      },
      "source": [
        "\n",
        "###############\n",
        "# trained retrieval model usage\n",
        "###############\n",
        "def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=512, device=\"cuda\"):\n",
        "    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n",
        "    return a_reps.numpy()\n",
        "def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda\"):\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n",
        "    return q_reps.numpy()\n",
        "def make_qa_dense_index(\n",
        "    qa_embedder,\n",
        "    tokenizer,\n",
        "    passages_dset,\n",
        "    batch_size=512,\n",
        "    max_length=128,\n",
        "    index_name=\"kilt_passages_reps.dat\",\n",
        "    dtype=\"float32\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    st_time = time()\n",
        "    fp = np.memmap(index_name, dtype=dtype, mode=\"w+\", shape=(len(passages_dset),128))\n",
        "    n_batches = math.ceil(len(passages_dset) / batch_size)\n",
        "    for i in range(n_batches):\n",
        "        passages = [p[\"z\"] for p in passages_dset[i * batch_size : (i + 1) * batch_size]]\n",
        "        reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length, device)\n",
        "        fp[i * batch_size : (i + 1) * batch_size] = reps\n",
        "        if i % 50 == 0:\n",
        "            print(i, time() - st_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGkQc1L0Kop9"
      },
      "source": [
        "# os.chdir(r'/content/drive/MyDrive/TSAI/Capstone_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fB4Dp5hukVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a19e344-00fb-4547-d49a-40ce2438312d"
      },
      "source": [
        "if not os.path.isfile('passages_reps_32_l-8_h-768_b-512-512.dat'):\n",
        "  print(\"hi\")\n",
        "\n",
        "  make_qa_dense_index(\n",
        "          qar_model, qar_tokenizer, passage_snippets, device='cuda',\n",
        "          index_name='passages_reps_32_l-8_h-768_b-512-512.dat' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.4317922592163086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01GnGvUo1YPC"
      },
      "source": [
        "## Using the Trained Dense Retriever and document Index\n",
        "Now that we have trained our model to compute query and answer embeddings and used it to compute passage embeddings for all our document snippets, let's see whether it can actually find supporting evidence for a new question. Recalling the the two steps to using the dense retriever: we first compute an embedding for a new question, then do Max Inner Product Search with the pre-computed passage representations.\n",
        "\n",
        "The MIPS part can be executed efficiently with the faiss library. Additionally, since we computed 128-dimensional passage embeddings, the whole of the representations fits on a GPU, making retrieval even faster. We can create the faiss_gpu index with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdiIwQw4Qboj"
      },
      "source": [
        "n_ret = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWm_WCC1g--"
      },
      "source": [
        "faiss_res = faiss.StandardGpuResources()\n",
        "passage_reps = np.memmap(\n",
        "            'passages_reps_32_l-8_h-768_b-512-512.dat',\n",
        "            dtype='float32', mode='r',\n",
        "            # shape=(wiki40b_snippets.num_rows, 128)\n",
        "            # wiki40b_snippets.num_rows = 11378343,english sections from wiki40B dataset\n",
        "            shape=(len(passage_snippets), 128)\n",
        ")\n",
        "\n",
        "doc_index_flat = faiss.IndexFlatIP(128)\n",
        "doc_gpu_index = faiss.index_cpu_to_gpu(faiss_res, 0, doc_index_flat)\n",
        "doc_gpu_index.add(passage_reps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HFtgUv-30K4"
      },
      "source": [
        "\n",
        "def query_qa_dense_index(\n",
        "    question, qa_embedder, tokenizer, doc_passages, doc_index, n_results=n_ret, min_length=1, device=\"cuda\"\n",
        "):\n",
        "    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
        "    D, I = doc_index.search(q_rep, 2 * n_results)\n",
        "    res_passages = [doc_passages[int(i)] for i in I[0]]\n",
        "    support_doc = \"<P> \" + \" <P> \".join([p[\"z\"] for p in res_passages])\n",
        "    res_list = [dict([(k, p[k]) for k in [\"z\"]]) for p in res_passages]\n",
        "    res_list = [res for res in res_list if len(res[\"z\"].split()) > min_length][:n_results]\n",
        "    for r, sc in zip(res_list, D[0]):\n",
        "        r[\"score\"] = float(sc)\n",
        "    return support_doc, res_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1hJ2_U12lYQ"
      },
      "source": [
        "Now we can use the query_qa_dense_index function to query the dense index for our running example question :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhJqmCP52wnA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0324027-3655-4d09-ef9f-916ee8695d5f"
      },
      "source": [
        "question = test[90]['x']\n",
        "question"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Parameter not registering if .to(device) is used'"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYoEC2yU2jUA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ffd070fd-f401-4ed6-c095-2ac03c2de3ab"
      },
      "source": [
        "doc, res_list = query_qa_dense_index(question, qar_model, qar_tokenizer, passage_snippets, doc_gpu_index, device='cuda')\n",
        "print(res_list)\n",
        "df = pd.DataFrame({\n",
        "    \n",
        "    'Text': ['--- ' + question] + [res['z'] for res in res_list],\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'z': \"It's supposed to be `torch.device`\", 'score': 27.264524459838867}, {'z': 'your input is on gpu 1 but your net work is on gpu 0.', 'score': 25.201988220214844}, {'z': 'As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream.', 'score': 24.98475456237793}, {'z': \"DataParallel requires every input tensor be provided on the first device in its device_ids list.\\n\\nIt basically uses that device as a staging area before scattering to the other GPUs and it's the device where final outputs are gathered before returning from forward. If you want device 2 to be the primary device then you just need to put it at the front of the list as follows\\n\\nmodel = nn.DataParallel(model, device_ids = [2, 0, 1, 3])\\nmodel.to(f'cuda:{model.device_ids[0]}')\\n\\n\\nAfter which all tensors provided to model should be on the first device as well.\\n\\nx = ... # input tensor\\nx = x.to(f'cuda:{model.device_ids[0]}')\\ny = model(x)\\n\\n\", 'score': 24.732805252075195}, {'z': \"According to the documentation for torch.cuda.device\\n\\ndevice (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.\\n\\nBased on that we could use something like\\nwith torch.cuda.device(self.device if self.device.type == 'cuda' else None):\\n    # do a bunch of stuff\\n\\nwhich would simply be a no-op when self.device isn't a CUDA device.\\n\", 'score': 24.48703384399414}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row0_col0,#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row1_col0,#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row2_col0,#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row3_col0,#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row4_col0,#T_1be9bdaa_1922_11ec_8326_0242ac1c0002row5_col0{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Text</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row0_col0\" class=\"data row0 col0\" >--- Parameter not registering if .to(device) is used</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row1_col0\" class=\"data row1 col0\" >It's supposed to be `torch.device`</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row2_col0\" class=\"data row2 col0\" >your input is on gpu 1 but your net work is on gpu 0.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row3_col0\" class=\"data row3 col0\" >As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row4_col0\" class=\"data row4 col0\" >DataParallel requires every input tensor be provided on the first device in its device_ids list.\n",
              "\n",
              "It basically uses that device as a staging area before scattering to the other GPUs and it's the device where final outputs are gathered before returning from forward. If you want device 2 to be the primary device then you just need to put it at the front of the list as follows\n",
              "\n",
              "model = nn.DataParallel(model, device_ids = [2, 0, 1, 3])\n",
              "model.to(f'cuda:{model.device_ids[0]}')\n",
              "\n",
              "\n",
              "After which all tensors provided to model should be on the first device as well.\n",
              "\n",
              "x = ... # input tensor\n",
              "x = x.to(f'cuda:{model.device_ids[0]}')\n",
              "y = model(x)\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_1be9bdaa_1922_11ec_8326_0242ac1c0002row5_col0\" class=\"data row5 col0\" >According to the documentation for torch.cuda.device\n",
              "\n",
              "device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.\n",
              "\n",
              "Based on that we could use something like\n",
              "with torch.cuda.device(self.device if self.device.type == 'cuda' else None):\n",
              "    # do a bunch of stuff\n",
              "\n",
              "which would simply be a no-op when self.device isn't a CUDA device.\n",
              "</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb6e5814790>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGKrGgr6u6a"
      },
      "source": [
        "## Retriever Model Evaluation\n",
        "We have trained a retrieval model that *seems* to be working fine, at least on our running example. Before we use it to answer questions, however, we would like to be able to get some quantitative evaluation of the performance of our dense retriever model.\n",
        "\n",
        "For the retriever, we want to favor recall over precision as our priority is to make sure that all of the information needed to write the answers is present in the support document. If there is unrelated information, the generation model can learn to sort it out. We measure this by computing the proportion of words in the high-scoring answers which are present in the retrieved support document. To focus on important words, we also weigh answer words by their *Inverse Document Frequency*.This gives us the following IDF-recall scoring function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFp8BgM-GQnj"
      },
      "source": [
        "\n",
        "# We first select high-scoring answers (answers beyond the first must have a score of at least 3)\n",
        "test_qa_list = [(exple['x'],exple['y']) for exple in test]\n",
        "\n",
        "# We then compute word frequencies in answer text\n",
        "answer_doc_freq = {}\n",
        "for q, a in test_qa_list:\n",
        "    for w in a.lower().split():\n",
        "        answer_doc_freq[w] = answer_doc_freq.get(w, 0) + 1\n",
        "\n",
        "# The IDF-recall function is then:\n",
        "def da_idf_recall(doc, answer):\n",
        "    d_words = dict([(w, True) for w in doc.lower().split()])\n",
        "    a_words = answer.lower().split()   \n",
        "    recall = sum([1. / math.log(1 + answer_doc_freq.get(w, 1)) for w in a_words if w in d_words]) / \\\n",
        "                sum([1. / math.log(1 + answer_doc_freq.get(w, 1)) for w in a_words])\n",
        "    return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWpEC8OmJ-Tu"
      },
      "source": [
        "def evaluate_retriever(qa_list, retriever_func, scoring_func, n_ret, verbose=False):\n",
        "    total_retriever_time = 0.0\n",
        "    total_retriever_score = 0.0\n",
        "    st_time = time()\n",
        "    for i, (question, answer) in enumerate(qa_list):\n",
        "        r_time = time()\n",
        "        retrieved_passages = retriever_func(question, n_ret)\n",
        "        total_retriever_time += time() - r_time\n",
        "        total_retriever_score += scoring_func(retrieved_passages, answer)\n",
        "        if verbose and ((i + 1) % 500 == 0 or i <= 1):\n",
        "            print(\n",
        "                \"{:03d}: S-{:.4f} T-{:.4f} | {:.2f}\".format(\n",
        "                    i + 1, total_retriever_score / (i + 1), total_retriever_time / (i + 1), time() - st_time\n",
        "                )\n",
        "            )\n",
        "    return {\"idf_recall\": total_retriever_score / (i + 1), \"retrieval_time\": total_retriever_time / (i + 1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWTcMlkqHx1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "ed75988b-afe6-4a9a-d6a7-bdadd4661c43"
      },
      "source": [
        "def dense_ret_for_eval(question, n_ret):\n",
        "    _, dense_res_list = query_qa_dense_index(question, qar_model, qar_tokenizer, passage_snippets, doc_gpu_index, device='cuda')\n",
        "    dense_doc =\" \".join([p[\"z\"] for p in dense_res_list])\n",
        "    return dense_doc\n",
        "\n",
        "\n",
        "dense_score = evaluate_retriever(test_qa_list, dense_ret_for_eval, da_idf_recall,n_ret)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'IDF-Recall': [dense_score['idf_recall']],\n",
        "    'Time/Query': [dense_score['retrieval_time']],\n",
        "}, index=[ 'Dense'])\n",
        "df.style.format({'IDF-Recall': \"{:.4f}\", 'Time/Query': \"{:.4f}\"})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_7074c80e_18fc_11ec_8326_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >IDF-Recall</th>        <th class=\"col_heading level0 col1\" >Time/Query</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_7074c80e_18fc_11ec_8326_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Sparse</th>\n",
              "                        <td id=\"T_7074c80e_18fc_11ec_8326_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.2540</td>\n",
              "                        <td id=\"T_7074c80e_18fc_11ec_8326_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.0093</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb7eef10590>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suCtm8mSCI7H"
      },
      "source": [
        "## Generating Answers with a Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8rHQ10Zg33h"
      },
      "source": [
        "# generator seq2seq model training\n",
        "###############\n",
        "class DatasetS2S(Dataset):\n",
        "    def __init__(\n",
        "        self, examples_array,num_rows, make_doc_fun=None, doc_cache=None, training=True\n",
        "    ):\n",
        "        self.training = training\n",
        "        self.data = examples_array\n",
        "        self.make_doc_function = make_doc_fun\n",
        "        self.doc_cache = {} if doc_cache is None else doc_cache\n",
        "        self.num_rows = num_rows\n",
        "        assert not (make_doc_fun is None and doc_cache is None)\n",
        "        # make index of specific question-answer pairs from multi-answers\n",
        "        if self.training:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "        else:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_id_list)\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        i, j = self.qa_id_list[idx]\n",
        "        example = self.data[i]\n",
        "        question = example[\"x\"] \n",
        "        answer = example[\"y\"]\n",
        "        q_id = example[\"id\"]\n",
        "        if self.make_doc_function is not None:\n",
        "            self.doc_cache[q_id] = self.doc_cache.get(q_id, self.make_doc_function(example[\"x\"]))\n",
        "        document = self.doc_cache[q_id]\n",
        "        in_st = \"question: {} context: {}\".format(\n",
        "            question.lower().strip(), document.lower().strip(),\n",
        "        )\n",
        "        out_st = answer\n",
        "        return (in_st, out_st)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx)\n",
        "\n",
        "\n",
        "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        model.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    labels = a_ids[:, 1:].contiguous().clone()\n",
        "    labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
        "    # print(labels)\n",
        "    model_inputs = {\n",
        "        \"input_ids\": q_ids,\n",
        "        \"attention_mask\": q_mask,\n",
        "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "    # print(\"it'sme\",model_inputs)\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=True):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    if curriculum:\n",
        "        train_sampler = SequentialSampler(dataset)\n",
        "    else:\n",
        "        train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "\n",
        "  \n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch_inputs in enumerate(epoch_iterator):\n",
        "        # print(type(step))\n",
        "        pre_loss = model(**batch_inputs)[0].unsqueeze(dim=0)\n",
        "        # print(pre_loss,'pre_loss')\n",
        "        # print(pre_loss.shape)\n",
        "        # print(pre_loss.sum(),'sum')\n",
        "        loss = pre_loss.sum() / pre_loss.shape[0]\n",
        "        loss.backward()\n",
        "        # optimizer\n",
        "        if step % args.backward_freq == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    train_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch_inputs in enumerate(epoch_iterator):\n",
        "            pre_loss = model(**batch_inputs)[0].unsqueeze(dim=0)\n",
        "            # print(pre_loss,'pre_loss')\n",
        "            # print(pre_loss.shape)\n",
        "            # print(pre_loss.sum(),'sum')\n",
        "            loss = pre_loss.sum() / pre_loss.shape[0]\n",
        "            loc_loss += loss.item()\n",
        "            # print(\"loc loss here\",loc_loss)\n",
        "            loc_steps += 1\n",
        "            if step % args.print_freq == 0:\n",
        "                print(\n",
        "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                    )\n",
        "                )\n",
        "    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time,))\n",
        "\n",
        "\n",
        "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
        "    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
        "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
        "        s2s_optimizer,\n",
        "        num_warmup_steps=400,\n",
        "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
        "    )\n",
        "    for e in range(s2s_args.num_epochs):\n",
        "        # print((e == 0))\n",
        "\n",
        "        train_qa_s2s_epoch(\n",
        "            qa_s2s_model,\n",
        "            s2s_train_dset,\n",
        "            qa_s2s_tokenizer,\n",
        "            s2s_optimizer,\n",
        "            s2s_scheduler,\n",
        "            s2s_args,\n",
        "            e,\n",
        "            curriculum=True,\n",
        "        )\n",
        "        m_save_dict = {\n",
        "            \"model\": qa_s2s_model.state_dict(),\n",
        "            \"optimizer\": s2s_optimizer.state_dict(),\n",
        "            \"scheduler\": s2s_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
        "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
        "        # torch.save(m_save_dict, \"\\{}_{}.pth\".format(s2s_args.model_save_name, e))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0JOPJ1S995Q"
      },
      "source": [
        "n_ret = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "55034a1da1ee40ec8bf6dfafe242a081",
            "0dcfdf0f39854a7dab73a8bfa7001ed1",
            "0879349f65644185a9b5d43b2a4a2b4a",
            "c30a9eb5ebbc409ebaa088b09f2ada99",
            "f550d454136344f39819617661d8fada",
            "870fbbcfc27d46da9358a8d4b34ab861",
            "97e51ebeec5149dd8f2de8d2f787f171",
            "01c8f3f9208e45888e93070e6849575c",
            "e2d38c2a264a4affbb860676af871b3d",
            "0fb4164e98bf444aa65a67fb2049efb0",
            "530fd9c6ca8143b0ad858d82a5a62552",
            "85377cd8f3cb465e8ef05a5a0d019df2",
            "1152638e41884d3a81ff8d244157e2b0",
            "da701940089841429114e220e18330a6",
            "6534b4cf4302415d83311e914252a360",
            "5135c8397ee54f36839d5e320be7b8fc",
            "669a132ed4fe46fa8863a9180fed635c",
            "4cdd002c61c9485ebe165d97bfdc544a",
            "ef62e63548024d6c9826bc7192543d8a",
            "2446f93867d2467fa879bd75946b406c",
            "96a42b3d91a642e09d86c89d75b6d19a",
            "6b2ce628d9034eb3b6848b5e387d90b9",
            "54256f9f1b164d4f882680651aaa59ea",
            "a72f265e30d54f2284b93956c2be0ba0",
            "4f91f299390f45c8b1cd910574d7af49",
            "2fc33c228b614fc5b7f6447128d04c8d",
            "a3613e24a7764ea591a500615f5aeb32",
            "1e61ca02144642e0bce2a071df631503",
            "b55f51977f2443188a57c5cd7751ae2a",
            "be65781737bf40e68703d9146242a4dc",
            "6f28470f80354d3d8ed255d608a41ade",
            "c284aa157f814c97b8ed244207f77d45",
            "78e545b0aa0f4a1ca4f4e462d70da5ef",
            "395781d292634fa38e67772c5485a3b0",
            "6628d2a0ad3047f8b6c3088c3a04cf86",
            "3e556c25d663424284888abbc05002cb",
            "c57d7861a26a41329dcc557c2033c9b8",
            "d627189a6b2945dab1fda1f42981b417",
            "c7e5d8a84fe647b8bf6c5673a4623674",
            "a952fc453489496bb6ee450e2abacf3e",
            "8072c8f2682443d692930a124e6c17eb",
            "e1b0e6ed0a4f4478b51125171eb006e4",
            "966305a134b44cb695f9ebe69ea11780",
            "cbda97731ce34b47af3f8cbeb842f41c",
            "2549c554948340c08b2f3716678ec2e0",
            "84883d04f64a48cb99989046a8b8368f",
            "e5bf14c40a47463c8e190167b23bef41",
            "6c2fbcf402d947bba2738e22ae8953fa",
            "a4f4679f0b03403d9f1a12f0bb22282b",
            "954ac031e75d437a88da1f5e676b0810",
            "704aba92165249edb38754ea25aa7653",
            "7fba7cb2843c4b64bdcae32e2e78614f",
            "153bbe985822450fb48c233d47ac5be1",
            "e59cbd57e11843dd9154d1a53eee1c91",
            "e148ca1a345e4adb89467129068242ed",
            "4b3fce1413214b3cabcda7ddca71103f",
            "dc20e5b2ea874ca5bfd40a0791b2d478",
            "e253f1bc04f1431687194a0351515240",
            "9a4eca95103d4ce3a95723622a467d23",
            "9a0844f236ba4895841e27f2dd101072",
            "7e870539c5b645e588f0dd16c5f4bb64",
            "b278a1f09ce6403bab2b1edf0072db58",
            "3fe44030c170457bbf7eadae1e4ac713",
            "9a0cb712e92d430ba62cd7b9c151ed44",
            "5b841c76859b4a3eb19cd8e3e2edc70c",
            "3ef580c2a6c746eeb60028b6e205fce3"
          ]
        },
        "id": "5Fhed0htCIL5",
        "outputId": "818c981a-2e8c-41ec-bd65-70a222364c6b"
      },
      "source": [
        "# pre-computing support documents\n",
        "qna_train_docs = []\n",
        "for example in train:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer,passage_snippets, doc_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    qna_train_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "qna_valid_docs = []\n",
        "for example in test:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer, passage_snippets, doc_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    qna_valid_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "# training loop proper\n",
        "class ArgumentsS2S():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 2\n",
        "        self.backward_freq = 16\n",
        "        self.max_length = 512\n",
        "        self.print_freq = 100\n",
        "        self.model_save_name = \"s2s_bart_model\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs = 5\n",
        "\n",
        "s2s_args = ArgumentsS2S()\n",
        "\n",
        "# qna_train_docs = json.load(open('precomputed/qna_train_precomputed_dense_docs.json'))\n",
        "# qna_valid_docs = json.load(open('precomputed/qna_valid_precomputed_dense_docs.json'))\n",
        "s2s_train_dset = DatasetS2S(train,num_rows =len(train), doc_cache=dict([(k, d) for k, d, src_ls in qna_train_docs]))\n",
        "s2s_valid_dset = DatasetS2S(test,num_rows =len(test), doc_cache=dict([(k, d) for k, d, src_ls in qna_valid_docs]), training=False)\n",
        "\n",
        "qa_s2s_tokenizer, pre_model = make_qa_s2s_model(\n",
        "    model_name=\"facebook/bart-large\",\n",
        "    from_file=None,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "# qa_s2s_model = torch.nn.DataParallel(pre_model)\n",
        "qa_s2s_model =pre_model\n",
        "train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55034a1da1ee40ec8bf6dfafe242a081",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85377cd8f3cb465e8ef05a5a0d019df2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54256f9f1b164d4f882680651aaa59ea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "395781d292634fa38e67772c5485a3b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2549c554948340c08b2f3716678ec2e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b3fce1413214b3cabcda7ddca71103f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of  4570 \t L: 10.090 \t -- 0.436\n",
            " 0     1 of  4570 \t L: 19.756 \t -- 0.749\n",
            " 0   100 of  4570 \t L: 5.388 \t -- 33.072\n",
            " 0   200 of  4570 \t L: 4.342 \t -- 65.793\n",
            " 0   300 of  4570 \t L: 4.027 \t -- 98.213\n",
            " 0   400 of  4570 \t L: 3.936 \t -- 130.459\n",
            " 0   500 of  4570 \t L: 3.783 \t -- 162.676\n",
            " 0   600 of  4570 \t L: 3.532 \t -- 195.352\n",
            " 0   700 of  4570 \t L: 3.236 \t -- 227.447\n",
            " 0   800 of  4570 \t L: 2.926 \t -- 259.430\n",
            " 0   900 of  4570 \t L: 3.050 \t -- 291.820\n",
            " 0  1000 of  4570 \t L: 2.854 \t -- 324.104\n",
            " 0  1100 of  4570 \t L: 3.297 \t -- 356.097\n",
            " 0  1200 of  4570 \t L: 3.502 \t -- 387.916\n",
            " 0  1300 of  4570 \t L: 3.090 \t -- 419.629\n",
            " 0  1400 of  4570 \t L: 2.913 \t -- 452.050\n",
            " 0  1500 of  4570 \t L: 2.895 \t -- 484.833\n",
            " 0  1600 of  4570 \t L: 2.804 \t -- 517.658\n",
            " 0  1700 of  4570 \t L: 2.789 \t -- 550.460\n",
            " 0  1800 of  4570 \t L: 3.015 \t -- 583.275\n",
            " 0  1900 of  4570 \t L: 2.946 \t -- 615.972\n",
            " 0  2000 of  4570 \t L: 2.554 \t -- 648.777\n",
            " 0  2100 of  4570 \t L: 2.753 \t -- 681.486\n",
            " 0  2200 of  4570 \t L: 2.639 \t -- 714.280\n",
            " 0  2300 of  4570 \t L: 2.949 \t -- 746.490\n",
            " 0  2400 of  4570 \t L: 2.705 \t -- 779.305\n",
            " 0  2500 of  4570 \t L: 2.480 \t -- 811.919\n",
            " 0  2600 of  4570 \t L: 2.699 \t -- 844.709\n",
            " 0  2700 of  4570 \t L: 2.550 \t -- 877.428\n",
            " 0  2800 of  4570 \t L: 2.272 \t -- 910.301\n",
            " 0  2900 of  4570 \t L: 2.595 \t -- 943.091\n",
            " 0  3000 of  4570 \t L: 2.418 \t -- 975.932\n",
            " 0  3100 of  4570 \t L: 2.313 \t -- 1008.594\n",
            " 0  3200 of  4570 \t L: 2.658 \t -- 1041.449\n",
            " 0  3300 of  4570 \t L: 2.229 \t -- 1074.334\n",
            " 0  3400 of  4570 \t L: 2.198 \t -- 1107.122\n",
            " 0  3500 of  4570 \t L: 2.702 \t -- 1139.962\n",
            " 0  3600 of  4570 \t L: 2.463 \t -- 1172.718\n",
            " 0  3700 of  4570 \t L: 2.636 \t -- 1204.517\n",
            " 0  3800 of  4570 \t L: 2.618 \t -- 1236.513\n",
            " 0  3900 of  4570 \t L: 2.403 \t -- 1268.802\n",
            " 0  4000 of  4570 \t L: 2.532 \t -- 1301.024\n",
            " 0  4100 of  4570 \t L: 2.328 \t -- 1333.320\n",
            " 0  4200 of  4570 \t L: 2.295 \t -- 1365.810\n",
            " 0  4300 of  4570 \t L: 2.467 \t -- 1397.614\n",
            " 0  4400 of  4570 \t L: 2.410 \t -- 1430.006\n",
            " 0  4500 of  4570 \t L: 2.384 \t -- 1462.461\n",
            "Saving model s2s_bart_model\n",
            "    0 of  1143 \t L: 3.427 \t -- 0.116\n",
            "  100 of  1143 \t L: 3.251 \t -- 11.266\n",
            "  200 of  1143 \t L: 3.340 \t -- 21.910\n",
            "  300 of  1143 \t L: 3.402 \t -- 32.884\n",
            "  400 of  1143 \t L: 3.380 \t -- 43.514\n",
            "  500 of  1143 \t L: 3.364 \t -- 54.279\n",
            "  600 of  1143 \t L: 3.381 \t -- 64.981\n",
            "  700 of  1143 \t L: 3.383 \t -- 76.558\n",
            "  800 of  1143 \t L: 3.381 \t -- 87.665\n",
            "  900 of  1143 \t L: 3.391 \t -- 98.307\n",
            " 1000 of  1143 \t L: 3.401 \t -- 108.946\n",
            " 1100 of  1143 \t L: 3.404 \t -- 119.616\n",
            "Total \t L: 3.403 \t -- 124.094\n",
            " 1     0 of  4570 \t L: 2.758 \t -- 0.384\n",
            " 1     1 of  4570 \t L: 3.565 \t -- 0.704\n",
            " 1   100 of  4570 \t L: 2.237 \t -- 33.118\n",
            " 1   200 of  4570 \t L: 2.143 \t -- 65.634\n",
            " 1   300 of  4570 \t L: 2.100 \t -- 98.390\n",
            " 1   400 of  4570 \t L: 2.457 \t -- 131.193\n",
            " 1   500 of  4570 \t L: 2.370 \t -- 163.525\n",
            " 1   600 of  4570 \t L: 2.285 \t -- 195.652\n",
            " 1   700 of  4570 \t L: 2.227 \t -- 228.410\n",
            " 1   800 of  4570 \t L: 2.075 \t -- 260.512\n",
            " 1   900 of  4570 \t L: 2.256 \t -- 292.312\n",
            " 1  1000 of  4570 \t L: 2.127 \t -- 324.131\n",
            " 1  1100 of  4570 \t L: 2.497 \t -- 356.004\n",
            " 1  1200 of  4570 \t L: 2.435 \t -- 387.781\n",
            " 1  1300 of  4570 \t L: 2.003 \t -- 419.496\n",
            " 1  1400 of  4570 \t L: 2.033 \t -- 451.829\n",
            " 1  1500 of  4570 \t L: 2.184 \t -- 483.990\n",
            " 1  1600 of  4570 \t L: 2.086 \t -- 516.868\n",
            " 1  1700 of  4570 \t L: 2.182 \t -- 549.682\n",
            " 1  1800 of  4570 \t L: 2.376 \t -- 582.499\n",
            " 1  1900 of  4570 \t L: 2.343 \t -- 615.195\n",
            " 1  2000 of  4570 \t L: 1.992 \t -- 647.983\n",
            " 1  2100 of  4570 \t L: 2.219 \t -- 680.673\n",
            " 1  2200 of  4570 \t L: 2.167 \t -- 713.404\n",
            " 1  2300 of  4570 \t L: 2.399 \t -- 746.131\n",
            " 1  2400 of  4570 \t L: 2.197 \t -- 778.326\n",
            " 1  2500 of  4570 \t L: 2.140 \t -- 810.429\n",
            " 1  2600 of  4570 \t L: 2.289 \t -- 842.299\n",
            " 1  2700 of  4570 \t L: 2.180 \t -- 874.000\n",
            " 1  2800 of  4570 \t L: 1.887 \t -- 906.077\n",
            " 1  2900 of  4570 \t L: 2.198 \t -- 938.019\n",
            " 1  3000 of  4570 \t L: 2.063 \t -- 969.848\n",
            " 1  3100 of  4570 \t L: 1.798 \t -- 1001.620\n",
            " 1  3200 of  4570 \t L: 2.313 \t -- 1033.471\n",
            " 1  3300 of  4570 \t L: 1.768 \t -- 1065.353\n",
            " 1  3400 of  4570 \t L: 1.804 \t -- 1097.155\n",
            " 1  3500 of  4570 \t L: 2.286 \t -- 1129.053\n",
            " 1  3600 of  4570 \t L: 2.021 \t -- 1161.153\n",
            " 1  3700 of  4570 \t L: 2.223 \t -- 1192.888\n",
            " 1  3800 of  4570 \t L: 2.126 \t -- 1224.726\n",
            " 1  3900 of  4570 \t L: 2.001 \t -- 1257.355\n",
            " 1  4000 of  4570 \t L: 2.099 \t -- 1289.800\n",
            " 1  4100 of  4570 \t L: 1.844 \t -- 1322.505\n",
            " 1  4200 of  4570 \t L: 1.905 \t -- 1354.538\n",
            " 1  4300 of  4570 \t L: 1.992 \t -- 1386.409\n",
            " 1  4400 of  4570 \t L: 1.907 \t -- 1418.985\n",
            " 1  4500 of  4570 \t L: 1.985 \t -- 1451.507\n",
            "Saving model s2s_bart_model\n",
            "    0 of  1143 \t L: 3.156 \t -- 0.117\n",
            "  100 of  1143 \t L: 3.128 \t -- 11.008\n",
            "  200 of  1143 \t L: 3.209 \t -- 21.813\n",
            "  300 of  1143 \t L: 3.269 \t -- 32.608\n",
            "  400 of  1143 \t L: 3.242 \t -- 43.251\n",
            "  500 of  1143 \t L: 3.221 \t -- 54.640\n",
            "  600 of  1143 \t L: 3.244 \t -- 66.023\n",
            "  700 of  1143 \t L: 3.242 \t -- 77.286\n",
            "  800 of  1143 \t L: 3.240 \t -- 88.229\n",
            "  900 of  1143 \t L: 3.251 \t -- 99.213\n",
            " 1000 of  1143 \t L: 3.264 \t -- 110.295\n",
            " 1100 of  1143 \t L: 3.270 \t -- 121.769\n",
            "Total \t L: 3.266 \t -- 126.370\n",
            " 2     0 of  4570 \t L: 2.327 \t -- 0.376\n",
            " 2     1 of  4570 \t L: 2.347 \t -- 0.688\n",
            " 2   100 of  4570 \t L: 1.868 \t -- 32.795\n",
            " 2   200 of  4570 \t L: 1.854 \t -- 65.512\n",
            " 2   300 of  4570 \t L: 1.750 \t -- 97.958\n",
            " 2   400 of  4570 \t L: 2.067 \t -- 129.798\n",
            " 2   500 of  4570 \t L: 2.012 \t -- 161.703\n",
            " 2   600 of  4570 \t L: 1.851 \t -- 193.863\n",
            " 2   700 of  4570 \t L: 1.844 \t -- 226.652\n",
            " 2   800 of  4570 \t L: 1.759 \t -- 259.444\n",
            " 2   900 of  4570 \t L: 1.913 \t -- 291.744\n",
            " 2  1000 of  4570 \t L: 1.785 \t -- 323.576\n",
            " 2  1100 of  4570 \t L: 2.113 \t -- 355.399\n",
            " 2  1200 of  4570 \t L: 1.993 \t -- 387.184\n",
            " 2  1300 of  4570 \t L: 1.624 \t -- 418.897\n",
            " 2  1400 of  4570 \t L: 1.673 \t -- 451.163\n",
            " 2  1500 of  4570 \t L: 1.771 \t -- 483.948\n",
            " 2  1600 of  4570 \t L: 1.698 \t -- 516.644\n",
            " 2  1700 of  4570 \t L: 1.828 \t -- 549.212\n",
            " 2  1800 of  4570 \t L: 1.932 \t -- 581.684\n",
            " 2  1900 of  4570 \t L: 1.887 \t -- 614.389\n",
            " 2  2000 of  4570 \t L: 1.583 \t -- 647.153\n",
            " 2  2100 of  4570 \t L: 1.748 \t -- 679.834\n",
            " 2  2200 of  4570 \t L: 1.740 \t -- 712.579\n",
            " 2  2300 of  4570 \t L: 1.923 \t -- 745.325\n",
            " 2  2400 of  4570 \t L: 1.760 \t -- 778.086\n",
            " 2  2500 of  4570 \t L: 1.799 \t -- 810.679\n",
            " 2  2600 of  4570 \t L: 1.866 \t -- 842.734\n",
            " 2  2700 of  4570 \t L: 1.746 \t -- 875.050\n",
            " 2  2800 of  4570 \t L: 1.545 \t -- 907.462\n",
            " 2  2900 of  4570 \t L: 1.801 \t -- 939.717\n",
            " 2  3000 of  4570 \t L: 1.715 \t -- 972.520\n",
            " 2  3100 of  4570 \t L: 1.515 \t -- 1005.281\n",
            " 2  3200 of  4570 \t L: 1.908 \t -- 1038.089\n",
            " 2  3300 of  4570 \t L: 1.477 \t -- 1070.860\n",
            " 2  3400 of  4570 \t L: 1.518 \t -- 1103.606\n",
            " 2  3500 of  4570 \t L: 1.974 \t -- 1136.396\n",
            " 2  3600 of  4570 \t L: 1.647 \t -- 1169.243\n",
            " 2  3700 of  4570 \t L: 1.902 \t -- 1201.962\n",
            " 2  3800 of  4570 \t L: 1.765 \t -- 1234.701\n",
            " 2  3900 of  4570 \t L: 1.661 \t -- 1267.452\n",
            " 2  4000 of  4570 \t L: 1.749 \t -- 1300.273\n",
            " 2  4100 of  4570 \t L: 1.543 \t -- 1333.001\n",
            " 2  4200 of  4570 \t L: 1.591 \t -- 1365.205\n",
            " 2  4300 of  4570 \t L: 1.651 \t -- 1397.213\n",
            " 2  4400 of  4570 \t L: 1.599 \t -- 1430.005\n",
            " 2  4500 of  4570 \t L: 1.667 \t -- 1462.701\n",
            "Saving model s2s_bart_model\n",
            "    0 of  1143 \t L: 3.005 \t -- 0.121\n",
            "  100 of  1143 \t L: 3.024 \t -- 11.641\n",
            "  200 of  1143 \t L: 3.122 \t -- 23.187\n",
            "  300 of  1143 \t L: 3.200 \t -- 34.789\n",
            "  400 of  1143 \t L: 3.179 \t -- 45.859\n",
            "  500 of  1143 \t L: 3.156 \t -- 56.693\n",
            "  600 of  1143 \t L: 3.182 \t -- 67.380\n",
            "  700 of  1143 \t L: 3.182 \t -- 78.498\n",
            "  800 of  1143 \t L: 3.179 \t -- 89.637\n",
            "  900 of  1143 \t L: 3.190 \t -- 101.212\n",
            " 1000 of  1143 \t L: 3.204 \t -- 112.807\n",
            " 1100 of  1143 \t L: 3.208 \t -- 124.431\n",
            "Total \t L: 3.204 \t -- 129.316\n",
            " 3     0 of  4570 \t L: 1.964 \t -- 0.385\n",
            " 3     1 of  4570 \t L: 1.229 \t -- 0.706\n",
            " 3   100 of  4570 \t L: 1.544 \t -- 33.171\n",
            " 3   200 of  4570 \t L: 1.568 \t -- 65.925\n",
            " 3   300 of  4570 \t L: 1.464 \t -- 98.587\n",
            " 3   400 of  4570 \t L: 1.770 \t -- 131.394\n",
            " 3   500 of  4570 \t L: 1.706 \t -- 163.685\n",
            " 3   600 of  4570 \t L: 1.558 \t -- 196.125\n",
            " 3   700 of  4570 \t L: 1.566 \t -- 228.094\n",
            " 3   800 of  4570 \t L: 1.462 \t -- 259.906\n",
            " 3   900 of  4570 \t L: 1.585 \t -- 291.662\n",
            " 3  1000 of  4570 \t L: 1.524 \t -- 323.385\n",
            " 3  1100 of  4570 \t L: 1.822 \t -- 355.877\n",
            " 3  1200 of  4570 \t L: 1.655 \t -- 388.583\n",
            " 3  1300 of  4570 \t L: 1.379 \t -- 421.237\n",
            " 3  1400 of  4570 \t L: 1.411 \t -- 453.985\n",
            " 3  1500 of  4570 \t L: 1.502 \t -- 486.715\n",
            " 3  1600 of  4570 \t L: 1.467 \t -- 519.326\n",
            " 3  1700 of  4570 \t L: 1.586 \t -- 552.068\n",
            " 3  1800 of  4570 \t L: 1.608 \t -- 584.858\n",
            " 3  1900 of  4570 \t L: 1.559 \t -- 617.540\n",
            " 3  2000 of  4570 \t L: 1.342 \t -- 650.303\n",
            " 3  2100 of  4570 \t L: 1.497 \t -- 682.981\n",
            " 3  2200 of  4570 \t L: 1.496 \t -- 715.691\n",
            " 3  2300 of  4570 \t L: 1.614 \t -- 748.429\n",
            " 3  2400 of  4570 \t L: 1.504 \t -- 781.188\n",
            " 3  2500 of  4570 \t L: 1.578 \t -- 813.972\n",
            " 3  2600 of  4570 \t L: 1.595 \t -- 846.287\n",
            " 3  2700 of  4570 \t L: 1.483 \t -- 878.025\n",
            " 3  2800 of  4570 \t L: 1.328 \t -- 910.606\n",
            " 3  2900 of  4570 \t L: 1.552 \t -- 943.340\n",
            " 3  3000 of  4570 \t L: 1.487 \t -- 976.101\n",
            " 3  3100 of  4570 \t L: 1.286 \t -- 1008.869\n",
            " 3  3200 of  4570 \t L: 1.681 \t -- 1041.667\n",
            " 3  3300 of  4570 \t L: 1.261 \t -- 1074.263\n",
            " 3  3400 of  4570 \t L: 1.316 \t -- 1106.990\n",
            " 3  3500 of  4570 \t L: 1.715 \t -- 1139.765\n",
            " 3  3600 of  4570 \t L: 1.362 \t -- 1172.603\n",
            " 3  3700 of  4570 \t L: 1.638 \t -- 1205.316\n",
            " 3  3800 of  4570 \t L: 1.518 \t -- 1238.036\n",
            " 3  3900 of  4570 \t L: 1.395 \t -- 1270.781\n",
            " 3  4000 of  4570 \t L: 1.583 \t -- 1303.586\n",
            " 3  4100 of  4570 \t L: 1.432 \t -- 1335.833\n",
            " 3  4200 of  4570 \t L: 1.452 \t -- 1367.757\n",
            " 3  4300 of  4570 \t L: 1.429 \t -- 1399.563\n",
            " 3  4400 of  4570 \t L: 1.392 \t -- 1432.249\n",
            " 3  4500 of  4570 \t L: 1.441 \t -- 1464.018\n",
            "Saving model s2s_bart_model\n",
            "    0 of  1143 \t L: 2.877 \t -- 0.117\n",
            "  100 of  1143 \t L: 3.147 \t -- 11.194\n",
            "  200 of  1143 \t L: 3.227 \t -- 21.827\n",
            "  300 of  1143 \t L: 3.317 \t -- 32.506\n",
            "  400 of  1143 \t L: 3.292 \t -- 43.155\n",
            "  500 of  1143 \t L: 3.259 \t -- 54.198\n",
            "  600 of  1143 \t L: 3.288 \t -- 64.865\n",
            "  700 of  1143 \t L: 3.287 \t -- 76.073\n",
            "  800 of  1143 \t L: 3.283 \t -- 87.162\n",
            "  900 of  1143 \t L: 3.292 \t -- 97.794\n",
            " 1000 of  1143 \t L: 3.308 \t -- 108.599\n",
            " 1100 of  1143 \t L: 3.313 \t -- 119.270\n",
            "Total \t L: 3.309 \t -- 123.766\n",
            " 4     0 of  4570 \t L: 1.920 \t -- 0.384\n",
            " 4     1 of  4570 \t L: 0.060 \t -- 0.704\n",
            " 4   100 of  4570 \t L: 1.362 \t -- 32.510\n",
            " 4   200 of  4570 \t L: 1.387 \t -- 64.213\n",
            " 4   300 of  4570 \t L: 1.259 \t -- 96.502\n",
            " 4   400 of  4570 \t L: 1.521 \t -- 128.323\n",
            " 4   500 of  4570 \t L: 1.505 \t -- 160.672\n",
            " 4   600 of  4570 \t L: 1.302 \t -- 192.499\n",
            " 4   700 of  4570 \t L: 1.358 \t -- 224.824\n",
            " 4   800 of  4570 \t L: 1.291 \t -- 257.216\n",
            " 4   900 of  4570 \t L: 1.356 \t -- 288.991\n",
            " 4  1000 of  4570 \t L: 1.321 \t -- 321.114\n",
            " 4  1100 of  4570 \t L: 1.615 \t -- 353.115\n",
            " 4  1200 of  4570 \t L: 1.432 \t -- 384.926\n",
            " 4  1300 of  4570 \t L: 1.196 \t -- 416.655\n",
            " 4  1400 of  4570 \t L: 1.226 \t -- 448.967\n",
            " 4  1500 of  4570 \t L: 1.312 \t -- 481.837\n",
            " 4  1600 of  4570 \t L: 1.261 \t -- 514.653\n",
            " 4  1700 of  4570 \t L: 1.378 \t -- 547.460\n",
            " 4  1800 of  4570 \t L: 1.359 \t -- 580.305\n",
            " 4  1900 of  4570 \t L: 1.329 \t -- 613.032\n",
            " 4  2000 of  4570 \t L: 1.170 \t -- 645.829\n",
            " 4  2100 of  4570 \t L: 1.275 \t -- 677.871\n",
            " 4  2200 of  4570 \t L: 1.299 \t -- 710.623\n",
            " 4  2300 of  4570 \t L: 1.369 \t -- 743.398\n",
            " 4  2400 of  4570 \t L: 1.290 \t -- 776.202\n",
            " 4  2500 of  4570 \t L: 1.429 \t -- 809.049\n",
            " 4  2600 of  4570 \t L: 1.363 \t -- 841.813\n",
            " 4  2700 of  4570 \t L: 1.274 \t -- 874.521\n",
            " 4  2800 of  4570 \t L: 1.140 \t -- 907.376\n",
            " 4  2900 of  4570 \t L: 1.342 \t -- 940.137\n",
            " 4  3000 of  4570 \t L: 1.298 \t -- 972.776\n",
            " 4  3100 of  4570 \t L: 1.108 \t -- 1004.718\n",
            " 4  3200 of  4570 \t L: 1.421 \t -- 1036.573\n",
            " 4  3300 of  4570 \t L: 1.078 \t -- 1068.419\n",
            " 4  3400 of  4570 \t L: 1.162 \t -- 1100.217\n",
            " 4  3500 of  4570 \t L: 1.532 \t -- 1132.071\n",
            " 4  3600 of  4570 \t L: 1.170 \t -- 1163.972\n",
            " 4  3700 of  4570 \t L: 1.399 \t -- 1195.770\n",
            " 4  3800 of  4570 \t L: 1.322 \t -- 1227.651\n",
            " 4  3900 of  4570 \t L: 1.243 \t -- 1259.443\n",
            " 4  4000 of  4570 \t L: 1.290 \t -- 1291.613\n",
            " 4  4100 of  4570 \t L: 1.195 \t -- 1323.642\n",
            " 4  4200 of  4570 \t L: 1.252 \t -- 1355.387\n",
            " 4  4300 of  4570 \t L: 1.236 \t -- 1387.205\n",
            " 4  4400 of  4570 \t L: 1.205 \t -- 1419.961\n",
            " 4  4500 of  4570 \t L: 1.212 \t -- 1452.031\n",
            "Saving model s2s_bart_model\n",
            "    0 of  1143 \t L: 3.008 \t -- 0.118\n",
            "  100 of  1143 \t L: 3.191 \t -- 11.627\n",
            "  200 of  1143 \t L: 3.271 \t -- 22.308\n",
            "  300 of  1143 \t L: 3.361 \t -- 33.010\n",
            "  400 of  1143 \t L: 3.341 \t -- 43.705\n",
            "  500 of  1143 \t L: 3.308 \t -- 54.351\n",
            "  600 of  1143 \t L: 3.339 \t -- 65.041\n",
            "  700 of  1143 \t L: 3.340 \t -- 76.472\n",
            "  800 of  1143 \t L: 3.339 \t -- 88.204\n",
            "  900 of  1143 \t L: 3.352 \t -- 99.858\n",
            " 1000 of  1143 \t L: 3.362 \t -- 111.506\n",
            " 1100 of  1143 \t L: 3.370 \t -- 123.214\n",
            "Total \t L: 3.366 \t -- 128.126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sdw-BbC_oV",
        "outputId": "d6c2a88e-e1b9-4d43-8e17-484659381d4d"
      },
      "source": [
        "import psutil\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "        bytes /= factor\n",
        "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
        "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================== Memory Information ========================================\n",
            "Total: 25.46GB\n",
            "Available: 22.28GB\n",
            "Used: 5.57GB\n",
            "Percentage: 12.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inSpTFrw9H6s"
      },
      "source": [
        "    import torch\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RXg9GMtSs2P"
      },
      "source": [
        "We now have everything we need to answer any question! Now let's try the full system on our running example along with the first 100 questions of the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1xobu8OStdQ"
      },
      "source": [
        "# generate answer from input \"question: ... context: <p> ...\"\n",
        "def qa_s2s_generate(\n",
        "    question_doc,\n",
        "    qa_s2s_model,\n",
        "    qa_s2s_tokenizer,\n",
        "    num_answers=1,\n",
        "    num_beams=None,\n",
        "    min_len=64,\n",
        "    max_len=512,\n",
        "    do_sample=False,\n",
        "    temp=1.0,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    max_input_length=1024,\n",
        "    device=\"cuda:0\",\n",
        "):\n",
        "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n",
        "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
        "    generated_ids = qa_s2s_model.generate(\n",
        "        input_ids=model_inputs[\"input_ids\"],\n",
        "        attention_mask=model_inputs[\"attention_mask\"],\n",
        "        min_length=min_len,\n",
        "        max_length=max_len,\n",
        "        do_sample=do_sample,\n",
        "        early_stopping=True,\n",
        "        num_beams=1 if do_sample else n_beams,\n",
        "        temperature=temp,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        num_return_sequences=num_answers,\n",
        "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
        "    )\n",
        "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49FI64_uS6JZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9750fa5e-dccb-4976-9df1-3e8cd52a3e16"
      },
      "source": [
        "questions = []\n",
        "pred_answers = []\n",
        "act_answers = []\n",
        "docs = []\n",
        "\n",
        "for i in range(100):\n",
        "    # create support document with the dense index\n",
        "    question = test[i]['x']\n",
        "    act_answer = test[i]['y']\n",
        "    doc, res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer,\n",
        "        passage_snippets, doc_gpu_index, device='cuda'\n",
        "    )\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=64,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    questions += [question]\n",
        "    pred_answers += [answer]\n",
        "    act_answers += [act_answer]\n",
        "    docs += [doc]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Question': questions,\n",
        "    'Predicted Answer': pred_answers,\n",
        "    'Actual Answer' :  act_answers,\n",
        "    'Documents' :docs\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col3,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col0,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col1,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col2,#T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col3{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Question</th>        <th class=\"col_heading level0 col1\" >Predicted Answer</th>        <th class=\"col_heading level0 col2\" >Actual Answer</th>        <th class=\"col_heading level0 col3\" >Documents</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col0\" class=\"data row0 col0\" >\"exp_cuda\" not implemented for 'ComplexDouble'</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col1\" class=\"data row0 col1\" >You need to overwrite the reference with a new CUDA tensor:\n",
              "python\n",
              "input = input.cuda()\n",
              "model(Variable(input))\n",
              "\n",
              "Let's investigate supporting this once #6688 is merged. If you want this get prioritized, please just say so. There is no need to blame devs on working on other changes that, while may be BC breaking, are also and may be more important.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col2\" class=\"data row0 col2\" >It is fixed. Works on latest master.\n",
              " \n",
              " python\n",
              " \n",
              " >>> import torch\n",
              " \n",
              " >>> torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
              " \n",
              " >>> tt = torch.Tensor([1])\n",
              " \n",
              " >>> torch.exp(1j*tt)\n",
              " \n",
              " tensor([0.5403+0.8415j])\n",
              " \n",
              " </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row0_col3\" class=\"data row0 col3\" ><P> Thanks for catching this! We would accept a PR to fix this. <P> OK, it is indeed a race. This diff, for example, \"fixes\" it:\n",
              " \n",
              " \n",
              " \n",
              " diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py\n",
              " \n",
              " index 411cfb7315..5abb645fc5 100644\n",
              " \n",
              " --- a/torch/cuda/__init__.py\n",
              " \n",
              " +++ b/torch/cuda/__init__.py\n",
              " \n",
              " @@ -165,6 +165,7 @@ def _lazy_init():\n",
              " \n",
              "  global _initialized, _cudart, _original_pid, _queued_calls\n",
              " \n",
              "  if _initialized:\n",
              " \n",
              "  return\n",
              " \n",
              " + _initialized = True\n",
              " \n",
              "  if _in_bad_fork:\n",
              " \n",
              "  from sys import version_info\n",
              " \n",
              "  if version_info < (3, 4):\n",
              " \n",
              " @@ -181,7 +182,6 @@ def _lazy_init():\n",
              " \n",
              "  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n",
              " \n",
              "  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n",
              " \n",
              "  _original_pid = os.getpid()\n",
              " \n",
              " - _initialized = True\n",
              " \n",
              "  # Important to do this after _initialized, since some queued calls\n",
              " \n",
              "  # may themselves call _lazy_init()\n",
              " \n",
              "  for queued_call, orig_traceback in _queued_calls:\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " If you look at the relevant segment:\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              "  torch._C._cuda_init()\n",
              " \n",
              "  _cudart = _load_cudart()\n",
              " \n",
              "  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n",
              " \n",
              "  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n",
              " \n",
              "  _original_pid = os.getpid()\n",
              " \n",
              "  _initialized = True\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " The problem is that `_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`. <P> If you look into the list of types in the `got (...)` part, you'll find a mix of CPU and CUDA tensors, with `input` and `output` being on CPU, while `weight` and `bias` is on the GPU.\n",
              "You probably forgot to send the input to the GPU. Alternatively, keep in mind that `.cuda()` is an out of place operation i.e.\n",
              "python\n",
              "input.cuda()\n",
              "model(Variable(input))\n",
              "\n",
              "will fail. You need to overwrite the reference with a new CUDA tensor:\n",
              "python\n",
              "input = input.cuda()\n",
              "model(Variable(input))\n",
              " <P> Let's investigate supporting this once #6688 is merged.\n",
              " \n",
              " \n",
              " \n",
              " @mario98 histogram on GPU is really difficult. If you want this get prioritized, please just say so. There is no need to blame devs on working on other changes that, while may be BC breaking, are also and may be more important. <P> @soumith Thanks a lot! `cudaDeviceReset()` can release the resource associated with the current process. <P> @albanD Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes. <P> repeating a message from Olexa on slack, closing this as wontfix::\n",
              "\n",
              "\n",
              "@Amir, when I wrote in #general that CUDA is totally incompatible with forking, I meant this in the most serious and literal sense of those words possible.\n",
              "\n",
              "This simply *cannot* be fixed in CUDA, PyTorch or anyone else. The problems inherent in `fork()`'ing *any* _multithreaded_ program are fundamentally unsolvable, and simply beyond the power of anyone to fix, at least not until a revolution in OS design happens.\n",
              "\n",
              "I can only plead that you accept the general difficulty of safely forking within a multithreaded program and try to do things another way.\n",
              "\n",
              "There's several blogs out there discussing why it's dangerous to fork in a multithreaded program, such as https://thorstenball.com/blog/2014/10/13/why-threads-cant-fork/\n",
              "\n",
              "The gist of it is that when a thread `fork()`'s, *in the child process, all other threads _die instantly._* It doesn't matter what they were doing, they're _gone_.\n",
              "\n",
              "- If they had locked a mutex, that mutex will never be unlocked again.\n",
              "- If they were modifying a data structure, that data structure might be invalid.\n",
              "- If they `malloc()`'ed some memory, that memory might never be deallocated, and `malloc()` may use locks and data structures anyways.\n",
              "- If a thread was doing useful work, that work will never be complete, because the threads no longer exist.\n",
              "- You can't join these non-existent threads.\n",
              "- `pthread_atfork()` is a function meant to solve the problems above, but it's simply incapable of doing what it was meant to do safely, and that's why the POSIX.1 standard explicitly says that this function may be formally deprecated in the next version of the standard. It was a mistake.\n",
              "\n",
              "So almost the only safe thing to do if you `fork()`'ed from a multi-threaded process is to call `exec()`. That's what `spawn` does.\n",
              "\n",
              "Because the CUDA runtime uses threads to implement its runtime and asynchronous streams, once the CUDA runtime is initialized, it's insanely dangerous to `fork()`. Don't do it.\n",
              " <P> Is anyone working on this, I would be happy take this up with some guidance. <P> Hi, I am from @quansight team. I would like to work on this issue. <P> Issue is still present on `pytorch==1.3.1`\n",
              "\n",
              "To fix, replace the following:\n",
              "`X = X.to(device)`\n",
              "With this:\n",
              "`X = X.to(device=device)`\n",
              "Provided that \"X\" is a packed sequence.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col0\" class=\"data row1 col0\" >How to correctly use CTC Loss with GRU in pytorch?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col1\" class=\"data row1 col1\" >I suspect your issue has to do with your outputs / data[1] (it would help if you show examples of your train_set). Running the following piece of code gives no NaN, but I forced shape of output by hand before calling the loss_fn(pred, outputs) :\n",
              "\n",
              "class BaselineModel(nn.Module):\n",
              "    def __init__(self, feature_dim=5, hidden_size=3, num_layers=2, batch size=32):\n",
              " super(BaselineModel, self).__init__()\n",
              "\n",
              "self.num_l</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col2\" class=\"data row1 col2\" >Your model predicts 28 classes, therefore the output of the model has size [batch_size, seq_len, 28] (or [seq_len, batch_size, 28] for the log probabilities that are given to the CTC loss). In the nn.CTCLoss you set blank=28, which means that the blank label is the class with index 28. To get the log probabilities for the blank label you would index it as output[:, :, 28], but that doesn't work, because that index is out of range, as the valid indices are 0 to 27.\n",
              "\n",
              "The last class in your output is at index 27, hence it should be blank=27:\n",
              "\n",
              "criterion = nn.CTCLoss(blank=27, zero_infinity=False)\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row1_col3\" class=\"data row1 col3\" ><P> I suspect your issue has to do with your outputs / data[1] (it would help if you show examples of your train_set). Running the following piece of code gives no nan, but I forced shape of output by hand before calling the loss_fn(pred, outputs) :\n",
              "\n",
              "class BaselineModel(nn.Module):\n",
              "    def __init__(self, feature_dim=5, hidden_size=5, num_layers=2, batch_size=32):\n",
              "        super(BaselineModel, self).__init__()\n",
              "        self.num_layers = num_layers\n",
              "        self.hidden_size = hidden_size\n",
              "\n",
              "        self.lstm = nn.LSTM(input_size=feature_dim,\n",
              "                            hidden_size=hidden_size, num_layers=num_layers)\n",
              "\n",
              "    def forward(self, x, hidden):\n",
              "        lstm_out, hidden = self.lstm(x, hidden)\n",
              "        return lstm_out, hidden\n",
              "\n",
              "    def init_hidden(self, batch_size):\n",
              "        hidden = Variable(next(self.parameters()).data.new(\n",
              "            self.num_layers, batch_size, self.hidden_size))\n",
              "        cell = Variable(next(self.parameters()).data.new(\n",
              "            self.num_layers, batch_size, self.hidden_size))\n",
              "        return (hidden, cell)\n",
              "\n",
              "model = BaselineModel(batch_size=32)\n",
              "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
              "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
              "\n",
              "hidden = model.init_hidden(10)\n",
              "model.zero_grad()\n",
              "pred, hidden = model(torch.randn(2,10,5), hidden)\n",
              "pred.size() #torch.Size([2, 10, 5])\n",
              "outputs = torch.zeros(2,10,5)\n",
              "\n",
              "loss = loss_fn(pred, outputs)\n",
              "loss\n",
              "\n",
              "loss.backward()\n",
              "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
              "optimizer.step()\n",
              "print(loss)\n",
              "\n",
              "\n",
              "Please note a common reason for nan values can be related to numerical stability of your learning phase, but usually you have values for the first steps before you see the divergence happening, which is apparently not the case here.\n",
              " <P> This exception is raised because the objetive function from your study must return a float.\n",
              "In your case, the problem is in this line:\n",
              "study.optimize(autotune, n_trials=1)\n",
              "\n",
              "The autotune function you defined before does not return a value and cannot be used for optimization.\n",
              "How to fix?\n",
              "For hyperparameter search, the autotune function must return the either some metric you can get after some training - like the loss or cross-entropy.\n",
              "A quick fix on your code could be something like this:\n",
              "def autotune():\n",
              "  cfg= { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
              "        ...etc...\n",
              "       }\n",
              "\n",
              "  best_loss = 1e100;  # or larger\n",
              "\n",
              "  # Train the model\n",
              "  for _ in range(epochs):\n",
              "     for i, (data, labels) in enumerate(trainloader):\n",
              "        ... (train the model) ...\n",
              "        # compute loss/error by comparing predicted out vs actual labels\n",
              "        loss = criterion(y_pred, labels)\n",
              "        best_loss = min(loss,best_loss)\n",
              "\n",
              "  return best_loss\n",
              "\n",
              "There is a good example with Pythorch in the Optuna repo that uses a pythoch callback to retrieve the accuracy (but can be changed easily to use the RMSE if needed). It also uses more than one experiment and takes the median for hyperparameters.\n",
              " <P> I belive the error lies in the following line:\n",
              "y_pred = (y_pred  0).float().requires_grad_()\n",
              "\n",
              "You try to binarize the model prediction in a weird way, I suggest do the following instead:\n",
              "y_pred = torch.sigmoid(y_pred)\n",
              "\n",
              "And pass this to the loss function.\n",
              "Explanation\n",
              "The output of the model can be any value, but we want to normalize that values to reside in the [0,1] range. This is exactly what the sigmoid function does. Once we have the values in the range of [0,1] the comparison with the binary labels will make sense, closer to 1 will be \"1\" and the opposite.\n",
              "You can refer to the following link: https://www.youtube.com/watch?v=WsFasV46KgQ\n",
              " <P> This problem seams to me like instead you define F:\n",
              "\n",
              "import torch.nn.functional as F\n",
              "\n",
              "\n",
              "You in accident have set F to some tuple\n",
              "\n",
              "F=(1,2)\n",
              "\n",
              "\n",
              "And then when you call F.log_softmax you get exactly this error.\n",
              "\n",
              "\n",
              " <P> > See also: #2209\n",
              "> \n",
              "> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\n",
              "\n",
              "I got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\n",
              "\n",
              "and I tried to use `nn.BCELoss()` like:\n",
              "\n",
              "\n",
              "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
              "criterion = nn.BCELoss()\n",
              "\n",
              "\n",
              "_loop epoch train part:_\n",
              "\n",
              "\n",
              "prediction = model(batch_input)\n",
              "loss = criterion(torch.sigmoid(prediction), label)\n",
              "\n",
              "optimizer.zero_grad()\n",
              "loss.backward()\n",
              "optimizer.step()\n",
              "\n",
              "\n",
              "Then I solved the problem. Thank you for your comment!\n",
              "(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\n",
              " <P> This is something that happens when you use PyTorch inside fastai (I believe this should be fixed).\n",
              "Just create custom loss_func. For example:\n",
              "def loss_func(output, target): return CrossEntropyLossFlat()(out, targ.long())\n",
              "\n",
              "and pass it when creating the DataBlock:\n",
              "dblock = DataBlock(... , loss_func=loss_func, ...)\n",
              "\n",
              " <P> I was also very surprised of this issue. Although I have never used the library I went down and did some debugging and found out that the issue is coming from the library transformers. The problem is comming from from this line :\n",
              "encoder_states = tuple(hidden_state.transpose(0, 1) for hidden_state in encoder_states)\n",
              "\n",
              "If you comment it out, you will get the gradient just with some dimensions transposed.\n",
              "This issue is related to the fact that Pytorch Autograd does not do very well on inplace operations as mentioned here.\n",
              "So to recap the solution is to comment line 382 in modeling_bart.py.\n",
              "You will get the gradient with this shape T x B x C instead of  B x T x C, but you can reshape it as you want later.\n",
              " <P> Change the criterion call to:\n",
              "age_loss, gender_loss, race_loss = criterion(output, age.float(), gender, race)\n",
              "\n",
              "If you look at your error we can trace it to:\n",
              "frame #3: at::native::smooth_l1_loss_backward_out\n",
              "\n",
              "In the MultiLoss Class, the smooth_l1_loss works with age. So I changed it's type to float (as the expected dtype is Float) while passing it to the criterion. You can check that age is torch.int64 (i.e. torch.long) by printing age.dtype\n",
              "I am not getting the error after doing this. Hope it helps.\n",
              " <P> The negative log likelihood loss (NLLLoss) is suitable for classification problems, where the output is one out of C classes. Since the classes are discrete, your labels need to be of the long type.\n",
              "In your case, in a comment, you say:\n",
              "\n",
              "I want to create a network that simulates a quadratic function with x as input and sth similar to x**2 as output.\n",
              "\n",
              "This is a regression problem, where the output can have a real, continuous value. For this, you should use a suitable loss function such as the mean squared error loss (MSELoss). So, one way to fix would be changing F.nll_loss in your code to F.mse_loss.\n",
              " <P> Defake:loss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size)) ,I think you need to do criterion_test(dec_outs.view(-1, vocab_size),targets.view(-1)) In your case, ( C )-   vocab_size and (N)-   (batch_size*seq_length). I am assuming all the batches have the same sequence length. If not, youll have to use pack_padded_sequence and also mask the loss for the pad token.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col0\" class=\"data row2 col0\" >glibc error while importing torch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col1\" class=\"data row2 col1\" >For the record, the problem was:\n",
              " - In a conda environment, I installed pytorch with `conda install`(as described on PyTorch web site) and fastText with `pip install.` from their git clone.\n",
              " - That resulted in a segfault when doing `import fastText` and `import torch` \n",
              "Reason:\n",
              "\n",
              " - Pytorch is compiled with gcc 4.9.2\n",
              " - Conda's default gcc is 4.8.5\n",
              "Fix:\n",
              "- install gcc-4.9 in conda (e.g. `</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col2\" class=\"data row2 col2\" >I am also having this issue. I know that the version of GLIBC in my conda environment is up to date (version 2.55) but pytorch seems to be using the global GLIBC on my machine (version 2.12). how can I configure pytorch so that it uses the correct GLIBC installation?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row2_col3\" class=\"data row2 col3\" ><P> For the record, the problem was:\n",
              " - in a conda environment, I installed pytorch with `conda install`(as described on pytorch web site) and fastText with `pip install .` from their git clone.\n",
              " - that resulted in a segfault when doing `import fastText` and `import torch` \n",
              "Reason:\n",
              " - pytorch is compiled with gcc 4.9.2\n",
              " - conda's default gcc is 4.8.5\n",
              "Fix:\n",
              " - install gcc-4.9 in conda (e.g. `conda install -c serge-sans-paille gcc_49`)\n",
              " - install pytorch with `conda install` (in my case, `conda install pytorch torchvision cuda90 -c pytorch`)\n",
              " - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone\n",
              "That's it! Thanks a lot @weiyangfb and @SsnL for your help! <P> Update conda first with `conda update conda` and try again <P> `brew install libomp` solves the problem. <P> i think\n",
              "\n",
              "$ brew install libomp\n",
              "\n",
              "can help u, cause i solve the same problem by it.  \n",
              "\n",
              "according\n",
              "\n",
              "github-issue-\"libomp.dylib can't be loaded\"\n",
              " <P> I encounter the same error, and I solve it by running command 'conda install mkl' in my activated conda env.\n",
              "> File \\\"/usr/local/lib/python3.5/site-packages/torch/__init__.py\\\", line 45, in \n",
              "> from torch._C import *\n",
              "> ImportError: dlopen(/usr/local/lib/python3.5/site-packages/torch/_C.cpython-35m-darwin.so, 10): Library not loaded: @rpath/libmkl_intel_lp64.dylib\n",
              "> Referenced from: /usr/local/lib/python3.5/site-packages/torch/lib/libTH.1.dylib\n",
              "> Reason: image not found <P> We had the same issue. Problem was the version of MKL which was not good. Just running:\n",
              "\n",
              "conda remove mkl mkl-include\n",
              "conda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\n",
              "\n",
              "before the installation fixed the issue. <P> Probably duplicate of #20030\n",
              "try `brew install libomp ` <P> I had the same error. I fixed it using: system('env -i /usr/bin/python3 -c \"import torch\"').\n",
              " \n",
              " Some environment variable set by matlab is messing with pytorch, but I don't know which. <P> Hi @malfet,\n",
              " \n",
              " \n",
              " \n",
              " I've just been giving the PyTorch 1.8 whl a go. I `pip install`ed `torch-1.8.0-cp36-cp36m-manylinux2014_aarch64.whl` on a CentOS 8.3.2011 machine, but I'm getting the following error:\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " OSError: /lib64/python3.6/site-packages/torch/lib/libtorch_global_deps.so: ELF load command alignment not page-aligned\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " This appears to be an issue previously encountered with NumPy ManyLinux builds (https://github.com/numpy/numpy/issues/16677) where the whl was built with the wrong pagesize.\n",
              " \n",
              " \n",
              " \n",
              " The same issue appears to affect the nightly .whls from https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html too.\n",
              " \n",
              " \n",
              " \n",
              " Ubuntu 20.10 with Python 3.8.6 appears to work as expected. <P> If you are not using conda, pip install mkl-devel gets you the headers. I haven't looked at your CI script. @SsnL can we add this to the error message?\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col0\" class=\"data row3 col0\" >TypeError: add(): argument &#39;other&#39; (position 1) must be Tensor, not numpy.ndarray</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col1\" class=\"data row3 col1\" >This is not a cmake-related error, it's just how the library was implemented. I do not know why, but it appears that the specialization of T* at::Tensor::dataT const with T = long long was forgotten/omitted.\n",
              "If you want to get your signed 64-bits pointer, you can still get it with int64_t:\n",
              "auto data = T.DataT();\n",
              "It's good practice to use these types for which the size is explicit in general, in order to avoid compatibility issues.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col2\" class=\"data row3 col2\" >I have had the same issue with this Kaggle kernel. My workarounds are the following:\n",
              "\n",
              "1st option: In the F1 __call__ method convert preds and targs from pytorch tensors to numpy arrays;\n",
              "\n",
              "2nd option: Initialise TP/FP/FN with pytorch tensors instead of numpy arrays, i.e. replace np.zeros(self.n) with torch.zeros(1, self.n).\n",
              "\n",
              "Basically, the main idea - all variables should be of the same type.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row3_col3\" class=\"data row3 col3\" ><P> The error message says it all. The tensors involved contain elements of different data types. By default, w and b have elements of type torch.float32, while data_input is a NumPy array with the Python default floating point type, i.e. double. That datatype will be preserved when you convert with from_numpy. Try using dtype=np.float32 in your np.genfromtxt call.\n",
              " <P> If I understood your question correctly, you have a python list and want to convert to a pytorch tensor.\n",
              "python_list = [[1,2,3], [4, 5, 6]]\n",
              "\n",
              "torch_list = torch.tensor(python_list)\n",
              "\n",
              "Can you point out where you are getting the error in the code?\n",
              "Yes, that’s right.\n",
              "In #3,\n",
              "X = torch.FloatTensor(data)\n",
              "This is where I’m getting the error. I have also tried X = torch.Tensor(data) , but ended up with the same error.\n",
              "Hi!\n",
              "I have mistaken list of tensor as a simple python list. (So I have also changed the title of question.)\n",
              "The variable ‘data’ was actually a list of tensors with only one item. And I can’t create a tensor from a list of tensors using torch.Tensor() method. Hence the error.\n",
              "I used the below method to turn the list of tensor into a single tensor with link \"https://discuss.pytorch.org/t/how-to-turn-a-list-of-tensor-to-tensor/8868\" :\n",
              "X = torch.stack(data)\n",
              "And it works now. Thanks!  <P> I would suggest you to check the input type\n",
              "I had the same issue which solved by converting the input type from  int32 to int64.(running on win10)\n",
              "ex:\n",
              "\n",
              "x = torch.tensor(train).to(torch.int64)\n",
              "\n",
              " <P> I encounter the same error. Here's the solution. \n",
              "\n",
              "You should change the type of input from float64 to float32, which means you should type:\n",
              "\n",
              "input_seq = input_seq.float()\n",
              "\n",
              " <P> This is not a cmake-related error, it's just how the library was implemented. I do not know why, but it appears that the specialization of T* at::Tensor::dataT const with T = long long was forgotten/omitted.\n",
              "If you want to get your signed 64-bits pointer, you can still get it with int64_t:\n",
              "auto data = T.dataint64_t();\n",
              "\n",
              "It's good practice to use these types for which the size is explicit in general, in order to avoid compatibility issues.\n",
              " <P> The error is because k tensor is of dtype torch.int32 while other tensors P and d_k are of dtype torch.float32. But the cat operation requires all the input tensors to be of same type. From the documentation\n",
              "\n",
              "\n",
              "  torch.cat(tensors, dim=0, out=None) → Tensor\n",
              "  \n",
              "  tensors (sequence of Tensors) – any python sequence of tensors of the\n",
              "  same type.\n",
              "\n",
              "\n",
              "One of the solutions is to convert k to float dtype as follows:\n",
              "\n",
              "k = torch.from_numpy(k).float()\n",
              "\n",
              " <P> As the error message explains, c is a tensor. To use torch.cat() you must pass a group of tensors or a list. To solve your problem you may use:\n",
              "\n",
              "temp = list()\n",
              "for key, b in reader:\n",
              "    temp.append(torch.from_numpy(b))\n",
              "labels = torch.cat(temp)\n",
              "\n",
              "\n",
              "For more, you can check the manual here\n",
              "\n",
              "Cheers\n",
              " <P> I had same issue\n",
              "resolved\n",
              "Before converting to Tensor, try this\n",
              "X_train = X_train.astype(np.float32)\n",
              "\n",
              " <P> AFAIK, the tensor is basically a NumPy array bound to the device. If not too expensive for your application and you can afford to do it on CPU, you can simply convert it to NumPy and do what you need with the comparison.\n",
              " <P> It is converting; torch.float64 and torch.double are the same thing\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col0\" class=\"data row4 col0\" >Get a 10x10 patch from a 100x100 pytorch tensor with torus style wrap around the boundries</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col1\" class=\"data row4 col1\" >In pytorch, the nn.Conv2d module needs the data to be in float. You can just make a simple edit:\n",
              "\n",
              "x = torch.randint(500,(256,), dtype=torch.float32)\n",
              "\n",
              "\n",
              "Alternatively you can also do:\n",
              "\n",
              "\n",
              "x = print(x)\n",
              "x[0] = x[0].view(-1)\n",
              "X = x.float()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col2\" class=\"data row4 col2\" >torch.roll\n",
              "def random_patch(a, size) - Tensor:\n",
              "  shifts = np.random.randint(low = 0, high = a.size())\n",
              "  return torch.roll(a, shifts=shifts, dims=(0, 1))[:size[0], :size[1]]\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row4_col3\" class=\"data row4 col3\" ><P> In pytorch, the nn.Conv2d module needs the data to be in float. You can just make a simple edit:\n",
              "\n",
              "x = torch.randint(500,(256,), dtype=torch.float32)\n",
              "\n",
              "\n",
              "Alternatively you can also do:\n",
              "\n",
              "x = torch.randint(500,(256,))\n",
              "x = x.float()\n",
              "\n",
              " <P> i got it!Instead of using the for loop for the whole matrix, i just have to use it for the block_matrix size####a=torch.ones(8, 8)####print(a)####block_size=2####for i in range(block_size):####a[i: : 4]=2*a[i: : 4] #### print(a) <P> It is actually very simple : torch.nn.functional.conv2d !\n",
              " <P> Check out the example on the tensorflow doc page (link):\n",
              "\n",
              "im1 = tf.decode_png('path/to/im1.png')\n",
              "im2 = tf.decode_png('path/to/im2.png')\n",
              "print(tf.image.ssim(im1, im2, max_val=255))\n",
              "\n",
              "\n",
              "This should work on latest version of tensorflow. If you use older versions tf.image.ssim will return a tensor (print will not give you a value), but you can call .run() to evaluate it.\n",
              " <P> contiguous is a torch method, it has to be a torch tensor not a numpy array <P> Got it.\n",
              "Just create a tensor using :\n",
              "- torch.zeros(batch_size,100,y,dtype = embedded.dtype,device = embedded.device)\n",
              " <P> Try looking into this, if you have to edit package called onnx-caffe2 to add the mapping b/w Unsqueeze to ExpandDims\n",
              "https://github.com/onnx/onnx/issues/1481\n",
              "\n",
              "Look for the answer:\n",
              "\n",
              "I found that the Caffe2 equivalence for Unsqueeze in ONNX is ExpandDims, and there is a special mapping in onnx_caffe2/backend.py around line 121 for those operators that are different only in their names and attribute names, but somehow Unsqueeze isn't presented there (have no idea why). So I manually added the mapping rules for it in the _renamed_operators and _per_op_renamed_attrs dicts and the code would look like:\n",
              "\n",
              "_renamed_operators = {\n",
              "    'Caffe2ConvTranspose':   'ConvTranspose',\n",
              "    'GlobalMaxPool':         'MaxPool',\n",
              "    'GlobalAveragePool':     'AveragePool',\n",
              "    'Pad':                   'PadImage',\n",
              "    'Neg':                   'Negative',\n",
              "    'BatchNormalization':    'SpatialBN',\n",
              "    'InstanceNormalization': 'InstanceNorm',\n",
              "    'MatMul':                'BatchMatMul',\n",
              "    'Upsample':              'ResizeNearest',\n",
              "    'Equal':                 'EQ',\n",
              "    'Unsqueeze':             'ExpandDims',  # add this line\n",
              "}\n",
              "\n",
              "_global_renamed_attrs = {'kernel_shape': 'kernels'}\n",
              "_per_op_renamed_attrs = {\n",
              "    'Squeeze':              {'axes': 'dims'},\n",
              "    'Transpose':            {'perm': 'axes'},\n",
              "    'Upsample':             {'mode': ''},\n",
              "    'Unsqueeze':            {'axes': 'dims'},  # add this line\n",
              "}\n",
              "\n",
              "\n",
              "And everything works as expected.\n",
              "\n",
              "I am not the OP, thanks to OP though.\n",
              " <P> You may just add .to(torch.float32) to your train_x and val_x tensors\n",
              " <P> Static linking libtorch doesn’t work well yet, please see, with link https://github.com/pytorch/pytorch/issues/21737 <P> As you can see matplotlib works fine even without conversion to numpy array. But PyTorch Tensors (\"Image tensors\") are channel first, so to use them with matplotlib you need to reshape it:\n",
              "\n",
              "Code:\n",
              "\n",
              "from scipy.misc import face\n",
              "import matplotlib.pyplot as plt\n",
              "import torch\n",
              "\n",
              "np_image = face()\n",
              "print(type(np_image), np_image.shape)\n",
              "tensor_image = torch.from_numpy(np_image)\n",
              "print(type(tensor_image), tensor_image.shape)\n",
              "# reshape to channel first:\n",
              "tensor_image = tensor_image.view(tensor_image.shape[2], tensor_image.shape[0], tensor_image.shape[1])\n",
              "print(type(tensor_image), tensor_image.shape)\n",
              "\n",
              "# If you try to plot image with shape (C, H, W)\n",
              "# You will get TypeError:\n",
              "# plt.imshow(tensor_image)\n",
              "\n",
              "# So we need to reshape it to (H, W, C):\n",
              "tensor_image = tensor_image.view(tensor_image.shape[1], tensor_image.shape[2], tensor_image.shape[0])\n",
              "print(type(tensor_image), tensor_image.shape)\n",
              "\n",
              "plt.imshow(tensor_image)\n",
              "plt.show()\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "class 'numpy.ndarray' (768, 1024, 3)\n",
              "class 'torch.Tensor' torch.Size([768, 1024, 3])\n",
              "class 'torch.Tensor' torch.Size([3, 768, 1024])\n",
              "class 'torch.Tensor' torch.Size([768, 1024, 3])\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col0\" class=\"data row5 col0\" >`F.logsigmoid(input, out=blah)` crashes</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col1\" class=\"data row5 col1\" >This issue looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken. This was fixed at some point, we forgot to close the issue. <P>Oh, sorry, I read the repro completely wrong, there is a PackedSequence and it's being created with a CUDA tensor. Thank you for fact checking me, @ailzhang! If we're confident that the above example doesn't segfault without PackedSence, then we should close this.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col2\" class=\"data row5 col2\" >`log_softmax` and `log_sigmoid` used underscores consistently</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row5_col3\" class=\"data row5 col3\" ><P> Please shout if you have this problem, so I can prioritize it accordingly. <P> Thank you for reporting this issue, @shmsong. The team agrees it's a high priority. <P> This looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken. <P> this was fixed at some point, we forgot to close the issue. <P> Oh, sorry, I read the repro completely wrong, there is a PackedSequence and it's being created with a CUDA tensor. Thank you for fact checking me, @ailzhang! If we're confident that the above example doesn't segfault without PackedSequence, then we should close this. <P> Oh sorry I see it has been fixed. <P> thanks for the bug report, we'll fix this. <P> That makes sense. I'll send a fix. <P> I think this has been fixed on `master`. <P> Thanks for catching this! We would accept a PR to fix this.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col0\" class=\"data row6 col0\" >How to understand creating leaf tensors in PyTorch?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col1\" class=\"data row6 col1\" >This is a bug in PyTorch, because after this a.requires_grad is still True.\n",
              "\n",
              "a = torch.tensor([1.])\n",
              "a requires_grad = True\n",
              "a.reshape(a.shape)\n",
              "A.retain_grad()\n",
              "b = torch.(a.T.)\n",
              "c = a.cat([a,b])\n",
              "d = torch(c)\n",
              "d.backward()\n",
              "\n",
              "\n",
              "Old answer:\n",
              "\n",
              "If you move a.Requires_grad >= True after the reshape, it works:\n",
              "\n",
              "\n",
              "a =torch.Tensor([</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col2\" class=\"data row6 col2\" >When a tensor is first created, it becomes a leaf node.\n",
              "Basically, all inputs and weights of a neural network are leaf nodes of the computational graph.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row6_col3\" class=\"data row6 col3\" ><P> Edit:\n",
              "\n",
              "Here is a detailed explanation of what's going on (\"this isn't a bug per se, but it is definitely a source of confusion\"): https://github.com/pytorch/pytorch/issues/19778\n",
              "\n",
              "So one solution is to specifically ask to retain grad for now non-leaf a:\n",
              "\n",
              "a = torch.tensor([1.])\n",
              "a.requires_grad = True\n",
              "a = a.reshape(a.shape)\n",
              "a.retain_grad()\n",
              "b = torch.tensor([1.])\n",
              "c = torch.cat([a,b])\n",
              "d = torch.sum(c)\n",
              "d.backward()\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Old answer:\n",
              "\n",
              "If you move a.requires_grad = True after the reshape, it works:\n",
              "\n",
              "a = torch.tensor([1.])\n",
              "a = a.reshape(a.shape)\n",
              "a.requires_grad = True\n",
              "b = torch.tensor([1.])\n",
              "c = torch.cat([a,b])\n",
              "d = torch.sum(c)\n",
              "d.backward()\n",
              "\n",
              "\n",
              "Seems like a bug in PyTorch, because after this a.requires_grad is still true.\n",
              "\n",
              "a = torch.tensor([1.])\n",
              "a.requires_grad = True\n",
              "a = a.reshape(a.shape)\n",
              "\n",
              "\n",
              "This seems to be related to the fact the a is no longer a leaf in your \"Doesn't work\" example, but still a leaf in other cases (print a.is_leaf to check).\n",
              " <P> detach()\n",
              "One example without detach():\n",
              "from torchviz import make_dot\n",
              "x=torch.ones(2, requires_grad=True)\n",
              "y=2*x\n",
              "z=3+x\n",
              "r=(y+z).sum()    \n",
              "make_dot(r)\n",
              "\n",
              "\n",
              "The end result in green r is a root of the AD computational graph and in blue is the leaf tensor.\n",
              "Another example with detach():\n",
              "from torchviz import make_dot\n",
              "x=torch.ones(2, requires_grad=True)\n",
              "y=2*x\n",
              "z=3+x.detach()\n",
              "r=(y+z).sum()    \n",
              "make_dot(r)\n",
              "\n",
              "\n",
              "This is the same as:\n",
              "from torchviz import make_dot\n",
              "x=torch.ones(2, requires_grad=True)\n",
              "y=2*x\n",
              "z=3+x.data\n",
              "r=(y+z).sum()    \n",
              "make_dot(r)\n",
              "\n",
              "But, x.data is the old way (notation), and x.detach() is the new way.\n",
              "What is the difference with x.detach()\n",
              "print(x)\n",
              "print(x.detach())\n",
              "\n",
              "Out:\n",
              "tensor([1., 1.], requires_grad=True)\n",
              "tensor([1., 1.])\n",
              "\n",
              "So x.detach() is a way to remove requires_grad and what you get is a new detached tensor (detached from AD computational graph).\n",
              "torch.no_grad\n",
              "torch.no_grad is actually a class.\n",
              "x=torch.ones(2, requires_grad=True)\n",
              "with torch.no_grad():\n",
              "    y = x * 2\n",
              "print(y.requires_grad)\n",
              "\n",
              "Out:\n",
              "False\n",
              "\n",
              "From help(torch.no_grad):\n",
              "\n",
              "Disabling gradient calculation is useful for inference, when you are sure\n",
              "|  that you will not call :meth:Tensor.backward(). It will reduce memory\n",
              "|  consumption for computations that would otherwise have requires_grad=True.\n",
              "|\n",
              "|  In this mode, the result of every computation will have\n",
              "|  requires_grad=False, even when the inputs have requires_grad=True.\n",
              "\n",
              " <P> There is no \"one shot instruction\" to switch .requires_grad for all tensors in graph.\n",
              "Usually parameters are kept in torch.nn.Module instances but in case they are elsewhere, you can always add them to some list and iterate over it, I'd do something like this:\n",
              "import torch\n",
              "\n",
              "\n",
              "class Leafs:\n",
              "    def __init__(self):\n",
              "        self.leafs = []\n",
              "\n",
              "    def add(self, tensor):\n",
              "        self.leafs.append(tensor)\n",
              "        return tensor\n",
              "\n",
              "    def clear(self):\n",
              "        for leaf in self.leafs:\n",
              "            leaf.requires_grad_(False)\n",
              "\n",
              "\n",
              "keeper = Leafs()\n",
              "\n",
              "x = keeper.add(torch.tensor([1.2], requires_grad=True))\n",
              "y = keeper.add(torch.tensor([1.3], requires_grad=True))\n",
              "\n",
              "print(x.requires_grad, y.requires_grad)\n",
              "\n",
              "keeper.clear()\n",
              "\n",
              "print(x.requires_grad, y.requires_grad)\n",
              "\n",
              "Usually there is no need for that, also if you don't want gradient for some part of computation you can always use with torch.no_grad() context manager.\n",
              " <P> features = torch.rand(1, 5) \n",
              "weights = torch.Tensor([1, 2, 3, 4, 5])\n",
              "print(features)\n",
              "print(weights)\n",
              "\n",
              "# Element-wise multiplication of shape (1 x 5)\n",
              "# out = [f1*w1, f2*w2, f3*w3, f4*w4, f5*w5]\n",
              "print(features*weights)\n",
              "\n",
              "# weights has been reshaped to (5, 1)\n",
              "# Element-wise multiplication of shape (5 x 5)\n",
              "# out =   [f1*w1, f2*w1, f3*w1, f4*w1, f5*w1]\n",
              "#         [f1*w2, f2*w2, f3*w2, f4*w2, f5*w2]\n",
              "#         [f1*w3, f2*w3, f3*w3, f4*w3, f5*w3]\n",
              "#         [f1*w4, f2*w4, f3*w4, f4*w4, f5*w4]\n",
              "#         [f1*w5, f2*w5, f3*w5, f4*w5, f5*w5]\n",
              "print(features*weights.view(5, 1))\n",
              "\n",
              "# Matrix-multiplication\n",
              "# (1, 5) * (5, 1) - (1, 1)\n",
              "# out = [f1*w1 + f2*w2 + f3*w3 + f4*w4 + f5*w5]\n",
              "print(torch.mm(features, weights.view(5, 1)))\n",
              "\n",
              "\n",
              "output\n",
              "\n",
              "tensor([[0.1467, 0.6925, 0.0987, 0.5244, 0.6491]])  # features\n",
              "tensor([1., 2., 3., 4., 5.])                        # weights\n",
              "\n",
              "tensor([[0.1467, 1.3851, 0.2961, 2.0976, 3.2455]])  # features*weights\n",
              "tensor([[0.1467, 0.6925, 0.0987, 0.5244, 0.6491],\n",
              "        [0.2934, 1.3851, 0.1974, 1.0488, 1.2982],\n",
              "        [0.4400, 2.0776, 0.2961, 1.5732, 1.9473],\n",
              "        [0.5867, 2.7701, 0.3947, 2.0976, 2.5964],\n",
              "        [0.7334, 3.4627, 0.4934, 2.6220, 3.2455]])  # features*weights.view(5,1)\n",
              "tensor([[7.1709]])                                  # torch.mm(features, weights.view(5, 1))\n",
              "\n",
              " <P> Though I don't suggest to do that, if you want, then\n",
              "\n",
              "In [18]: torch.set_printoptions(edgeitems=1)\n",
              "\n",
              "In [19]: a\n",
              "Out[19]:\n",
              "tensor([[-0.7698,  ..., -0.1949],\n",
              "        ...,\n",
              "        [-0.7321,  ...,  0.8537]])\n",
              "\n",
              "In [20]: torch.set_printoptions(edgeitems=3)\n",
              "\n",
              "In [21]: a\n",
              "Out[21]:\n",
              "tensor([[-0.7698,  1.3383,  0.5649,  ...,  1.3567,  0.6896, -0.1949],\n",
              "        [-0.5761, -0.9789, -0.2058,  ..., -0.5843,  2.6311, -0.0008],\n",
              "        [ 1.3152,  1.8851, -0.9761,  ...,  0.8639, -0.6237,  0.5646],\n",
              "        ...,\n",
              "        [ 0.2851,  0.5504, -0.9471,  ...,  0.0688, -0.7777,  0.1661],\n",
              "        [ 2.9616, -0.8685, -1.5467,  ..., -1.4646,  1.1098, -1.0873],\n",
              "        [-0.7321,  0.7610,  0.3182,  ...,  2.5859, -0.9709,  0.8537]])\n",
              "\n",
              " <P> As alternative to [https://stackoverflow.com/users/6210807/kharshit] suggestion, you can define network functional way:\n",
              "\n",
              "class MyCell(torch.nn.Module):\n",
              "    def __init__(self):\n",
              "        super(MyCell, self).__init__()\n",
              "        self.w = []\n",
              "        for i in range(5):\n",
              "            self.w.append( torch.Tensor( 1, 1, 2*i+3 ) )\n",
              "            # init w[i] here, maybe make it \"requires grad\" \n",
              "\n",
              "    def forward(self, x):\n",
              "        for i in range(5):\n",
              "            x = torch.nn.functional.conv1d( x, self.w[i] )\n",
              "            x = torch.nn.functional.relu( x )\n",
              "        return x\n",
              "\n",
              " <P> It does not really make much sense to have a single tensor which requires_grad for only part of its entries.\n",
              "Why not have two separate tensors one that us updated (requires_grad=True) and another one fixed (requires_grad=False)? You can then merge them for computational ease:\n",
              "\n",
              "fixed = torch.rand([2, 3], require_grad=False)\n",
              "upd = torch.rand([2, 3], require_grad=True)\n",
              "mask = torch.tensor([[0, 1, 0], [1, 0, 1]], require_grad=False)  # how to combine the two\n",
              "# combine them using fixed \"mask\":\n",
              "z = mask * fixed + (1-mask) * upd\n",
              "\n",
              "\n",
              "You can obviously have other methods of combining fixed and upd other than using a binary mask.\n",
              "For example, if upd occupies the first two columns of z and fixed the rest, then:\n",
              "\n",
              "fixed = torch.rand([2, 1], require_grad=False)\n",
              "upd = torch.rand([2, 2], require_grad=True)\n",
              "# combine them using concatination\n",
              "z = torch.cat((upd, fixed),dim=1)\n",
              "\n",
              "\n",
              "Or, if you know the indices\n",
              "\n",
              "fidx = torch.tensor([0, 2], dtype=torch.long)\n",
              "uidx = torch.tensor([1, 3, 4, 5], dtype=torch.long)\n",
              "fixed = torch.rand([2,], require_grad=False)\n",
              "upd = torch.rand([4,], require_grad=True)\n",
              "z = torch.empty([2, 3])\n",
              "z[fidx] = fixed \n",
              "z[uidx] = upd\n",
              "\n",
              " <P> Intro\n",
              "First, definition of what a leaf variable in PyTorch is, you can check official documentation for tensor.is_leaf (emphasis mine):\n",
              "\n",
              "All Tensors that have requires_grad which is False will be leaf\n",
              "Tensors by convention.\n",
              "For Tensors that have requires_grad which is True, they will be\n",
              "leaf Tensors if they were created by the user. This means that they\n",
              "are not the result of an operation and so grad_fn is None.\n",
              "\n",
              "So let's see how this looks for outi variable in original code. Immediately after creation, running this snippet:\n",
              "outi = torch.empty(2, requires_grad=True)\n",
              "print(outi.is_leaf, outi.grad_fn, outi.requires_grad)\n",
              "\n",
              "gives:\n",
              "True, None, True\n",
              "\n",
              "as it was created by user and there is no previous operation creating it so it should be the second bolded case from the above citation.\n",
              "Now this line:\n",
              "outi[0] = out1\n",
              "outi[1] = out2\n",
              "\n",
              "Uses two nodes which are not leafs and are part of the graph which goes back to x (which is the only leaf in it). By doing this outi is also part of the original x graph and would have to be backpropagated through, yet you specified it as a leaf (more on that later), which cannot be backpropagated through (by the definition they either don't require gradient or are created by user). Version of outi as leaf was already put on graph, after above assignment, this snippet:\n",
              "print(outi.is_leaf, outi.grad_fn, outi.requires_grad)\n",
              "\n",
              "changes to:\n",
              "False CopySlices object at 0x7f2dfa83a3d0 True\n",
              "\n",
              "Error\n",
              "Now, I agree it's a pretty uninformative error given that changing requires_grad=False does not make it non-leaf variable (requires_grad=False is implicit):\n",
              "outi = torch.empty(2)\n",
              "print(outi.is_leaf, outi.grad_fn, outi.requires_grad)\n",
              "#  True None False\n",
              "\n",
              "But this tensor could be \"upgraded\" to non-leaf tensor if you use assignment as you did without breaking the expected behaviour.\n",
              "Why? Because you implicitly (or explicitly in case of your code) said you don't need gradient for this variable and PyTorch retains gradient only for leaf variables (unless you specify .retain_grad for specific tensor) due to memory optimization. So the only change here would be it will no longer be a leaf, but this would not break promises as .grad would be None anyway.\n",
              "If you were to have requires_grad=True as you originally did you could, reasonably, according to PyTorch semantics, think that this:\n",
              "outi.grad\n",
              "\n",
              "Will give you a tensor with gradient. But if this requires_grad=True tensor were to be changed to non-leaf tensor, then, by definition it wouldn't have this field (as non-leaf tensors have .grad=None).\n",
              "To me it seems like a design decision on their part to avoid confusion with requires_grad=True and breaking expected user experience.\n",
              "BTW. If they were to disallow leaf variables inside graph then operation which works fine now (requires_grad=False) should be disallowed as well. But as requires_grad=False is implicit and often used (creating tensors or something like you did) it seems not to be to much of a stretch to allow it. Disallowing it would be much more severe. On the other hand if you specify requires_grad=True it could be assumed you know better what you are doing and really need that gradient here.\n",
              "BTW2. This explanation might be a stretch but hopefully will shed some light. I haven't found anything official regarding this error (admittedly though I didn't dig too deep).\n",
              "Some resources here, here (this one is important, someone was asking for justification of some design decisions though didn't get one AFAIK).\n",
              "Comments\n",
              "Comment 1\n",
              "\n",
              "I think the requires_grad is getting inherited from the slice and also\n",
              ".grad is available.\n",
              "\n",
              "Yes, it has requires_grad as True also as it's part of the graph now, BUT grad is not available as it is no longer a leaf. Printing outi.grad after backward gives you None and the following warning:\n",
              "\n",
              "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor\n",
              "is being accessed. Its .grad attribute won't be populated during\n",
              "autograd.backward(). If you indeed want the gradient for a non-leaf\n",
              "Tensor, use .retain_grad() on the non-leaf Tensor. If you access the\n",
              "non-leaf Tensor by mistake, make sure you access the leaf Tensor\n",
              "instead. See github.com/pytorch/pytorch/pull/30531 for more\n",
              "informations.\n",
              "\n",
              "So the .grad attribute is None anyway as user would expect giving requires_grad=False as creation argument. User could expect gradient to be not None if he was to set requires_grad=True and that's when PyTorch raises the error, IMO due to possible inconsistency with user expectation in that case.\n",
              "Comment 2\n",
              "For example:\n",
              "a = torch.ones(2,requires_grad=False)\n",
              "b = 2*a\n",
              "b.requires_grad=True\n",
              "print(b.is_leaf) #True\n",
              "\n",
              "I have changed your code a little to go through it step by step:\n",
              "a = torch.ones(2, requires_grad=False)\n",
              "print(a.is_leaf) # True\n",
              "\n",
              "We should start with a here, a is a leaf according to docs as:\n",
              "\n",
              "All Tensors that have requires_grad which is False will be leaf\n",
              "Tensors by convention.\n",
              "\n",
              "b = a * 2\n",
              "print(b.is_leaf)\n",
              "\n",
              "Now b is leaf as it does not require gradient (because a does not need a gradient it doesn't have to be backpropagated through this branch). Manipulating tensors with requires_grad=False creates tensors which do not require_grad otherwise it would be wasteful and non-sensical to turn it on.\n",
              "b.requires_grad = True\n",
              "print(b.is_leaf)\n",
              "\n",
              "Now this one returns True as well. Once again, docs wording might not be the best here (as I've stated before), but (my additions in bold):\n",
              "\n",
              "For Tensors that have requires_grad which is True (our case now)\n",
              "they will be leaf Tensors if they were created by the user (debatable about creation here as you have modified the existing one, indeed). This means that they are not the result of an operation and so grad_fn is None (this one IMO clarifies the previous point)\n",
              "\n",
              "About clarification- as this tensor is not a result of any mathematical operation, you simply said you want this b tensor to require_grad.\n",
              "IMO it is a user created tensor as it was placed (created) on graph for the first time (before there was no need for that as it didn't require gradient).\n",
              "And it does have it's requires_grad set to True, you did it explicitly here.\n",
              "Comment 3  4\n",
              "\n",
              "Everything with requires_grad=True is on the graph\n",
              "\n",
              "Yes, but something with requires_grad=False can be on a graph as well if it is a leaf. Actually every PyTorch operation is created and added dynamically onto computational graph, here we use simplification: it's on graph if it takes part in backpropagation. For example neural network parameters are leafs yet they are on graph as the last part during backpropagation and they have their gradients (as they are optimized they need to be in graph in order to backprop through it).\n",
              "\n",
              "Everything not on the graph is a leaf\n",
              "\n",
              "Yes, essentially\n",
              "\n",
              "everything on the graph that is not the product of operation on graph\n",
              "tensors is a leaf\n",
              "\n",
              "Yes, if you add some tensor to it (e.g. created by torch.randn or such) is a leaf\n",
              "\n",
              "every leaf on the graph and non-leaf where I set retain_grad=True\n",
              "manually will get .grad attribute populated.\n",
              "\n",
              "Yes, it if it is part of backpropagation which is almost always the case in our \"mental graph\" case (I think at least). Unless it already has requires_grad=True, in this case it will be populated with gradient. Basically, except for creation, you shouldn't tinker with setting requires_grad=True as it is prone to fail (as you saw) and will definitely raise some eyebrows for other people reading your code.\n",
              "\n",
              "every non-leaf on the graph has a grad_fn associated with it\n",
              "\n",
              "Yes, that follows as some operation had to create it (and if it was created by some operation and this operation is differentiable, grad_fn is registered to be used during backward() call).\n",
              " <P> How about (tens == 101).nonzero()[:, 1]\n",
              "\n",
              "In [20]: from torch import tensor                                                                       \n",
              "\n",
              "In [21]: tens = torch.tensor([[  101,   146,  1176, 21806,  1116,  1105, 18621,   119,   102,     0, \n",
              "    ...:              0,     0,     0], \n",
              "    ...:         [  101,  1192,  1132,  1136,  1184,   146,  1354,  1128,  1127,   117, \n",
              "    ...:           1463,   119,   102], \n",
              "    ...:         [  101,  6816,  1905,  1132, 14918,   119,   102,     0,     0,     0, \n",
              "    ...:              0,     0,     0]])                                                                \n",
              "\n",
              "In [22]: (tens == 101).nonzero()[:, 1]                                                                  \n",
              "Out[22]: tensor([0, 0, 0])\n",
              "\n",
              " <P> Instead of constructing W_mat_directly from the elements of w, try assigning w into W:\n",
              "\n",
              "W_mat_directly = torch.zeros((3, 3), dtype=w.dtype)\n",
              "W_mat_directly[(0, 0, 1, 1, 2, 2), (1, 2, 0, 2, 0, 1)] = w\n",
              "\n",
              "\n",
              "You'll get\n",
              "\n",
              "\n",
              "tensor([[ 0., 10., 11.],\n",
              "        [12.,  0., 13.],\n",
              "        [14., 15.,  0.]], grad_fn=IndexPutBackward)\n",
              "\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col0\" class=\"data row7 col0\" >Multiprocessping-distributed ERROR</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col1\" class=\"data row7 col1\" >This is kind of expected. There are two main problems at play here:\n",
              "\n",
              "1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n",
              "2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col2\" class=\"data row7 col2\" >After almost one year later, now, I know why.\n",
              " \n",
              " If we are using multi-GPU to train models, it would like to start multi-thread for different GPUs. And each thread has to rerun the script. If we change the code during this time, the other threads may load the modified code, which caused this problem.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row7_col3\" class=\"data row7 col3\" ><P> This is kind of expected. There are two main problems at play here:\n",
              "\n",
              "1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n",
              "2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property. <P> Hi there! Thank you for the detailed report. You're right and this is a bug. We do properly set the completed flag for collectives but not for the send/recv operations. It's not that the operation hasn't completed yet (it has), but the `completed_` flag in `ProcessGroup::Work` isn't updated accordingly. Fixing this is a bit more complex than simply fixing the boolean, because Gloo's send/recv doesn't have a non-blocking way to check if the operation completed or not.\n",
              "\n",
              "To fix this properly, we need to:\n",
              "1) Update Gloo to allow non-blocking check for send/recv completion.\n",
              "2) Update the bindings to call this function when `is_completed()` is called. <P> ## Trying Solution 2\n",
              "\n",
              "#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\n",
              "\n",
              "1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \n",
              "2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\n",
              "\n",
              "I initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\n",
              "\n",
              "~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type. <P> [here you go](https://gist.githubusercontent.com/Wulfsta/5d8b2b902e1068433aaf737657453a25/raw/18028b37ea909905d23b60805399df4ea0c6172b/gistfile1.txt). \n",
              " \n",
              " \n",
              " \n",
              " Edit: I obviously haven't parsed this whole file, but the problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening? <P> Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\n",
              "\n",
              "From your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\n",
              "\n",
              "It could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro. <P> I can see two solutions to this issue:\n",
              "\n",
              "1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\n",
              "2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\n",
              "\n",
              "[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147 <P> Just to clarify before I start putting in pull requests, the way forward is:\n",
              " \n",
              " \n",
              " \n",
              " 1. ASAP remove the `torch_function_dispatch` decorator from everything in `torch.functional`\n",
              " \n",
              " 2. Look into either:\n",
              " \n",
              "  A) Speed up the decorator so the overhead is smaller. I don't think we can ever get the overhead to zero but we can probably at least get it much lower by doing C++-level checks for if parameters are `torch.tensor` instances or not\n",
              " \n",
              "  B) Rewrite the operators in `torch.functional` to be in C++ <P> @dawnwch The equivalent to `with torch.no_grad()` in C++ is `torch::NoGradGuard no_grad`. For example:\n",
              " \n",
              " Python:\n",
              " \n",
              " python\n",
              " \n",
              " with torch.no_grad():\n",
              " \n",
              "  module.weight += 1\n",
              " \n",
              " \n",
              " \n",
              " C++:\n",
              " \n",
              " cpp\n",
              " \n",
              " {\n",
              " \n",
              "  torch::NoGradGuard no_grad;\n",
              " \n",
              "  module->weight += 1;\n",
              " \n",
              " } // Note that anything out of this scope will still record gradients\n",
              " \n",
              "  \n",
              " \n",
              " Please try it out and let me know if it resolves the memory usage issue. <P> Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\n",
              "py\n",
              "torch.distributions.Distribution.set_default_validate_args(True)\n",
              "\n",
              "In fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion? <P> Once you have an ATen context object, you can just call `lazyInitCUDA()` and get a `THCState*`. Then, you can pass that into the THCS functions.\n",
              " \n",
              " \n",
              " \n",
              " Also, please remember that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](https://discuss.pytorch.org).</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col0\" class=\"data row8 col0\" >Differences between F.relu(X) and torch.max(X, 0)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col1\" class=\"data row8 col1\" >Yes, tf.math.reduce_max does allow gradients to flow. It is easy to check (this is TensorFlow 2.x but it is the same result in 1.x):\n",
              "import tensorflow as tf\n",
              "\n",
              "with tf.GradientTape() as tape:\n",
              "\n",
              "x = tf.linspace(0., 2. * 3.1416, 10)\n",
              ">>> tape.watch(x)\n",
              "    # A sequence of operations involving reduce_max\n",
              " y = TF.matmul(x, dim=1)\n",
              "Sorted\n",
              "[[11.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col2\" class=\"data row8 col2\" >torch.max is not differentiable according to this discussion. A loss function needs to be continuous and differentiable to do backprop. relu is differentiable as it can be approximated and hence the use of it in a loss function.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row8_col3\" class=\"data row8 col3\" ><P> Yes, tf.math.reduce_max does allow gradients to flow. It is easy to check (this is TensorFlow 2.x but it is the same result in 1.x):\n",
              "import tensorflow as tf\n",
              "\n",
              "with tf.GradientTape() as tape:\n",
              "    x = tf.linspace(0., 2. * 3.1416, 10)\n",
              "    tape.watch(x)\n",
              "    # A sequence of operations involving reduce_max\n",
              "    y = tf.math.square(tf.math.reduce_max(tf.math.sin(x)))\n",
              "# Check gradients\n",
              "g = tape.gradient(y, x)\n",
              "print(g.numpy())\n",
              "# [ 0.         0.         0.3420142 -0.        -0.        -0.\n",
              "#  -0.         0.         0.         0.       ]\n",
              "\n",
              "As you can see, there is a valid gradient for y with respect to x. Only one of the values is not zero, because it is the value that then resulted in the maximum value, so it is the only value in x that affects the value of y. This is the correct gradient for the operation.\n",
              " <P> One must admit the unique function can sometimes be very confusing without given proper examples and explanations.\n",
              "\n",
              "The dim parameter specifies which dimension on the matrix tensor you want to apply on. \n",
              "\n",
              "For instance, in a 2D matrix, dim=0 will let operation perform vertically where dim=1 means horizontally.\n",
              "\n",
              "Example, let's consider a 4x4 matrix with dim=1. As you can see from my code below, the unique operation is applied row by row. \n",
              "\n",
              "You notice the double occurrence of the number 11 in the first and last row. Numpy and Torch does this to preserve the shape of the final matrix. \n",
              "\n",
              "However, if you do not specify any dimension, torch will automatically flatten your matrix and then apply unique to it and you will get a 1D array that contains unique data.\n",
              "\n",
              "import torch\n",
              "\n",
              "m = torch.Tensor([\n",
              "    [11, 11, 12,11], \n",
              "    [13, 11, 12,11], \n",
              "    [16, 11, 12, 11],  \n",
              "    [11, 11, 12, 11]\n",
              "])\n",
              "\n",
              "output, indices = torch.unique(m, sorted=True, return_inverse=True, dim=1)\n",
              "print(\"Ori \\n{}\".format(m.numpy()))\n",
              "print(\"Sorted \\n{}\".format(output.numpy()))\n",
              "print(\"Indices \\n{}\".format(indices.numpy()))\n",
              "\n",
              "# without specifying dimension\n",
              "output, indices = torch.unique(m, sorted=True, return_inverse=True)\n",
              "print(\"Sorted (no dim) \\n{}\".format(output.numpy()))\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Result (dim=1)\n",
              "\n",
              "Ori\n",
              "[[11. 11. 12. 11.]\n",
              " [13. 11. 12. 11.]\n",
              " [16. 11. 12. 11.]\n",
              " [11. 11. 12. 11.]]\n",
              "Sorted\n",
              "[[11. 11. 12.]\n",
              " [11. 13. 12.]\n",
              " [11. 16. 12.]\n",
              " [11. 11. 12.]]\n",
              "Indices\n",
              "[1 0 2 0]\n",
              "\n",
              "\n",
              "Result (no dimension)\n",
              "\n",
              "Sorted (no dim)\n",
              "[11. 12. 13. 16.]\n",
              "\n",
              " <P> Actually, this is what you are looking for:\n",
              "\n",
              "Case 1: when z = 2*x**3 + x\n",
              "\n",
              "import torch\n",
              "from torch.autograd import Variable\n",
              "x = Variable(2*torch.ones(2, 2), requires_grad=True)\n",
              "z = x*x*x*2+x\n",
              "z.backward(torch.ones_like(z))\n",
              "print(x.grad)\n",
              "\n",
              "\n",
              "output:\n",
              "\n",
              "tensor([[25., 25.],\n",
              "        [25., 25.]])\n",
              "\n",
              "\n",
              "Case 2: when z = x*x\n",
              "\n",
              "x = Variable(2*torch.ones(2, 2), requires_grad=True)\n",
              "z = x*x\n",
              "z.backward(torch.ones_like(z))\n",
              "print(x.grad)\n",
              "\n",
              "\n",
              "output:\n",
              "\n",
              "tensor([[4., 4.],\n",
              "        [4., 4.]])\n",
              "\n",
              "\n",
              "Case 3: when z = x (your case)\n",
              "\n",
              "x = Variable(2*torch.ones(2, 2), requires_grad=True)\n",
              "z = x\n",
              "z.backward(torch.ones_like(z))\n",
              "print(x.grad)\n",
              "\n",
              "\n",
              "output:\n",
              "\n",
              "tensor([[1., 1.],\n",
              "        [1., 1.]])\n",
              "\n",
              "\n",
              "To learn more how to calculate gradient in pytorch, check this.\n",
              " <P> Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi​/otheri​ with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc(). <P> Yes, they are the same!\n",
              "\n",
              "import tensorflow as tf\n",
              "tensor = [[1., 2.], [4., 5.], [3., 6.], [7., 8.], [5., 2.]]\n",
              "tensor = tf.convert_to_tensor(tensor, dtype=tf.float32)\n",
              "t_norm = tf.norm(tensor, ord=2, axis=1)\n",
              "print(t_norm)\n",
              "\n",
              "\n",
              "Output\n",
              "\n",
              "tf.Tensor([ 2.236068   6.4031243  6.708204  10.630146   5.3851647], shape=(5,), dtype=float32)\n",
              "\n",
              "\n",
              "import torch\n",
              "tensor = [[1., 2.], [4., 5.], [3., 6.], [7., 8.], [5., 2.]]\n",
              "tensor = torch.tensor(tensor, dtype=torch.float32)\n",
              "t_norm = torch.norm(tensor, p=2, dim=1)\n",
              "print(t_norm)\n",
              "\n",
              "\n",
              "Output\n",
              "\n",
              "tensor([ 2.2361,  6.4031,  6.7082, 10.6301,  5.3852])\n",
              "\n",
              " <P> When you use *, the multiplication is elementwise, when you use torch.mm it is matrix multiplication.\n",
              "\n",
              "Example:\n",
              "\n",
              "a = torch.rand(2,5)\n",
              "b = torch.rand(2,5)\n",
              "result = a*b \n",
              "\n",
              "\n",
              "result will be shaped the same as a or b i.e (2,5)\n",
              "whereas considering operation \n",
              "\n",
              "result = torch.mm(a,b)\n",
              "\n",
              "\n",
              "It will give a size mismatch error, as this is proper matrix multiplication (as we study in linear algebra) and a.shape[1] != b.shape[0]. When you apply the view operation in torch.mm you are trying to match the dimensions. \n",
              "\n",
              "In the special case of the shape in some particular dimension being 1, it becomes a dot product and hence sum (a*b) is same as mm(a, b.view(5,1))\n",
              " <P> For quick understanding, layout=torch.strided corresponds to dense tensors while layout=torch.sparse_coo corresponds to sparse tensors.\n",
              "\n",
              "From another perspective, we can understand it together with torch.tensor.view.\n",
              "A tensor can be viewed indicates it is contiguous. If we change the view of a tensor, the strides will change accordingly, but the data will keep the same. More specifically, view returns a new tensor with the same data but different shape, and strides is compatible with the view to indicate how to access the data in the memory.\n",
              "\n",
              "For example\n",
              "\n",
              "In [1]: import torch\n",
              "In [2]: a = torch.arange(15)\n",
              "\n",
              "In [3]: a.data_ptr()\n",
              "Out[3]: 94270437164688\n",
              "\n",
              "In [4]: a.stride()\n",
              "Out[4]: (1,)\n",
              "\n",
              "In [5]: a = a.view(3, 5)\n",
              "\n",
              "In [6]: a.data_ptr() # share the same data pointer\n",
              "Out[6]: 94270437164688\n",
              "\n",
              "In [7]: a.stride() # the stride changes as the view changes\n",
              "Out[7]: (5, 1)\n",
              "\n",
              "\n",
              "In addition, the idea of torch.strided is basically the same as strides in numpy.\n",
              "View this question for more detailed understanding.\n",
              "How to understand numpy strides for layman?\n",
              " <P> See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape and float inputs. Note The backward pass with respect to input is not yet supported. Please open an issue on PyTorch’s Github to request it. input (Tensor) – the first non-negative input tensor other (Tensor) – the second non-negative input tensor out (Tensor, optional) – the output tensor. Example: <P> Looks like the [`torch.amax()`]() docs are aware of this behavior:\n",
              "\n",
              "amax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\n",
              " <P> The minus essentially means you go backwards through the dimensions. Let A be a n-dimensional matrix. Then dim=n-1=-1, dim=n-2=-2, ..., dim=1=-(n-1), dim=0=-n. See the numpy doc for more information, as pytorch is heavily based on numpy.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col0\" class=\"data row9 col0\" >quantization.fuse_modules fails with Conv1d and BatchNorm1d</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col1\" class=\"data row9 col1\" >The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `Convert` and `Quantize` function from here:\n",
              "\n",
              "Previously we have fixed the general `Bias = nullptr` here:</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col2\" class=\"data row9 col2\" >Here ( is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row9_col3\" class=\"data row9 col3\" ><P> cudnn does not support backprop in evaluate mode, we should add a fallback to nn for this case <P> The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\n",
              "https://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\n",
              "\n",
              "Previously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403 <P> This sounds like a silent correctness issue so I marked it as high-priority but someone with more context behind the normalization layers should take a look at this. <P> I have not yet fully analyzed the problem and I am not sure if the observed effect is a bug or by design.\n",
              " \n",
              " \n",
              " \n",
              " It seems that in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect. \n",
              " \n",
              " \n",
              " \n",
              " I modified the repro script a bit in order to get the effect without `add_zero_attn=True` (initial in-projection bias should be 0) and to split the contribution of query and key & value: \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " import torch\n",
              " \n",
              " \n",
              " \n",
              " embedding_dim = 1\n",
              " \n",
              " batch_size = 1\n",
              " \n",
              " num_heads = 1\n",
              " \n",
              " seq_len = 4\n",
              " \n",
              " \n",
              " \n",
              " net = torch.nn.MultiheadAttention(embedding_dim, num_heads, add_zero_attn=False)\n",
              " \n",
              " mask = torch.cat([torch.ones(seq_len, seq_len).triu(), torch.zeros(seq_len, 1)], dim=1)\n",
              " \n",
              " mask[mask==1]=float('-inf')\n",
              " \n",
              " print(mask)\n",
              " \n",
              " \n",
              " \n",
              " for i in range(seq_len):\n",
              " \n",
              "  x = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n",
              " \n",
              "  y = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n",
              " \n",
              "  z = torch.cat([y, torch.zeros(1, 1, embedding_dim)]) # add zero sequence element\n",
              " \n",
              "  o, w = net(x, z, z, attn_mask=mask)\n",
              " \n",
              "  #print(w)\n",
              " \n",
              "  # o.shape is (seq_len, batch_size, embedding_dim)\n",
              " \n",
              "  o.mean([1, 2])[i].backward()\n",
              " \n",
              "  print(i, 'x:', x.grad.abs().sum([1, 2]).view(-1))\n",
              " \n",
              "  print(i, 'y:', y.grad.abs().sum([1, 2]).view(-1))\n",
              " \n",
              "  \n",
              " \n",
              " \n",
              " \n",
              " Output is:\n",
              " \n",
              " \n",
              " \n",
              " tensor([[-inf, -inf, -inf, -inf, 0.],\n",
              " \n",
              "  [0., -inf, -inf, -inf, 0.],\n",
              " \n",
              "  [0., 0., -inf, -inf, 0.],\n",
              " \n",
              "  [0., 0., 0., -inf, 0.]])\n",
              " \n",
              " 0 x: tensor([0., 0., 0., 0.])\n",
              " \n",
              " 0 y: tensor([0., 0., 0., 0.])\n",
              " \n",
              " 1 x: tensor([0.0000, 0.0148, 0.0000, 0.0000])\n",
              " \n",
              " 1 y: tensor([0.2801, 0.0000, 0.0000, 0.0000])\n",
              " \n",
              " 2 x: tensor([0.0000, 0.0000, 0.0127, 0.0000])\n",
              " \n",
              " 2 y: tensor([0.1798, 0.1798, 0.0000, 0.0000])\n",
              " \n",
              " 3 x: tensor([0.0000, 0.0000, 0.0000, 0.0105])\n",
              " \n",
              " 3 y: tensor([0.1323, 0.1323, 0.1323, 0.0000])\n",
              " \n",
              "  <P> I think adding noise to gradients is simple. A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.\n",
              "I initially proposed adding Noisy SGD, but the proposal was rejected considering its triviality. Ref: https://github.com/pytorch/pytorch/pull/4332 <P> The same happens with `index_copy` iirc.\n",
              "It comes from the fact that the current tests are very thorough. They have cases for all cases of contiguous / non-contiguous inputs with all the possible flags.\n",
              "\n",
              "Making the input sizes smaller would alleviate but not completely solve this problem  <P> QR batching PR was done after 1.1, please allow me to investigate. <P> correct. The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies. <P> @albanD @ezyang \n",
              " \n",
              " Thanks for your suggestions.\n",
              " \n",
              " \n",
              " \n",
              " We understood the risk of varargs in custom Function, and will follow your suggestion.\n",
              " \n",
              " \n",
              " \n",
              " Thanks. <P> this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col0\" class=\"data row10 col0\" >`with torch.enable_grad` also works outside a `no_grad` context</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col1\" class=\"data row10 col1\" >The torch.autograd.enable_grad documentation says:\n",
              "\n",
              "Enables gradient calculation inside a no_grad context. This has no effect outside of No_grad.\n",
              "Given this wording, the following is expected:\n",
              "\n",
              "\n",
              "torch.set_grad_enabled(False)\n",
              "with torch.no_grad:\n",
              "    # Gradient tracking will NOT be enabled here.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col2\" class=\"data row10 col2\" >\"Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`\" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you!</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row10_col3\" class=\"data row10 col3\" ><P> Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n",
              "\n",
              "Are zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n",
              "\n",
              "If the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()). <P> torch.no_grad() will disable gradient information for the results of operations involving tensors that have their requires_grad set to True. So consider the following:\n",
              "\n",
              "import torch\n",
              "\n",
              "net = torch.nn.Linear(4, 3)\n",
              "\n",
              "input_t = torch.randn(4)\n",
              "\n",
              "with torch.no_grad():\n",
              "\n",
              "    for name, param in net.named_parameters():\n",
              "        print(\"{} {}\".format(name, param.requires_grad))\n",
              "\n",
              "    out = net(input_t)\n",
              "\n",
              "    print('Output: {}'.format(out))\n",
              "\n",
              "    print('Output requires gradient: {}'.format(out.requires_grad))\n",
              "    print('Gradient function: {}'.format(out.grad_fn))\n",
              "\n",
              "\n",
              "This prints\n",
              "\n",
              "weight True\n",
              "bias True\n",
              "Output: tensor([-0.3311,  1.8643,  0.2933])\n",
              "Output requires gradient: False\n",
              "Gradient function: None\n",
              "\n",
              "\n",
              "If you remove with torch.no_grad(), you get\n",
              "\n",
              "weight True\n",
              "bias True\n",
              "Output: tensor([ 0.5776, -0.5493, -0.9229], grad_fn=AddBackward0)\n",
              "Output requires gradient: True\n",
              "Gradient function: AddBackward0 object at 0x7febe41e3240\n",
              "\n",
              "\n",
              "Note that in both cases the module parameters have requires_grad set to True, but in the first case the out tensor doesn't have a gradient function associated with it whereas it does in the second case.\n",
              " <P> Yeah that makes sense. There's really nothing that prevents people from setting `requires_grad` to `False` after they construct the optimizer, and it won't even complain. We should just remove the check. <P> \"Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`\" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you! <P> Thanks for the report, I will take a look why this happens. It might simply be that we don't restore the status in `autograd.grad` properly. <P> The torch.autograd.enable_grad documentation says:\n",
              "\n",
              "\n",
              "  Enables gradient calculation inside a no_grad context. This has no effect outside of no_grad.\n",
              "\n",
              "\n",
              "Given this wording, the following is expected:\n",
              "\n",
              "torch.set_grad_enabled(False)\n",
              "with torch.enable_grad:\n",
              "    # Gradient tracking will NOT be enabled here.\n",
              "torch.set_grad_enabled(True)\n",
              "\n",
              "\n",
              "vs:\n",
              "\n",
              "with torch.no_grad():\n",
              "    with torch.enable_grad:\n",
              "        # Gradient tracking IS enabled here.\n",
              "\n",
              "\n",
              "But as blue-phoenix shows, this is not the case.\n",
              "\n",
              "I raised an issue here.\n",
              " <P> What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied. <P> > One question: do we need to do special handling when the `create_graph=True` flag is set?\n",
              " \n",
              " \n",
              " \n",
              " Other than setting create_graph=True when we call autograd.grad, no, I don't think so. <P> \n",
              "  Why do we need clone the grad_output and assign it to grad_input other than simple assignment during backpropagation?\n",
              "\n",
              "\n",
              "tensor.clone() creates a copy of tensor that imitates the original tensor's requires_grad field. clone is a way to copy the tensor while still keeping the copy as a part of the computation graph it came from.\n",
              "\n",
              "So, grad_input is part of the same computation graph as grad_output and if we compute the gradient for grad_output, then the same will be done for grad_input.\n",
              "\n",
              "Since we make changes in grad_input, we clone it first. \n",
              "\n",
              "\n",
              "  What's the purpose of 'grad_input[input  0] = 0'? Does it mean we don't update the gradient when input less than zero?\n",
              "\n",
              "\n",
              "This is done as per ReLU function's definition. The ReLU function is f(x)=max(0,x). It means if x=0 then f(x)=0, else f(x)=x. In the first case, when x0, the derivative of f(x) with respect to x is f'(x)=0. So, we perform grad_input[input  0] = 0. In the second case, it is f'(x)=1, so we simply pass the grad_output to grad_input (works like an open gate).\n",
              " <P> This approach would pass the same param_mat to all optimizers, which would update it with its gradient and I assume that's the use case you are looking for.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col0\" class=\"data row11 col0\" >Pytorch transformation on MNIST dataset</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col1\" class=\"data row11 col1\" >You need to send the target tensor to your dataloader, not its result:\n",
              "\n",
              "target = torch.utils.data.DataLoader(dataset, criterion=self.criterion)\n",
              "criterion = CrossEntropyLoss(...)\n",
              "for epoch in range(0, epochs):\n",
              "    for epoch in enumerate(dataloader):</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col2\" class=\"data row11 col2\" >def erase_middle(image: torch.Tensor) - torch.Tensor:\n",
              "    for i in range(int(image_size/2-5),int(image_size/2+3)):\n",
              "        for j in range(int(image_size/2-5),int(image_size/2+3)):\n",
              "            image[:, i, j] = 0\n",
              "    return image\n",
              "\n",
              "transform = torchvision.transforms.Compose(\n",
              "    [\n",
              "        # First transform it to a tensor\n",
              "        torchvision.transforms.ToTensor(),\n",
              "        # Then erase the middle\n",
              "        torchvision.transforms.Lambda(erase_middle),\n",
              "    ]\n",
              ")\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row11_col3\" class=\"data row11 col3\" ><P> def predict(self, test_images):\n",
              "    self.eval()\n",
              "    # model is self(VGG class's object)\n",
              "    \n",
              "    count = test_images.shape[0]\n",
              "    result_np = []\n",
              "        \n",
              "    for idx in range(0, count):\n",
              "        # print(idx)\n",
              "        img = test_images[idx, :, :, :]\n",
              "        img = np.expand_dims(img, axis=0)\n",
              "        img = torch.Tensor(img).permute(0, 3, 1, 2).to(device)\n",
              "        # print(img.shape)\n",
              "        pred = self(img)\n",
              "        pred_np = pred.cpu().detach().numpy()\n",
              "        for elem in pred_np:\n",
              "            result_np.append(elem)\n",
              "    return result_np\n",
              "\n",
              "network is VGG-19 and ref my source code.\n",
              "like this architecture:\n",
              "class VGG(object):\n",
              "    def __init__(self):\n",
              "    ...\n",
              "\n",
              "\n",
              "    def train(self, train_images, valid_images):\n",
              "        train_dataset = torch.utils.data.Dataset(train_images)\n",
              "        valid_dataset = torch.utils.data.Dataset(valid_images)\n",
              "\n",
              "        trainloader = torch.utils.data.DataLoader(train_dataset)\n",
              "        validloader = torch.utils.data.DataLoader(valid_dataset)\n",
              "\n",
              "        self.optimizer = Adam(...)\n",
              "        self.criterion = CrossEntropyLoss(...)\n",
              "    \n",
              "        for epoch in range(0, epochs):\n",
              "            ...\n",
              "            self.evaluate(validloader, model=self, criterion=self.criterion)\n",
              "    ...\n",
              "\n",
              "    def evaluate(self, dataloader, model, criterion):\n",
              "        model.eval()\n",
              "        for i, sample in enumerate(dataloader):\n",
              "    ...\n",
              "\n",
              "    def predict(self, test_images):\n",
              "    \n",
              "    ...\n",
              "\n",
              "if __name__ == \"__main__\":\n",
              "    network = VGG()\n",
              "    trainset, validset = get_dataset()    # abstract function for showing\n",
              "    testset = get_test_dataset()\n",
              "    \n",
              "    network.train(trainset, validset)\n",
              "\n",
              "    result = network.predict(testset)\n",
              "\n",
              " <P> Why not apply the binarization after the conversion from PIL.Image to torch.Tensor?\n",
              "class ThresholdTransform(object):\n",
              "  def __init__(self, thr_255):\n",
              "    self.thr = thr_255 / 255.  # input threshold for [0..255] gray level, convert to [0..1]\n",
              "\n",
              "  def __call__(self, x):\n",
              "    return (x  self.thr).to(x.dtype)  # do not change the data type\n",
              "\n",
              "Once you have this transformation, you simply add it:\n",
              "data_transformer = transforms.Compose([\n",
              "        transforms.Resize((10, 10)),\n",
              "        transforms.Grayscale(),\n",
              "        transforms.ToTensor(),\n",
              "        ThresholdTransform(thr_255=240)\n",
              "    ])\n",
              "\n",
              " <P> I believe the Pytorch transforms only work on images (PIL images or np arrays in this case) and not labels (which are dicts according to the trace). As such, I don't think you need to \"tensorify\" the labels as in this line target = T.ToTensor()(target) in the __getitem__ function.\n",
              " <P> Try changing data = data.flatten(start_dim=2) to data = data.view(data.size(0), -1).\n",
              "I use start_dim=2 based on the dimensions of the MNIST data.\n",
              "The data is brought in as follows:\n",
              "# Fashion-MNIST dataset\n",
              "train_set = torchvision.datasets.FashionMNIST(\n",
              "\troot = './data/FashionMNIST',\n",
              "\ttrain = True,\n",
              "\tdownload = True,\n",
              "\ttransform = transforms.Compose([\n",
              "\t\ttransforms.ToTensor()                                 \n",
              "\t])\n",
              ")\n",
              "\n",
              "test_set = torchvision.datasets.FashionMNIST(\n",
              "\troot = './data/FashionMNIST',\n",
              "\ttrain = False,\n",
              "\tdownload = True,\n",
              "\ttransform = transforms.Compose([\n",
              "\t\ttransforms.ToTensor()                                 \n",
              "\t])\n",
              ")\n",
              "\n",
              "Then performing this:\n",
              "test_loader = torch.utils.data.DataLoader(dataset)\n",
              "for data, target in test_loader:\n",
              "\t\t\tprint(\"Init shape: {}, nreshape: {}, length: {}, target shape: {}\".format(\n",
              "\t\t\t\tdata.shape, data.flatten(start_dim=2).shape, len(test_loader.dataset), target.shape))\n",
              "\n",
              "Yields this:\n",
              "Init shape: torch.Size([1, 1, 28, 28]), nreshape: torch.Size([1, 1, 784]), length: 10000, target shape: torch.Size([1])\n",
              "\n",
              "Do you still think the start_dim=1 would be helpful?\n",
              "Thanks\n",
              "Try the solution in my latest updated reply.\n",
              " the issue is that the extra channel dimension in your tensor is retained in the output so your model output has shape (n_examples, 1, 10), which makes your loss function expect a label of shape (1, 10). If you reshape your output to (n_examples, 10), then the loss will take scalar labels.\n",
              "Ok, I see. Wouldn’t I need something closer to data = data.flatten(start_dim=2).view(data.size(0), -1) to ensure I keep all 784 dimensions?\n",
              "No just try the fix I mentioned.\n",
              "The extra dummy channel dimension makes the loss function think you are giving it multi-dimensional outputs, which you really aren’t. See the doc for more details.\n",
              "Looks like that did the trick. I will look into the .view() documentation at some point, right now I have training to do…  so much! <P> Done it!\n",
              "def cycle(iterable):\n",
              "    while True:\n",
              "        for x in iterable:\n",
              "            yield x\n",
              "\n",
              "# Return only images of certain class (eg. aeroplanes = class 0)\n",
              "def get_same_index(target, label):\n",
              "    label_indices = []\n",
              "    for i in range(len(target)):\n",
              "        if target[i] == label:\n",
              "            label_indices.append(i)\n",
              "    return label_indices\n",
              "\n",
              "# STL10 dataset\n",
              "train_dataset = torchvision.datasets.STL10('drive/My Drive/training/stl10', split='train+unlabeled', download=True, transform=torchvision.transforms.Compose([\n",
              "        torchvision.transforms.ToTensor()]))\n",
              "\n",
              "label_class = 1# birds\n",
              "\n",
              "# Get indices of label_class\n",
              "train_indices = get_same_index(train_dataset.labels, label_class)\n",
              "\n",
              "bird_set = torch.utils.data.Subset(train_dataset, train_indices)\n",
              "\n",
              "train_loader = torch.utils.data.DataLoader(dataset=bird_set, shuffle=True,\n",
              "                                           batch_size=batch_size, drop_last=True)\n",
              "train_iterator = iter(cycle(train_loader))\n",
              "\n",
              " <P> I found 2 solutions so far to convert torchvision MNIST dataset to tensors. The first one is derived from Fábio Perez comment :\n",
              "\n",
              "print(\"\\nFirst...\")\n",
              "st = time()\n",
              "x_all_ts = torch.tensor([mnist_ds[i][0].numpy() for i in range(0, len(mnist_ds))])\n",
              "t_all_ts = mnist_ds.train_labels\n",
              "print(f\"{time()-st}   images:{x_all_ts.size()}  targets:{t_all_ts.size()} \")\n",
              "\n",
              "print(\"\\nSecond...\")\n",
              "st = time()\n",
              "mnist_dl = DataLoader(dataset=mnist_ds, batch_size=len(mnist_ds))\n",
              "x_all_ts2, t_all_ts2 = list(mnist_dl)[0]\n",
              "print(f\"{time()-st}   images:{x_all_ts2.size()}  targets:{t_all_ts2.size()} \")\n",
              "\n",
              "\n",
              "First...\n",
              "19.573785066604614   images:torch.Size([60000, 1, 16, 16])  targets:torch.Size([60000]) \n",
              "Second...\n",
              "16.826476573944092   images:torch.Size([60000, 1, 16, 16])  targets:torch.Size([60000]) \n",
              "\n",
              "\n",
              "Please let me know if you find better ones.\n",
              " <P> The problem is as I wrote in the comment, skimage requires the data to be ndarray but you give it a torch tensor thus that error.\n",
              "Try this\n",
              "    train_trans = transforms.Compose([\n",
              "        transforms.ToPILImage(),\n",
              "        transforms.RandomResizedCrop(224),\n",
              "        transforms.RandomHorizontalFlip(),\n",
              "        transforms.ToTensor(),\n",
              "        transforms.Normalize(img_mean, img_std),\n",
              "        lambda x: np.rollaxis(x.numpy(), 0, 3)\n",
              "    ])\n",
              "\n",
              "Edit\n",
              "This is basically transform the output to ndarray and change channel axis.\n",
              "But as you can see it's not the best way to fix things since you have to transform PIL image to tensor then transform tensor to ndarray and then transform ndarray back to tensor again.\n",
              "The better way to do this is transform PIL image directly to ndarray and normalize that, for example.\n",
              "in getitem\n",
              "    if self.transform:\n",
              "      image = self.transform(image)\n",
              "\n",
              "      # add these\n",
              "      image = np.array(image)\n",
              "\n",
              "      mean = [0.485, 0.456, 0.406]\n",
              "      std = [0.229, 0.224, 0.225]\n",
              "\n",
              "      x[..., 0] -= mean[0]\n",
              "      x[..., 1] -= mean[1]\n",
              "      x[..., 2] -= mean[2]\n",
              "      x[..., 0] /= std[0]\n",
              "      x[..., 1] /= std[1]\n",
              "      x[..., 2] /= std[2]\n",
              "\n",
              "    # these are your code\n",
              "    hog_, hog_image = hog(\n",
              "\n",
              "And in transform just use\n",
              "    train_trans = transforms.Compose([\n",
              "        transforms.ToPILImage(),\n",
              "        transforms.RandomResizedCrop(224),\n",
              "        transforms.RandomHorizontalFlip(),\n",
              "    ])\n",
              "\n",
              "Edit2\n",
              "Refer to this line. You need to either add visualize=True in hog() or remove , hog_image. If you don't need hog_image then the latter is preferred.\n",
              "    hog_, hog_image = hog(\n",
              "        image, visualize=True,\n",
              "\n",
              "hog_ = hog(\n",
              "\n",
              " <P> You need to feed images to net the same as in training: that is, you should apply exactly the same transformations to get similar results.  \n",
              "\n",
              "Assuming your net was trained using this code (or similar), you can see that an input image (for validation) undergoes the following transformations:\n",
              "\n",
              "\n",
              "transforms.Compose([\n",
              "            transforms.Resize(256),\n",
              "            transforms.CenterCrop(224),\n",
              "            transforms.ToTensor(),\n",
              "            normalize,\n",
              "        ])),\n",
              "\n",
              "\n",
              "\n",
              "Following torchvision.transforms docs you can see that an input image goes through:\n",
              "\n",
              "\n",
              "Resizing to 256x256 pix  \n",
              "Cropping 224x224 rect from the center of the image\n",
              "The image is converted from uint8 datatype to float in range [0, 1], and transposed to 3-by-224-by-224 array  \n",
              "The image is normalize by subtracting mean and dividing by std.\n",
              "\n",
              "\n",
              "You can do all this manually to any image\n",
              "\n",
              "import numpy as np\n",
              "from PIL import Image\n",
              "\n",
              "pil_img = Image.open('image.jpg').resize((256, 256), Image.BILINEAR)  # read and resize\n",
              "# center crop\n",
              "w, h = pil_img.size\n",
              "i = int(round((h - 224) / 2.))\n",
              "j = int(round((w - 224) / 2.))\n",
              "pil_img = pil_img.crop((j, i, j+224, i+224))\n",
              "np_img = np.array(pil_img).astype(np.float32) / 255.\n",
              "np_img = np.transpose(np_img, (2, 0, 1))  \n",
              "# normalize\n",
              "mean = [0.485, 0.456, 0.406]\n",
              "std = [0.229, 0.224, 0.225]\n",
              "for c in range(3):\n",
              "  np_img = (np_img[c, ...] - mean[c]) / std[c]\n",
              "\n",
              "\n",
              "Once you have np_img ready for your model, you can run a feed forward pass:\n",
              "\n",
              "pred = model(np_img[None, ...])  # note that we add a singleton leading dim for batch\n",
              "\n",
              " <P> Change the order of\n",
              "t_img = transforms(image)\n",
              "t_img = torch.cat((t_img, t_img, t_img), 0)\n",
              "\n",
              "to\n",
              "t_img = torch.cat((image, image, image), 0)\n",
              "t_img = transforms(t_img)\n",
              "\n",
              "transforms expects input to be of shape [C, W, H]\n",
              " <P> Assuming your image is a numpy.array upon loading (please see comments for explanation of each step):\n",
              "\n",
              "import numpy as np\n",
              "import torch\n",
              "\n",
              "# Assuming you have 3 color channels in your image\n",
              "# Assuming your data is in Width, Height, Channels format\n",
              "numpy_img = np.random.randint(low=0, high=255, size=(512, 512, 3))\n",
              "\n",
              "# Transform to tensor\n",
              "tensor_img = torch.from_numpy(numpy_img)\n",
              "# PyTorch takes images in format Channels, Width, Height\n",
              "# We have to switch their dimensions using `permute`\n",
              "tensor_img = tensor_img.permute(2, 0, 1)\n",
              "tensor_img.shape # Shape [3, 512, 512]\n",
              "\n",
              "# Layers always need batch as first dimension (even for one image)\n",
              "# unsqueeze will add it for you    \n",
              "ready_tensor_img = tensor_img.unsqueeze(dim=0)\n",
              "ready_tensor_img.shape # Shape [1, 3, 512, 512]\n",
              "\n",
              "pooling = torch.nn.MaxPool2d(kernel_size=3, stride=1)\n",
              "\n",
              "# You need to cast your image to float as\n",
              "# pooling is not implemented for Tensors of type long\n",
              "new_img = pooling(ready_tensor_img.float())\n",
              "\n",
              "\n",
              "If your image is black and white you would need shape [1, 1, 512, 512] (single channel only), you can't leave/squeeze those dimensions, they always have to be there for any torch.nn.Module!\n",
              "\n",
              "To transform tensor into image again you could use similar steps:\n",
              "\n",
              "# Cast to long and squeeze batch dimension\n",
              "no_batch = new_img.long().squeeze(dim=0)\n",
              "\n",
              "# Unpermute\n",
              "width_height_channels = no_batch.permute(1, 2, 0)\n",
              "width_height_channels.shape  # Shape: [510, 510, 3]\n",
              "\n",
              "# Cast to numpy and you have your image\n",
              "final_image = width_height_channels.numpy()\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col0\" class=\"data row12 col0\" >In what platform do the modules Conv2d() and Linear() run?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col1\" class=\"data row12 col1\" >PTX. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch will make NvCC fall back to building kernels with the newest VERSION of PTX your NVCC does support (See below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col2\" class=\"data row12 col2\" >FP32</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row12_col3\" class=\"data row12 col3\" ><P> Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better. Note that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). <P> SGTM <P> By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py <P> By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better. Note that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). <P> friday <P> All arguments are forwarded to the setuptools.Extension constructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py <P> logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              " <P> Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library. All arguments are forwarded to the setuptools.Extension constructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better. <P> The Pytorch Previous Version Log offers installations for CUDA versions 9.2, 10.1, 10.2 and 11. Therefore, CUDA 10 is probably not officially supported.\n",
              " <P> The GPU version of PyTorch is actually a superset of the CPU PyTorch. You can use the GPU PyTorch on a CPU, but you cannot use the CPU PyTorch on a GPU. So in your case, installing just the GPU version of PyTorch would be sufficient.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col0\" class=\"data row13 col0\" >create representation of questions using LSTM via a pre-trained word embedding such as GloVe</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col1\" class=\"data row13 col1\" >First of all for a GRU (RNN) to be efficient, you may need more data to train.\n",
              "\n",
              "Second, it seems that you have a problem with the embedding. It looks like, the mapping vocabulary['id2letter'] does not work, otherwise you would obtain sequences of tags like headtitletitletitle, instead of p111.\n",
              "EDIT\n",
              "\n",
              "I have trained this character-level GRU network on the html source code of this page for 1700 epochs. And here an example 2000-character excerpt of what it generates:\n",
              "\n",
              "A+Implexementation--No</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col2\" class=\"data row13 col2\" >Please see torch embedding tutorial and use embedding with keras for knowledge about word embeddings.\n",
              "\n",
              "Basically nn.Embedding(vocab_size, embed_size) is a matrix vocab_size*embed_size, where each row corresponds to a word's representation.\n",
              "\n",
              "In order to know which word correspond to which row you should define a vocabulary that can transform a word into an index (for example a python dictionary {'hello': 0, 'word': 1}).\n",
              "\n",
              "And before that to transform a sentence into word (in order to compute the vocabulary) you need to tokenize the sentence (using nltk.word_tokenize or str.split() for example).\n",
              "\n",
              "Though torch.nn.Embedding expects a Tensor, so if you want to process multiple sentence into batches, and the sentence have different length you will need to pad the sentence with empty tokens in order to fit them into a Tensor.\n",
              "\n",
              "# This is pseudo code\n",
              "\n",
              "# Preprocess data\n",
              "documents = ['first sentence', 'the second sentence']\n",
              "tokenized_documents = [d.split(' ') for d in documents]\n",
              "# Create vocabulary and add a special token for padding\n",
              "words = [w for d in documents for word in tokenized_documents]\n",
              "vocabulary = {w: i+1 for i, w in enumerate(set(words))}\n",
              "vocabulary['PAD'] = 0\n",
              "indexed_documents = [[vocabulary[w] for w in d] for d in tokenized_documents]\n",
              "# indexed_documents will look like : [[0, 1], [2, 3, 1]]\n",
              "padded_documents = torch.nn.utils.rnn.pad_sequence(\n",
              "    indexed_documents,\n",
              "    padding_value=vocabulary['PAD'])\n",
              "\n",
              "# Data can be fed to the neural network\n",
              "model.word_embeddings(torch.tensor(padded_documents))</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row13_col3\" class=\"data row13 col3\" ><P> In order to obtain the sentence embedding from the T5, you need to take the take the last_hidden_state from the T5 encoder output:\n",
              "model.encoder(input_ids=s, attention_mask=attn, return_dict=True)\n",
              "pooled_sentence = output.last_hidden_state # shape is [batch_size, seq_len, hidden_size]\n",
              "# pooled_sentence will represent the embeddings for each word in the sentence\n",
              "# you need to sum/average the pooled_sentence\n",
              "pooled_sentence = torch.mean(pooled_sentence, dim=1)\n",
              "\n",
              "You have now a sentence embeddings from T5\n",
              " <P> The documentation says the following\n",
              "\n",
              "\n",
              "  This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
              "\n",
              "\n",
              "So if you want to feed in a sentence, you give a LongTensor of indices, each corresponding to a word in the vocabulary, which the nn.Embedding layer will map into word vectors going forward. \n",
              "\n",
              "Here's an illustration\n",
              "\n",
              "test_voc = [\"ok\", \"great\", \"test\"]\n",
              "# The word vectors for \"ok\", \"great\" and \"test\"\n",
              "# are at indices, 0, 1 and 2, respectively.\n",
              "\n",
              "my_embedding = torch.rand(3, 50)\n",
              "e = nn.Embedding.from_pretrained(my_embedding)\n",
              "\n",
              "# LongTensor of indicies corresponds to a sentence,\n",
              "# reshaped to (1, 3) because batch size is 1\n",
              "my_sentence = torch.tensor([0, 2, 1]).view(1, -1)\n",
              "\n",
              "res = e(my_sentence)\n",
              "print(res.shape)\n",
              "# = torch.Size([1, 3, 50])\n",
              "# 1 is the batch dimension, and there's three vectors of length 50 each\n",
              "\n",
              "\n",
              "In terms of RNNs, next you can feed that tensor into your RNN module, e.g\n",
              "\n",
              "lstm = nn.LSTM(input_size=50, hidden_size=5, batch_first=True)\n",
              "output, h = lstm(res)\n",
              "print(output.shape)\n",
              "# = torch.Size([1, 3, 5])\n",
              "\n",
              "\n",
              "I also recommend you look into torchtext. It can automatate some of the stuff you will have to do manually otherwise.\n",
              " <P> First of all for a GRU (RNN) to be efficient, you may need more data to train. \n",
              "\n",
              "Second, it seems that you have a problem with the embedding. It looks like, the mapping vocabulary['id2letter'] does not work, otherwise you would obtain \n",
              "sequences of tags like headtitletitletitle, instead of p111.\n",
              "\n",
              "EDIT\n",
              "\n",
              "I have trained this character-level GRU network on the html source code of this page for 1700 epochs. And here an example 2000-character excerpt of what it generates:\n",
              "\n",
              "A+Implexementation--nope bande that shoos/td/trtrtd class=\"line-number\" value=\"296\"/tdtd class=\"line-content\"              /td/trtrtd class=\"line-number\" value=\"1437\"/tdtd class=\"line-content\"                    span class=\"html-tag\"lt;/agt;/span/td/trtrtd class=\"line-number\" value=\"755\"/tdtd class=\"line-content\"                /td/trtrtd class=\"line-number\" value=\"584\"/tdtd class=\"line-content\"    span class=\"html-tag\"lt;a span class=\"html-attribute-name\"data-controller/span=\"span class=\"html-attribute-value\"footer__menu__link/span\"gt;/spanspan class=\"html-tag\"lt;div span class=\"html-attribute-name\"data-target/span=\"span class=\"html-attribute-value\"release__line/span\"gt;/spanspan class=\"html-tag\"lt;a span class=\"html-attribute-name\"class/span=\"span class=\"html-attribute-value\"/hase__version-date/span\"gt;/span/td/trtrtd class=\"line-number\" value=\"174\"/tdtd class=\"line-content\"br/td/trtrtd class=\"line-number\" value=\"1315\"/tdtd class=\"line-content\"Bule and the use the twith a hoas suiecode excess ardates/td/trtrtd class=\"line-number\" value=\"1003\"/tdtd class=\"line-content\"span class=\"html-tag\"lt;/agt;/span/td/trtrtd class=\"line-number\" value=\"129\"/tdtd class=\"line-content\"              /td/trtrtd class=\"line-number\" value=\"269\"/tdtd class=\"line-content\"              span class=\"html-tag\"lt;/ulgt;/span/td/trtrtd class=\"line-number\" value=\"591\"/tdtd class=\"line-content\"                span class=\"html-tag\"lt;/divgt;/span/td/trtrtd class=\"line-number\" value=\"553\"/tdtd class=\"line-content\"    span class=\"html-tag\"lt;div span class=\"html-attribute-name\"href/span=\"a class=\"html-attribute-value html-external-link\" target__link/tdtd class=\"line-content\"              /td/trtrtd class=\"line-number\" value=\"103\"/tdtd class=\"line-content\"    /td/trtrtd cla\n",
              "\n",
              "\n",
              "I hope, this helps.\n",
              " <P> You can directly access these properties and print them: model = nn.LSTM(10,10, 2,0.3, bidirectional=True) <P> I am not sure what do you mean by word2vec algorithm with LSTM because the original word2vec algorithm does not use LSTMs and uses directly embeddings to predict surrounding words.\n",
              "\n",
              "Anyway, it seems you have multiple categorical variables to embed. In the example, it is word ID, color ID, and font size (if you round it to integer values). You have two option:\n",
              "\n",
              "\n",
              "You can create new IDs for all possible combinations of your features and use nn.Embedding for them. There is however a risk that most of the IDs will appear too sparsely in the data to learn reliable embeddings.\n",
              "Have separate embedding for each of the features. Then, you will need to combine the embeddings for the features together. You have basically three options how to do it:\n",
              "\n",
              "\n",
              "Just concatenate the embeddings and let the following layers of the network to resolve the combination.\n",
              "Choose the same embedding dimension for all features and average them. (I would start with this one probably.)\n",
              "Add a nn.Dense layer (or two, the first one with ReLU activation and the second without activation) that will explicitly combine the embeddings for your features.\n",
              "\n",
              "\n",
              "\n",
              "If you need to include continuous features that cannot be discretized,  you can always take the continuous features, apply a layer or two on top of them and combine them with the embeddings of the discrete features.\n",
              " <P> One can either learn embeddings during the task, finetune them for task at hand or leave as they are (provided they have been learned in some fashion before).\n",
              "\n",
              "In the last case, with standard embeddings like word2vec one eventually finetunes (using small learning rate), but uses vocabulary and embeddings provided. When it comes to current SOTA like BERT fine-tuning on your data should always be done, but in unsupervised way (as trained originally).\n",
              "\n",
              "Easiest way to use them is static method of torch.nn.Embedding.from_pretrained (docs) and provide Tensor with your pretrained data.\n",
              "\n",
              "If you want the layer to be trainable, pass freeze=False, by default it's not as you want.\n",
              " <P> Please see torch embedding tutorial and use embedding with keras for knowledge about word embeddings.\n",
              "Basically nn.Embedding(vocab_size, embed_size) is a matrix vocab_size*embed_size, where each row corresponds to a word's representation.\n",
              "In order to know which word correspond to which row you should define a vocabulary that can transform a word into an index (for example a python dictionary {'hello': 0, 'word': 1}).\n",
              "And before that to transform a sentence into word (in order to compute the vocabulary) you need to tokenize the sentence (using nltk.word_tokenize or str.split() for example).\n",
              "Though torch.nn.Embedding expects a Tensor, so if you want to process multiple sentence into batches, and the sentence have different length you will need to pad the sentence with empty tokens in order to fit them into a Tensor.\n",
              "# This is pseudo code\n",
              "\n",
              "# Preprocess data\n",
              "documents = ['first sentence', 'the second sentence']\n",
              "tokenized_documents = [d.split(' ') for d in documents]\n",
              "# Create vocabulary and add a special token for padding\n",
              "words = [w for d in documents for word in tokenized_documents]\n",
              "vocabulary = {w: i+1 for i, w in enumerate(set(words))}\n",
              "vocabulary['PAD'] = 0\n",
              "indexed_documents = [[vocabulary[w] for w in d] for d in tokenized_documents]\n",
              "# indexed_documents will look like : [[0, 1], [2, 3, 1]]\n",
              "padded_documents = torch.nn.utils.rnn.pad_sequence(\n",
              "    indexed_documents,\n",
              "    padding_value=vocabulary['PAD'])\n",
              "\n",
              "# Data can be fed to the neural network\n",
              "model.word_embeddings(torch.tensor(padded_documents))\n",
              "\n",
              " <P> According to the PyTorch docs:\n",
              "\n",
              "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
              "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
              "\n",
              "In short, nn.Embedding embeds a sequence of vocabulary indices into a new embedding space. You can indeed roughly understand this as a word2vec style mechanism.\n",
              "As a dummy example, let's create an embedding layer that takes as input a total of 10 vocabularies (i.e. the input data only contains a total of 10 unique tokens), and returns embedded word vectors living in 5-dimensional space. In other words, each word is represented as 5-dimensional vectors. The dummy data is a sequence of 3 words with indices 1, 2, and 3, in that order.\n",
              " embedding = nn.Embedding(10, 5)\n",
              " embedding(torch.tensor([1, 2, 3]))\n",
              "tensor([[-0.7077, -1.0708, -0.9729,  0.5726,  1.0309],\n",
              "        [ 0.2056, -1.3278,  0.6368, -1.9261,  1.0972],\n",
              "        [ 0.8409, -0.5524, -0.1357,  0.6838,  3.0991]],\n",
              "       grad_fn=EmbeddingBackward)\n",
              "\n",
              "You can see that each of the three words are now represented as 5-dimensional vectors. We also see that there is a grad_fn function, which means that the weights of this layer will be adjusted through backprop. This answers your question of whether embedding layers are trainable: the answer is yes. And indeed this is the whole point of embedding: we expect the embedding layer to learn meaningful representations, the famous example of king - man = queen being the classic example of what these embedding layers can learn.\n",
              "\n",
              "Edit\n",
              "The embedding layer is, as the documentation states, a simple lookup table from a matrix. You can see this by doing\n",
              " embedding.weight\n",
              "Parameter containing:\n",
              "tensor([[-1.1728, -0.1023,  0.2489, -1.6098,  1.0426],\n",
              "        [-0.7077, -1.0708, -0.9729,  0.5726,  1.0309],\n",
              "        [ 0.2056, -1.3278,  0.6368, -1.9261,  1.0972],\n",
              "        [ 0.8409, -0.5524, -0.1357,  0.6838,  3.0991],\n",
              "        [-0.4569, -1.9014, -0.0758, -0.6069, -1.2985],\n",
              "        [ 0.4545,  0.3246, -0.7277,  0.7236, -0.8096],\n",
              "        [ 1.2569,  1.2437, -1.0229, -0.2101, -0.2963],\n",
              "        [-0.3394, -0.8099,  1.4016, -0.8018,  0.0156],\n",
              "        [ 0.3253, -0.1863,  0.5746, -0.0672,  0.7865],\n",
              "        [ 0.0176,  0.7090, -0.7630, -0.6564,  1.5690]], requires_grad=True)\n",
              "\n",
              "You will see that the first, second, and third rows of this matrix corresponds to the result that was returned in the example above. In other words, for a vocabulary whose index is n, the embedding layer will simply \"lookup\" the nth row in its weights matrix and return that row vector; hence the lookup table.\n",
              " <P> You don't need neither a neural network nor word embeddings. Use parsed trees with NLTK, where intents are Verbs V acting on entities (N) in a given utterance:\n",
              "\n",
              "\n",
              "\n",
              "To classify a sentence, then you can use a Neural Net. I personally like BERT in fast.ai. Once again, you won't need embeddings to run the classification, and you can do it in multilanguage:\n",
              "\n",
              "Fast.ai_BERT_ULMFit\n",
              "\n",
              "Also, you can use Named Entity Recognition if you are working on a chatbot, to guide conversations.\n",
              " <P> In allennlp you have access to the self.vocab attribute with Vocabulary. get_token_from_index.\n",
              "Usually to select a token from the logits one would apply a softmax (in order to have all the probability summing to 1) and then pick the most probable one.\n",
              "If you want to decode sequences from a model maybe you should look into [BeamSearch] (https://docs.allennlp.org/master/api/nn/beam_search/#beamsearch).\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col0\" class=\"data row14 col0\" >Testing an implementation of an LSTM in Pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col1\" class=\"data row14 col1\" >There have multiple problem in your code, for simplicity, I just give you one well defined model instead, following code build a LSTM Autoencoder that reconstruct the inputs with shape (batch_size, timesteps, number_of_features_at_each_timesteps):\n",
              "import torch\n",
              "from torch import nn\n",
              "\n",
              "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
              "\n",
              "class Encoder(nn.Module):\n",
              "  def __init__(self, seq_len, n_features, embedding_dim</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col2\" class=\"data row14 col2\" >The first argument to super() should be class itself, not a different class.\n",
              "\n",
              "class LSTMCell(nn.Module):\n",
              "\n",
              "    def __init__(self, input_size, hidden_size, bias=True):\n",
              "        super(LSTM, self).__init__()\n",
              "#             ^^^^ self is not an instance of LSTM but LSTMCell\n",
              "\n",
              "\n",
              "It should be:\n",
              "\n",
              "super(LSTMCell, self).__init__()\n",
              "\n",
              "\n",
              "Since Python 3 you can omit the arguments to super to get the same result (as you have done in the LSTM class):\n",
              "\n",
              "super().__init__()\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row14_col3\" class=\"data row14 col3\" ><P> If you check out the PyTorch LSTM documentation, you will see that the LSTM equations are applied to each timestep in your sequence. nn.LSTM will internally obtain the seq_len dimension and optimize from there, so you do not need to provide the number of time steps.\n",
              "\n",
              "At the moment, the line\n",
              "\n",
              "out = self.fc1(out[:, -1, :])\n",
              "\n",
              "\n",
              "is selecting the final hidden state (corresponding to time step 80) and then this is being projected onto a space of size input_dim.\n",
              "\n",
              "To output a sequence of length 80, you should have an output for each hidden state. All hidden states are stacked in out so you can simply use\n",
              "\n",
              "out = self.fc1(out)\n",
              "out = self.fc2(out)\n",
              "\n",
              "\n",
              "I would also note that if you must have two fully connected layers after encoding in your hidden state, you should use a non-linearity in between otherwise this is the equivalent of just one layer but with more parameters.\n",
              " <P> As Neaabfi answered hidden[-1] is correct. To be more specific to your question, as the docs wrote:\n",
              "\n",
              "output, (h_n, c_n) = self.lstm(x_pack) # batch_first = True\n",
              "\n",
              "# h_n is a vector of shape (num_layers * num_directions, batch, hidden_size)\n",
              "\n",
              "\n",
              "In your case, you have a stack of 2 LSTM layers with only forward direction, then:\n",
              "\n",
              "h_n shape is (num_layers, batch, hidden_size)\n",
              "\n",
              "\n",
              "Probably, you may prefer the hidden state h_n of the last layer, then **here is what you should do:\n",
              "\n",
              "output, (h_n, c_n) = self.lstm(x_pack)\n",
              "h = h_n[-1] # h of shape (batch, hidden_size)\n",
              "y = self.linear(h)\n",
              "\n",
              "\n",
              "Here is the code which wraps any recurrent layer LSTM, RNN or GRU into DynamicRNN. DynamicRNN has a capacity of performing recurrent computations on sequences of varied lengths without any care about the order of lengths.\n",
              " <P> There have multiple problem in your code, for simplicity, I just give you one well defined model instead, following code build a LSTM Autoencoder that reconstruct the inputs with shape (batch_size, timesteps, number_of_features_at_each_timesteps):\n",
              "import torch\n",
              "from torch import nn\n",
              "\n",
              "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
              "\n",
              "class Encoder(nn.Module):\n",
              "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
              "    super(Encoder, self).__init__()\n",
              "\n",
              "    self.seq_len, self.n_features = seq_len, n_features\n",
              "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
              "\n",
              "    self.rnn1 = nn.LSTM(\n",
              "      input_size=n_features,\n",
              "      hidden_size=self.hidden_dim,\n",
              "      num_layers=1,\n",
              "      batch_first=True\n",
              "    )\n",
              "    self.rnn2 = nn.LSTM(\n",
              "      input_size=self.hidden_dim,\n",
              "      hidden_size=self.embedding_dim,\n",
              "      num_layers=1,\n",
              "      batch_first=True\n",
              "    )\n",
              "\n",
              "  def forward(self, x):\n",
              "    x, (_, _) = self.rnn1(x)\n",
              "    x, (hidden_n, _) = self.rnn2(x)\n",
              "    return hidden_n\n",
              "\n",
              "class Decoder(nn.Module):\n",
              "  def __init__(self, seq_len, input_dim=64, n_features=1):\n",
              "    super(Decoder, self).__init__()\n",
              "\n",
              "    self.seq_len, self.input_dim = seq_len, input_dim\n",
              "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
              "\n",
              "    self.rnn1 = nn.LSTM(\n",
              "      input_size=input_dim,\n",
              "      hidden_size=input_dim,\n",
              "      num_layers=1,\n",
              "      batch_first=True\n",
              "    )\n",
              "    self.rnn2 = nn.LSTM(\n",
              "      input_size=input_dim,\n",
              "      hidden_size=self.hidden_dim,\n",
              "      num_layers=1,\n",
              "      batch_first=True\n",
              "    )\n",
              "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
              "\n",
              "  def forward(self, x):\n",
              "    x = x.repeat(self.seq_len, 1, 1)\n",
              "    x = x.permute(1, 0, 2)\n",
              "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
              "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
              "    return self.output_layer(x)\n",
              "\n",
              "class RecurrentAutoencoder(nn.Module):\n",
              "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
              "    super(RecurrentAutoencoder, self).__init__()\n",
              "\n",
              "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
              "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
              "\n",
              "  def forward(self, x):\n",
              "    print(\"Inputs size:\", x.size())\n",
              "    x = self.encoder(x)\n",
              "    print(\"Representation size: \", x.size())\n",
              "    x = self.decoder(x)\n",
              "    print(\"Outputs size: \", x.size())\n",
              "    return x\n",
              "\n",
              "batch_n = 5\n",
              "seq_len = 10\n",
              "n_features = 3\n",
              "inputs = torch.randn(batch_n, seq_len, n_features).to(device)\n",
              "\n",
              "model = RecurrentAutoencoder(seq_len, n_features).to(device)\n",
              "y = model(inputs)\n",
              "\n",
              "Outputs:\n",
              "Inputs size: torch.Size([5, 10, 3])\n",
              "Representation size:  torch.Size([1, 5, 64])\n",
              "Outputs size:  torch.Size([5, 10, 3])\n",
              "\n",
              "Beware the representation (i.e outputs of encoder) have shape (1, batch_size, embedding_dim)\n",
              " <P> From the documentation of tourch.nn.RNN, the RNN is actually an Elman network, and have the following properties seen here.\n",
              "The output of an Elman network is only dependent on the hidden state, while the hidden state is dependent on the last input and the previous hidden state.\n",
              "\n",
              "Since we have set “h_state = h_state.data”, we actually use the hidden state of the last sequence to predict the first state of the new sequence, which will result in an output heavily dependent on the last output of the previous sequence (which was 0). The Elman network can’t separate if we are in the beginning of the sequence or at the end, it only \"sees\" the state and last input.\n",
              "\n",
              "To fix this we can insted set “h_state = None”.\n",
              "Now every new sequence start with an empty state. This result in the following prediction (where green line again shows the prediction).\n",
              "Now we start off at 1, but quickly dips down to 0 before the puls push it back up again.\n",
              "The Elman network can account for some time dependency, but it is not good at remembering long term dependencies, and converge towards an \"most common output\" for that input.\n",
              "\n",
              "So to fix this problem, I suggest using a network which is well known for handling long term dependencies well, namely the Long short-term memory (LSTM) rnn, for more information see torch.nn.LSTM. Keep \"h_state = None\" and change torch.nn.RNN to torch.nn.LSTM.\n",
              "\n",
              "for complete code and plot see below\n",
              "\n",
              "import torch\n",
              "import numpy, math\n",
              "import matplotlib.pyplot as plt\n",
              "\n",
              "nofSequences = 5\n",
              "maxLength = 130\n",
              "\n",
              "# Generate training data\n",
              "x_np = numpy.zeros((nofSequences,maxLength,1))\n",
              "y_np = numpy.zeros((nofSequences,maxLength))\n",
              "numpy.random.seed(1)\n",
              "for i in range(0,nofSequences):\n",
              "    startPos = numpy.random.random()*50\n",
              "    for j in range(0,maxLength):\n",
              "        if j=startPos and jstartPos+10:\n",
              "            x_np[i,j,0] = math.sin((j-startPos)*math.pi/10)\n",
              "        else:\n",
              "            x_np[i,j,0] = 0.0\n",
              "        if jstartPos+10:\n",
              "            y_np[i,j] = 1\n",
              "        else:\n",
              "            y_np[i,j] = 0\n",
              "\n",
              "\n",
              "# Define the neural network\n",
              "INPUT_SIZE = 1\n",
              "class RNN(torch.nn.Module):\n",
              "    def __init__(self):\n",
              "        super(RNN, self).__init__()\n",
              "\n",
              "        self.rnn = torch.nn.LSTM(\n",
              "            input_size=INPUT_SIZE,\n",
              "            hidden_size=16,     # rnn hidden unit\n",
              "            num_layers=1,       # number of rnn layer\n",
              "            batch_first=True,\n",
              "        )\n",
              "        self.out = torch.nn.Linear(16, 2)\n",
              "\n",
              "    def forward(self, x, h_state):\n",
              "        r_out, h_state = self.rnn(x, h_state)\n",
              "\n",
              "        outs = []    # save all predictions\n",
              "        for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
              "            outs.append(self.out(r_out[:, time_step, :]))\n",
              "        return torch.stack(outs, dim=1), h_state\n",
              "\n",
              "# Learn the network\n",
              "rnn = RNN()\n",
              "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
              "h_state = None      # for initial hidden state\n",
              "\n",
              "x = torch.Tensor(x_np)    # shape (batch, time_step, input_size)\n",
              "y = torch.Tensor(y_np).long()\n",
              "\n",
              "torch.manual_seed(2)\n",
              "numpy.random.seed(2)\n",
              "\n",
              "for step in range(100):\n",
              "\n",
              "    prediction, h_state = rnn(x, h_state)   # rnn output\n",
              "\n",
              "    # !! next step is important !!\n",
              "    h_state = None        \n",
              "\n",
              "    loss = torch.nn.CrossEntropyLoss()(prediction.reshape((-1,2)),torch.autograd.Variable(y.reshape((-1,))))         # calculate loss\n",
              "    optimizer.zero_grad()                   # clear gradients for this training step\n",
              "    loss.backward()                         # backpropagation, compute gradients\n",
              "    optimizer.step()                        # apply gradients\n",
              "\n",
              "    errTrain = (prediction.max(2)[1].data != y).float().mean()\n",
              "    print(\"Error Training:\",errTrain.item())\n",
              "\n",
              "\n",
              "###############################################################################\n",
              "steps = range(0,maxLength)\n",
              "plotChoice = 3\n",
              "\n",
              "plt.figure(1, figsize=(12, 5))\n",
              "plt.ion()           # continuously plot\n",
              "\n",
              "plt.plot(steps, y_np[plotChoice,:].flatten(), 'r-')\n",
              "plt.plot(steps, numpy.argmax(prediction.detach().numpy()[plotChoice,:,:],axis=1), 'g-')\n",
              "plt.plot(steps, x_np[plotChoice,:,0].flatten(), 'b-')\n",
              "\n",
              "plt.ioff()\n",
              "plt.show()\n",
              "\n",
              "\n",
              "\n",
              " <P> You won't be able to use a nn.RNN inside a nn.Sequential since nn.LSTM layers will output a tuple containing (1) the output features and (2) hidden states and cell states.\n",
              "The output must first be unpacked in order to use the output features in your subsequent layer: nn.Linear. Something as, if your interested in the hidden states and cell states:\n",
              "rnn = nn.LSTM(300, 300)\n",
              "output, (h_n, c_n) = rnn(x)\n",
              "\n",
              "You could define a custom nn.Module and implement a simple forward function:\n",
              "class Model(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Model, self).__init__()\n",
              "\n",
              "        self.rnn = nn.LSTM(300, 300)\n",
              "        \n",
              "        self.body = nn.Sequential(\n",
              "          nn.Linear(300, 100),\n",
              "          nn.ReLU(),\n",
              "          nn.Linear(100, 7)) # - had it set to in_features=300\n",
              "\n",
              "    def forward(self, x):\n",
              "        x, _ = self.rnn(x) # - ignore second output\n",
              "        x = self.body(x)\n",
              "        return x\n",
              "\n",
              "Such that:\n",
              " model = Model()\n",
              " s = torch.ones(1, 50, 300)\n",
              "\n",
              " model(s).shape\n",
              "torch.Size([1, 50, 7])\n",
              "\n",
              " <P> The LSTM function in Pytorch returns not just the output of the last timestep but all outputs instead (this is useful in some cases). So in your example you seem to have exactly 100 timesteps (the amount of timesteps is just your sequence length).\n",
              "But since you are doing classification you just care about the last output. You can normally get it like this:\n",
              "outputs, _ = self.lstm(embeddings)\n",
              "# shape: batch_size x 100 x 15\n",
              "output = outputs[:, -1]    \n",
              "# shape: batch_size x 1 x 15\n",
              "\n",
              " <P> 1. Initializing hidden states\n",
              "\n",
              "In your source code you are using init_hidden_encoder and init_hidden_decoder functions to zero hidden states of both recurrent units in every forward pass.\n",
              "\n",
              "In PyTorch you don't have to do that, if no initial hidden state is passed to RNN-cell (be it LSTM, GRU or RNN from the ones currently available by default in PyTorch), it is implicitly fed with zeroes.\n",
              "\n",
              "So, to obtain the same code as your initial solution (which simplifies next parts), I will scrap unneeded parts, which leaves us with the model seen below:\n",
              "\n",
              "class LSTM(nn.Module):\n",
              "    def __init__(self, input_dim, latent_dim, num_layers):\n",
              "        super(LSTM, self).__init__()\n",
              "        self.input_dim = input_dim\n",
              "        self.latent_dim = latent_dim\n",
              "        self.num_layers = num_layers\n",
              "\n",
              "        self.encoder = nn.LSTM(self.input_dim, self.latent_dim, self.num_layers)\n",
              "\n",
              "        self.decoder = nn.LSTM(self.latent_dim, self.input_dim, self.num_layers)\n",
              "\n",
              "    def forward(self, input):\n",
              "        # Encode\n",
              "        _, (last_hidden, _) = self.encoder(input)\n",
              "        encoded = last_hidden.repeat(5, 1, 1)\n",
              "\n",
              "        # Decode\n",
              "        y, _ = self.decoder(encoded)\n",
              "        return torch.squeeze(y)\n",
              "\n",
              "\n",
              "Addition of torch.squeeze\n",
              "\n",
              "We don't need any superfluous dimensions (like the 1 in [5,1,1]).\n",
              "Actually, it's the clue to your results equal to 0.2\n",
              "\n",
              "Furthermore, I left input reshape out of the network (in my opinion, network should be fed with input ready to be processed), to separate strictly both tasks (input preparation and model itself). \n",
              "\n",
              "This approach gives us the following setup code and training loop:\n",
              "\n",
              "model = LSTM(input_dim=1, latent_dim=20, num_layers=1)\n",
              "loss_function = nn.MSELoss()\n",
              "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
              "\n",
              "y = torch.Tensor([[0.0], [0.1], [0.2], [0.3], [0.4]])\n",
              "# Sequence x batch x dimension\n",
              "x = y.view(len(y), 1, -1)\n",
              "\n",
              "while True:\n",
              "    y_pred = model(x)\n",
              "    optimizer.zero_grad()\n",
              "    loss = loss_function(y_pred, y)\n",
              "    loss.backward()\n",
              "    optimizer.step()\n",
              "    print(y_pred)\n",
              "\n",
              "\n",
              "Whole network is identical to yours (for now), except it is more succinct and readable.\n",
              "\n",
              "2. What we want, describing network changes\n",
              "\n",
              "As your provided Keras code indicates, what we want to do (and actually you are doing it correctly) is to obtain last hiddden state from the encoder (it encodes our entire sequence) and decode the sequence from this state to obtain the original one. \n",
              "\n",
              "BTW. this approach is called sequence to sequence or seq2seq for short (often used in tasks like language translation). Well, maybe a variation of that approach, but I would classify it as that anyway.\n",
              "\n",
              "PyTorch provides us the last hidden state as a separate return variable from RNNs family.\n",
              "I would advise against yours encoded[-1]. The reason for it would be bidirectional and multilayered approach. Say, you wanted to sum bidirectional output, it would mean a code along those lines \n",
              "\n",
              "# batch_size and hidden_size should be inferred cluttering the code further    \n",
              "encoded[-1].view(batch_size, 2, hidden_size).sum(dim=1)\n",
              "\n",
              "\n",
              "And that's why the line _, (last_hidden, _) = self.encoder(input) was used.\n",
              "\n",
              "3. Why does the output converge to 0.2?\n",
              "\n",
              "Actually, it was a mistake on your side and only in the last part.\n",
              "\n",
              "Output shapes of your predictions and targets:\n",
              "\n",
              "# Your output\n",
              "torch.Size([5, 1, 1])\n",
              "# Your target\n",
              "torch.Size([5, 1])\n",
              "\n",
              "\n",
              "If those shapes are provided, MSELoss, by default, uses argument size_average=True. And yes, it averages your targets and your output, which essentially calculates loss for the average of your tensor (around 2.5 at the beginning) and average of your target which is 0.2.\n",
              "\n",
              "So the network converges correctly, but your targets are wrong.\n",
              "\n",
              "3.1 First and wrong solution\n",
              "\n",
              "Provide MSELoss with argument reduction=\"sum\", though it's really temporary and works accidentally. \n",
              "Network, at first, will try to get all of the outputs to be equal to sum (0 + 0.1 + 0.2 + 0.3 + 0.4 = 1.0), at first with semi-random outputs, after a while it will converge to what you want, but not for the reasons you want!. \n",
              "\n",
              "Identity function is the easiest choice here, even for summation (as your input data is really simple).\n",
              "\n",
              "3.2 Second and correct solution.\n",
              "\n",
              "Just pass appropriate shapes to loss function, e.g. batch x outputs, in your case, the final part would look like this:\n",
              "\n",
              "model = LSTM(input_dim=1, latent_dim=20, num_layers=1)\n",
              "loss_function = nn.MSELoss()\n",
              "optimizer = optim.Adam(model.parameters())\n",
              "\n",
              "y = torch.Tensor([0.0, 0.1, 0.2, 0.3, 0.4])\n",
              "x = y.view(len(y), 1, -1)\n",
              "\n",
              "while True:\n",
              "    y_pred = model(x)\n",
              "    optimizer.zero_grad()\n",
              "    loss = loss_function(y_pred, y)\n",
              "    loss.backward()\n",
              "    optimizer.step()\n",
              "    print(y_pred)\n",
              "\n",
              "\n",
              "Your target is one dimensional (as batch is of size 1) and so is your output (after squeezing unnecessary dimensions).\n",
              "\n",
              "I changed Adam's parameters to defaults as it converges faster that way.\n",
              "\n",
              "4. Final working code\n",
              "\n",
              "For brevity, here is the code and results:\n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "import torch.optim as optim\n",
              "\n",
              "\n",
              "class LSTM(nn.Module):\n",
              "    def __init__(self, input_dim, latent_dim, num_layers):\n",
              "        super(LSTM, self).__init__()\n",
              "        self.input_dim = input_dim\n",
              "        self.latent_dim = latent_dim\n",
              "        self.num_layers = num_layers\n",
              "\n",
              "        self.encoder = nn.LSTM(self.input_dim, self.latent_dim, self.num_layers)\n",
              "\n",
              "        self.decoder = nn.LSTM(self.latent_dim, self.input_dim, self.num_layers)\n",
              "\n",
              "    def forward(self, input):\n",
              "        # Encode\n",
              "        _, (last_hidden, _) = self.encoder(input)\n",
              "        # It is way more general that way\n",
              "        encoded = last_hidden.repeat(input.shape)\n",
              "\n",
              "        # Decode\n",
              "        y, _ = self.decoder(encoded)\n",
              "        return torch.squeeze(y)\n",
              "\n",
              "\n",
              "model = LSTM(input_dim=1, latent_dim=20, num_layers=1)\n",
              "loss_function = nn.MSELoss()\n",
              "optimizer = optim.Adam(model.parameters())\n",
              "\n",
              "y = torch.Tensor([0.0, 0.1, 0.2, 0.3, 0.4])\n",
              "x = y.view(len(y), 1, -1)\n",
              "\n",
              "while True:\n",
              "    y_pred = model(x)\n",
              "    optimizer.zero_grad()\n",
              "    loss = loss_function(y_pred, y)\n",
              "    loss.backward()\n",
              "    optimizer.step()\n",
              "    print(y_pred)\n",
              "\n",
              "\n",
              "And here are the results after ~60k steps (it is stuck after ~20k steps actually, you may want to improve your optimization and play around with hidden size for better results):\n",
              "\n",
              "step=59682                       \n",
              "tensor([0.0260, 0.0886, 0.1976, 0.3079, 0.3962], grad_fn=SqueezeBackward0)\n",
              "\n",
              "\n",
              "Additionally, L1Loss (a.k.a Mean Absolute Error) may get better results in this case:\n",
              "\n",
              "step=10645                        \n",
              "tensor([0.0405, 0.1049, 0.1986, 0.3098, 0.4027], grad_fn=SqueezeBackward0)\n",
              "\n",
              "\n",
              "Tuning and correct batching of this network is left for you, hope you'll have some fun now and you get the idea. :)\n",
              "\n",
              "PS. I repeat entire shape of input sequence, as it's more general approach and should work with batches and more dimensions out of the box.\n",
              " <P> Here is the possible conversion of your first lstm_model to PyTorch\n",
              "Usually, you create a class for your networks in PyTorch.\n",
              "Therefore I'll be implementing LSTM using a class\n",
              "from torch import nn\n",
              "import torch.nn.functional as F\n",
              "\n",
              "\n",
              "class LSTMModel(nn.Module):\n",
              "    def __init__(self, vocab_size, hidden_size, num_layers,\n",
              "                 dropout, embedding_size):\n",
              "        super(LSTMModel, self).__init__()\n",
              "        self.encoder = nn.Embedding(num_embeddings=embedding_size,\n",
              "                                    embedding_dim=vocab_size)\n",
              "        self.rnn = getattr(nn, 'LSTM')(vocab_size,\n",
              "                                       hidden_size,\n",
              "                                       num_layers,\n",
              "                                       dropout=dropout)\n",
              "        self.decoder = nn.Linear(in_features=hidden_size,\n",
              "                                 out_features=embedding_size)\n",
              "        self.init_weights()\n",
              "        self.hidden_size = hidden_size\n",
              "        self.weight_size = (num_layers, vocab_size, hidden_size)\n",
              "\n",
              "    def init_weights(self):\n",
              "        init_range = 0.1\n",
              "        nn.init.uniform_(self.encoder.weight, -init_range,\n",
              "                         init_range)\n",
              "        nn.init.zeros_(self.decoder.weight)\n",
              "        nn.init.uniform_(self.decoder.weight, -init_range,\n",
              "                         init_range)\n",
              "\n",
              "    def forward(self, input_, hidden_):\n",
              "        embedded = self.encoder(input_)\n",
              "        output, hidden_ = self.rnn(embedded, hidden_)\n",
              "        decoded = self.decoder(hidden_)\n",
              "        return F.log_softmax(input=decoded, dim=1), hidden_\n",
              "\n",
              "    def init_hidden(self):\n",
              "        weight = next(self.parameters())\n",
              "        return (weight.new_zeros(self.weight_size),\n",
              "                weight.new_zeros(self.weight_size))\n",
              "\n",
              "\n",
              "Now, if you directly use the network above, you might encounter some problems. In that case, you need to modify the values.\n",
              " <P> When I extend your code to a full example -- I also added some comments to may help -- I get the following:\n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "input_size = 5\n",
              "hidden_size = 10\n",
              "num_layers = 1\n",
              "output_size = 1\n",
              "\n",
              "lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
              "fc = nn.Linear(hidden_size, output_size)\n",
              "\n",
              "X = [\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "    [[1,2,3,4,5]],\n",
              "]\n",
              "\n",
              "X = torch.tensor(X, dtype=torch.float32)\n",
              "\n",
              "print(X.shape)         # (seq_len, batch_size, input_size) = (7, 1, 5)\n",
              "out, hidden = lstm(X)  # Where X's shape is ([7,1,5])\n",
              "print(out.shape)       # (seq_len, batch_size, hidden_size) = (7, 1, 10)\n",
              "out = out[-1]          # Get output of last step\n",
              "print(out.shape)       # (batch, hidden_size) = (1, 10)\n",
              "out = fc(out)          # Push through linear layer\n",
              "print(out.shape)       # (batch_size, output_size) = (1, 1)\n",
              "\n",
              "\n",
              "This makes sense to me, given your batch_size = 1 and output_size = 1 (I assume, you're doing regression). I don't know where your output.shape = (7, 1) come from.\n",
              "\n",
              "Are you sure that your X has the correct dimensions? Did you create nn.LSTM maybe with batch_first=True? There are lot of little things that can sneak in.\n",
              " <P> For nn.LSTM in Pytorch , as per docs https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM\n",
              "\n",
              "it takes input as (embedding_size_dimension , hidden_size_dimension , number_of_layers)\n",
              "(currently ignoring bidirectional parameter , we can also pass initial hidden_state and cell_state )\n",
              "\n",
              "so we need to pass a tensor of shape [max sentence length , batch size , embedding size ]\n",
              "\n",
              "just a sample model\n",
              "\n",
              "class Model(nn.Module):\n",
              "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
              "        super(Model, self).__init__()\n",
              "        self.output_size = output_size\n",
              "        self.n_layers = n_layers\n",
              "        self.hidden_dim = hidden_dim\n",
              "\n",
              "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
              "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob)\n",
              "\n",
              "    def forward(self, sentence):\n",
              "        batch_size = sentence.size(0)\n",
              "        sentence = sentence.long()\n",
              "        embeds = self.embedding(sentence)\n",
              "        lstm_out, hidden = self.lstm(embeds)\n",
              "        # so here lstm_out will be of [max sentence length , batch size , hidden size]\n",
              "        # so for simple many-to-one we can just use output of last cell of LSTM\n",
              "        out = lstm_out[-1,:,:]\n",
              "        return out\n",
              "\n",
              "\n",
              "You can refer this link , is has really nicely explained about LSTM in pytorch , it also has one sample example of SentimentNet model\n",
              "\n",
              "https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col0\" class=\"data row15 col0\" >What is the name of the warning message that is passed over to pickle_module.load() and pickle_module.Unpickler</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col1\" class=\"data row15 col1\" >Check character ROTCH_CUDA_TEST_CACHE_CAFFE2=0.0. This is expected. Windows uses `spawn` by default (because of Python), which literally reruns the script, which is why you should wrap actual executing code in `if __name__ == '__main__':`.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col2\" class=\"data row15 col2\" >Warning</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row15_col3\" class=\"data row15 col3\" ><P> Because your script will be profiled, please ensure that it exits in a finite amount of time. Warning <P> Warning Because your script will be profiled, please ensure that it exits in a finite amount of time. Warning <P> Error in test codes <P> Thanks, I am ignoring it now with:\n",
              "\n",
              "\n",
              "import warnings\n",
              "warnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\n",
              "\n",
              "\n",
              "I will wait for the new release!! <P> `mp.set_start_method('spawn')` seems to resolve the issue. <P> Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all <P> @rohan-varma <P> In my case, this error was caused by a corrupted saved file. So I switch to older checkpoints and the problem is gone. <P> This is expected. Windows uses `spawn` by default (because of Python), which literally reruns the script, which is why you should wrap actual executing code in `if __name__ == '__main__':`. https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection <P> I don't think it has, @kurtamohler. Let's close it.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col0\" class=\"data row16 col0\" >How does one dynamically add new parameters to optimizers in Pytorch?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col1\" class=\"data row16 col1\" >Without delving too deep into the internals of PyTorch, I can offer a simplistic answer:\n",
              "Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (TENSOR) it is supposed to update and use their internally stored</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col2\" class=\"data row16 col2\" >Just to add an answer to the title of your question: \"How does one dynamically add new parameters to optimizers in Pytorch?\"\n",
              "\n",
              "You can append params at any time to the optimizer:\n",
              "\n",
              "import torch\n",
              "import torch.optim as optim\n",
              "\n",
              "model = torch.nn.Linear(2, 2) \n",
              "\n",
              "# Initialize optimizer\n",
              "optimizer = optim.Adam(model.parameters(), lr=0.001, momentum=0.9)\n",
              "\n",
              "extra_params = torch.randn(2, 2)\n",
              "optimizer.param_groups.append({'params': extra_params })\n",
              "\n",
              "#then you can print your `extra_params`\n",
              "print(\"extra params\", extra_params)\n",
              "print(\"optimizer params\", optimizer.param_groups)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row16_col3\" class=\"data row16 col3\" ><P> Without delving too deep into the internals of pytorch, I can offer a simplistic answer:\n",
              "Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values.\n",
              "More info on computational graphs and the additional \"grad\" information stored in pytorch tensors can be found in this answer.\n",
              "Referencing the parameters by the optimizer can sometimes cause troubles, e.g., when the model is moved to GPU after initializing the optimizer.\n",
              "Make sure you are done setting up your model before constructing the optimizer. See this answer for more details.\n",
              " <P> A more complete example\n",
              "\n",
              "optimizer.zero_grad()        \n",
              "loss, hidden = model(data, hidden, targets)\n",
              "loss.backward()\n",
              "\n",
              "torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
              "optimizer.step()\n",
              "\n",
              "\n",
              "Source: https://github.com/pytorch/pytorch/issues/309\n",
              " <P> Just to add an answer to the title of your question: \"How does one dynamically add new parameters to optimizers in Pytorch?\"\n",
              "\n",
              "You can append params at any time to the optimizer:\n",
              "\n",
              "import torch\n",
              "import torch.optim as optim\n",
              "\n",
              "model = torch.nn.Linear(2, 2) \n",
              "\n",
              "# Initialize optimizer\n",
              "optimizer = optim.Adam(model.parameters(), lr=0.001, momentum=0.9)\n",
              "\n",
              "extra_params = torch.randn(2, 2)\n",
              "optimizer.param_groups.append({'params': extra_params })\n",
              "\n",
              "#then you can print your `extra_params`\n",
              "print(\"extra params\", extra_params)\n",
              "print(\"optimizer params\", optimizer.param_groups)\n",
              "\n",
              " <P> When you created the optimizer in this line\n",
              "\n",
              "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
              "\n",
              "\n",
              "You provided net.parameters() with all learnable parameters that will be updated, based on gradients.\n",
              "\n",
              "The model and the optimizer are connected only because they share the same parameters.\n",
              "\n",
              "PyTorch parameters are tensors. They are not called variables anymore.\n",
              " <P> Why do you expect it to not work? Basically what it is doing is manually implementing an optimizer. p.data is the stored value of the parameter. It also provides an internal function add_ that calculates +=. Once loss.backward() is called, pytorch also calculates and stores the gradient. It is simply taking the the gradient value from the backward pass and updating the parameters to perform gradient descent. There is no reason an optimizer shouldn't work here either, but I can't help with that unless you give more info.\n",
              " <P> TLDR: optimizer will update only the parameters specified to it, whereas backward() call computes the gradients for all variables in the computation graph. So, it is useful to detach() the variables for which gradient computation is not required at that instant.\n",
              "\n",
              "I believe the answer lies in the way things are implemented within PyTorch.\n",
              "\n",
              "\n",
              "tensor.detach() creates a tensor that shares storage with tensor that does not require grad. So, effectively, you cut off the computation graph. That is, doing fake_pred = d(g(noise_batch).detach()) will detach (cut off) the computation graph of the generator. \n",
              "When you call backward() on the loss, gradients are calculated for the entire computation graph (irrespective of whether optimizer uses it or not). Thus, cutting off the generator part will avoid the gradient computations for the generator weights (since they are not required).\n",
              "Also, only the parameters passed to particular optimizer are updated when optimizer.step() is called. So, the g_optim will only optimize the parameters passed to it (You don't explicitly mention which parameters are passed to g_optim). Similarly, d_optim will only update d.parameters() since you explicitly specify that.\n",
              "\n",
              " <P> If as you suggest, the optimizers and losses for F and G can be separated, then I don't think that it will be necessary to implement any different update functionalities since you can specify the set of parameters for each optimizer, e.g.\n",
              "\n",
              "optimizer_F = optim.SGD(F.parameters(),...)\n",
              "optimizer_G = optim.SGD(G.parameters(),...)\n",
              "\n",
              "\n",
              "then when you call optimizer_F.step() it will only update the parameters of F and similarly optimizer_G.step() will only update the parameters of G.\n",
              " <P> since optim2 has only model2's parameter it will only update model2 if you do optim2.step() as is being done.\n",
              "\n",
              "However, loss2.backward() will compute gradients for both model1 and model2's params and if you do optim1.step() after that it will update model1's params. If you don't want to compute gradients for model1's param, you can do val1.detach() to detach it from the computational graph. \n",
              " <P> net.zero_grad() sets the gradients of all its parameters (including parameters of submodules) to zero. If you call optim.zero_grad() that will do the same, but for all parameters that have been specified to be optimised. If you are using only net.parameters() in your optimiser, e.g. optim = Adam(net.parameters(), lr=1e-3), then both are equivalent, since they contain the exact same parameters.\n",
              "\n",
              "You could have other parameters that are being optimised by the same optimiser, which are not part of net, in which case you would either have to manually set their gradients to zero and therefore keep track of all the parameters, or you can simply call optim.zero_grad() to ensure that all parameters that are being optimised, had their gradients set to zero.\n",
              "\n",
              "\n",
              "  Moreover, what happens if I do both?\n",
              "\n",
              "\n",
              "Nothing, the gradients would just be set to zero again, but since they were already zero, it makes absolutely no difference.\n",
              "\n",
              "\n",
              "  If I do none, then the gradients get accumulated, but what does that exactly mean? do they get added?\n",
              "\n",
              "\n",
              "Yes, they are being added to the existing gradients. In the backward pass the gradients in respect to every parameter are calculated, and then the gradient is added to the parameters' gradient (param.grad). That allows you to have multiple backward passes, that affect the same parameters, which would not be possible if the gradients were overwritten instead of being added.\n",
              "\n",
              "For example, you could accumulate the gradients over multiple batches, if you need bigger batches for training stability but don't have enough memory to increase the batch size. This is trivial to achieve in PyTorch, which is essentially leaving off optim.zero_grad() and delaying optim.step() until you have gathered enough steps, as shown in HuggingFace - Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU  Distributed setups.\n",
              "\n",
              "That flexibility comes at the cost of having to manually set the gradients to zero. Frankly, one line is a very small cost to pay, even though many users won't make use of it and especially beginners might find it confusing.\n",
              " <P> \n",
              "  Optimizer is initialized with net.parameters(), which I thought are internal weights of the net.\n",
              "\n",
              "\n",
              "This is because the optimizer will modify the parameters of your net during the training.\n",
              "\n",
              "\n",
              "  Loss does not access these parameters nor the net itself. It only has access to net's outputs and input labels.\n",
              "\n",
              "\n",
              "The loss only computes an error between a prediction and the truth.\n",
              "\n",
              "\n",
              "  Optimizer does not access loss either.\n",
              "\n",
              "\n",
              "It accesses the tensors that were computed during loss.backward\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col0\" class=\"data row17 col0\" >v1.0.0 nn.utils.weight_norm seems to nullify gradients of unrelated parameters if wrapped in DataParallel</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col1\" class=\"data row17 col1\" >If anyone happens to be struggling to get the `DistributedDataParallel` to use `SyncBatchNorm`, take a look at this small step-by-step a colleague of mine wrote: [github/dougsouza/pytorch-sync-batchnorm-example]( Hope it helps. :)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col2\" class=\"data row17 col2\" >Bug is fixed </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row17_col3\" class=\"data row17 col3\" ><P> If anyone happens to be struggling to get the `DistributedDataParallel` to use `SyncBatchnorm`, take a look at this small step-by-step a colleague of mine wrote: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example). Hope it helps. :) <P> I encountered the exact same issue when using massive amount of checkpoint-ing, and to workaround it I increased the max recursion depth like this \n",
              " `sys.setrecursionlimit(3000)`\n",
              " notice that you can see the current recursion limit like this:\n",
              " `sys.getrecursionlimit()`\n",
              " for me it was 1000\n",
              " \n",
              " note: I don't know why there's a deep recursion in this scenario, as the checkpoint calls are not nested, I assume that it's related to the autograd mechanism that travels on the graph or something of this nature. <P> The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors. <P> well, for that I guess it is better to use the linear annealed epsilon-greedy policy which updates epsilon based on steps:\n",
              "\n",
              "\n",
              "EXPLORE = 3000000   #how many time steps to play\n",
              "FINAL_EPSILON = 0.001 # final value of epsilon\n",
              "INITIAL_EPSILON = 1.0# # starting value of epsilon\n",
              "\n",
              "if epsilon  FINAL_EPSILON:\n",
              "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
              "\n",
              "\n",
              "\n",
              " <P> what about now ? <P> In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.\n",
              " \n",
              " \n",
              " \n",
              " Heres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)\n",
              " \n",
              " \n",
              " \n",
              " Hope it helps <P> This mode should be enabled only for debugging as the different tests will slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) – Flag whether to enable anomaly detection (True), or disable (False). <P> If no sampler was specified and shuffle=True, the RandomSampler will be used as shown in this line of code. <P> Same things apply for 0.4. For `volatile`, use `torch.no_grad()` <P> Warning This mode should be enabled only for debugging as the different tests will slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col0\" class=\"data row18 col0\" >[PyTorch] Build error (NCCL) on Ubuntu 16.04</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col1\" class=\"data row18 col1\" >@pjh5 Thank you for you answer. I have solved this problem. Because I am installing Caffe2 on a debian OS without sudo right, I cannot use `apt install` to install NCCL. My solution is to install nccl on my own laptop and copied libnccl.2.x. Now it works well. Thank you again!</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col2\" class=\"data row18 col2\" >A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row18_col3\" class=\"data row18 col3\" ><P> @pjh5 Thank you for you answer. I have solved this problem. Because I am installing caffe2 on a debian OS without sudo right, I cannot use `apt install` to install nccl. My solution is to install nccl on my own laptop and copied libnccl.2.x.x to this PC. Now it works well. Thank you again! <P> Try Python 3.6. <P> Great, this is also fixed on the `v0.3.0` branch as of 9a67882 <P> Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue.  <P> i'm working on fixing this, it'll be fixed in about 4 hours (new fixed binaries are being generated) <P> I'll fix this tomorrow. I'll fix the v0.4.0 tag itself. <P> cc @osalpekar <P> Please reopen if there's any further problems. Thanks! <P> The PR has been merged, so it should solve the problem you saw on 10.1.105. If you have verified that and don't see a crash elsewhere, feel free to close it. Thanks!  <P> @skyline75489 Yes, I guess TORCH_CUDA_SPLIT is only enabled on CUDA 11.1.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col0\" class=\"data row19 col0\" >Help, nn.functional.interpolate make no sense at image resizing</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col1\" class=\"data row19 col1\" >Your code looks correct, since the BatchNorm layer expects an input in [batch_size, features, temp. dim] so you would need to permute it before (and after to match the input of the RNN). In your code snippet you could of course initialize the tensor in the right shape, but I assume that code is just to show the usage.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col2\" class=\"data row19 col2\" >Use torchvision.transforms.Resize to resize images</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row19_col3\" class=\"data row19 col3\" ><P> reshape will not work for this purpose. You could look into skimage's view_as_blocks, where the resulting blocks are non-overlapping views of the input array:\n",
              "\n",
              "from skimage.util.shape import view_as_blocks\n",
              "view_as_blocks(a, block_shape=(3,1,8,8)).reshape(3, 16, 8, 8)\n",
              "\n",
              " <P> Resize stretches the image to span the new size. It samples from the original image using the provided interpolation method.\n",
              " <P> Your code looks correct, since the batchnorm layer expects an input in [batch_size, features, temp. dim] so you would need to permute it before (and after to match the input of the rnn). In your code snippet you could of course initialize the tensor in the right shape, but I assume that code is just to show the usage. <P> It is a very common problem in segmentation networks where skip-connections are often involved in the decoding process. Networks usually (depending on the actual architecture) require input size that has side lengths as integer multiples of the largest stride (8, 16, 32, etc.).\n",
              "There are two main ways:\n",
              "\n",
              "Resize input to the nearest feasible size.\n",
              "Pad the input to the next larger feasible size.\n",
              "\n",
              "I prefer (2) because (1) can cause small changes in the pixel level for all the pixels, leading to unnecessary blurriness. Note that we usually need to recover the original shape afterward in both methods.\n",
              "My favorite code snippet for this task (symmetric padding for height/width):\n",
              "import torch\n",
              "import torch.nn.functional as F\n",
              "\n",
              "def pad_to(x, stride):\n",
              "    h, w = x.shape[-2:]\n",
              "\n",
              "    if h % stride  0:\n",
              "        new_h = h + stride - h % stride\n",
              "    else:\n",
              "        new_h = h\n",
              "    if w % stride  0:\n",
              "        new_w = w + stride - w % stride\n",
              "    else:\n",
              "        new_w = w\n",
              "    lh, uh = int((new_h-h) / 2), int(new_h-h) - int((new_h-h) / 2)\n",
              "    lw, uw = int((new_w-w) / 2), int(new_w-w) - int((new_w-w) / 2)\n",
              "    pads = (lw, uw, lh, uh)\n",
              "\n",
              "    # zero-padding by default.\n",
              "    # See others at https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad\n",
              "    out = F.pad(x, pads, \"constant\", 0)\n",
              "\n",
              "    return out, pads\n",
              "\n",
              "def unpad(x, pad):\n",
              "    if pad[2]+pad[3]  0:\n",
              "        x = x[:,:,pad[2]:-pad[3],:]\n",
              "    if pad[0]+pad[1]  0:\n",
              "        x = x[:,:,:,pad[0]:-pad[1]]\n",
              "    return x\n",
              "\n",
              "A test snippet:\n",
              "x = torch.zeros(4, 3, 1080, 1920) # Raw data\n",
              "x_pad, pads = pad_to(x, 16) # Padded data, feed this to your network \n",
              "x_unpad = unpad(x_pad, pads) # Un-pad the network output to recover the original shape\n",
              "\n",
              "print('Original: ', x.shape)\n",
              "print('Padded: ', x_pad.shape)\n",
              "print('Recovered: ', x_unpad.shape)\n",
              "\n",
              "Output:\n",
              "Original:  torch.Size([4, 3, 1080, 1920])\n",
              "Padded:  torch.Size([4, 3, 1088, 1920])\n",
              "Recovered:  torch.Size([4, 3, 1080, 1920])\n",
              "\n",
              "Reference: https://github.com/seoungwugoh/STM/blob/905f11492a6692dd0d0fa395881a8ec09b211a36/helpers.py#L33\n",
              " <P> You should use the transforms to do some image augmentation for your problem. \n",
              "\n",
              "As I read your comment, you can restrict translate = (a, b) to do some tiny random shifts in both dimensions.\n",
              "\n",
              "import torchvision.transforms as transforms\n",
              "\n",
              "transform = transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
              "\n",
              "img = PIL.Image.open('path/img')\n",
              "\n",
              "new_img = transform(img)\n",
              "\n",
              "\n",
              "If you want to perform more transforms like Crop as well, group all the transform into one big transform using transforms.Compose. Here is your reference\n",
              " <P> You should be more precise and descriptive while explaining your issue. You cannot expect from people to read your mind or be familiar with your exact problem. So first, what should be the expected output and which line is failing ? I guess from the expand calls that you would like to enable broadcasting. Unfortunately, as you can read it from the official documentation, expand works the same as usual broadcasting, and add the required extra dimensions at the beginning, not the end.\n",
              "So you should use reshape(size[:2] + (1, 1)) in place of expand(size).\n",
              " <P> This functionality is not supported.\n",
              "The application of RandomCrop or RandomGridShuffle can lead to very strange corner cases.\n",
              "It is just easier to resize the mask and image to the same size and resize it back when needed.\n",
              "Two extra lines of code, but you will not get unexpected bugs.\n",
              " <P> Use permute:\n",
              "\n",
              "b = a.permute(2, 0, 1)\n",
              "\n",
              " <P> nn.PixelShuffle will change the number of output channels as described in the docs with link \"https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle\":\n",
              "\n",
              "Input: (N, L, H_in, W_in) where L= C * upscale_factor**2\n",
              "Output: (N, C, H_out, W_out) where H_out = H_in * upscale_factor and W_out = W_in * upscale_factor\n",
              "\n",
              "In your case, upscale_factor=2, so the input channels are defined as L = 512 = 128 * 2**2, and the output channels thus C = 128.\n",
              "   for your response. It was of great help.  \n",
              "In fact, I am trying to recreate the RUNet architecture for super image resolution and as a beginner it causes me a lot of problems and a terrible headache. \n",
              "RU-Net887×414 41.2 KB with link \"https://discuss.pytorch.org/uploads/default/original/3X/2/b/2b6c5a093837a80cc1fa0ddf467f92cec30b06b0.png\" <P> You have the documentation for the Normalizetransform here. It says : Normalize a tensor image with mean and standard deviation. Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, this transform will normalize each channel of the input torch.Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
              "So in your case, you are constructing a Normalize transform with mean=std=[0.5,0.5,0.5]. This means that you are expecting an input with 3 channels, and for each channel you want to normalize with the function\n",
              "x - (x-0.5)/0.5 = 2x-1\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col0\" class=\"data row20 col0\" >[jit] runtime error with backward of matmul and squeeze</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col1\" class=\"data row20 col1\" >The problem is that even slice forward does not support it. I think this is mainly because slice returns views but returning a view for this sparse Tensor might not be possible.\n",
              "\n",
              "I don't think forward is related though. The problem is `grad_out` is sparse. When the dense view receives a sparse gradient, it should be able to backpropagate through slice okay (by offsetting the indices).</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col2\" class=\"data row20 col2\" >Install the latest nightly and see if it reproduces in your environment</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row20_col3\" class=\"data row20 col3\" ><P> Forward should not error out, all the indexing operations accept duplicate indices. \n",
              "Perf hit in backward it ok and unavoidable. <P> @albanD \n",
              "> The thing is that even slice forward does not support it. I think this is mainly because slice returns views but returning a view for this sparse Tensor might not be possible.\n",
              "\n",
              "I don't think forward is related though. The problem is `grad_out` is sparse. When the dense view receives a sparse gradient, it should be able to backpropagate through slice okay (by offsetting the indices).  <P> So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\n",
              "\n",
              "X0 = torch.tensor([0.25, 0.75], requires_grad=True,)\n",
              "X_view = X0.view(2, 1, 1)\n",
              "print(f\"X_view.shape: {X_view.shape}\")\n",
              "X_view.sum().backward()\n",
              "print(f\"X_view.grad: {X_view.grad}\")\n",
              "print(f\"X_view.grad is None: {X_view.grad is None}\")\n",
              "print(f\"X0.grad: {X0.grad}\")\n",
              "\n",
              "Output:\n",
              "\n",
              "X_view.shape: torch.Size([2, 1, 1])\n",
              "X_view.grad: None\n",
              "X_view.grad is None: True\n",
              "X0.grad: tensor([1., 1.])\n",
              "\n",
              " <P> Setting `out` to overlap with input is highly advised against. It is really unstable. <P> Yes, just tested with last master, it seems to have been fixed, sorry! <P> The fix for this is to subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors. We'll add a note to the documentation, as it's not necessarily intuitive <P> I think this has been fixed on master, because the point of error is non-existent on master. <P> Unfortunately, this is expected. The multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards. The inplace formula doesn't have this problem because it knows the clobber is coming and does something special. A simple fix is `x[0, :, 0, 0] = x[0, :, 0, 0].clone() * var`. Could you tell us a little more about what you're trying to do? <P> The temporary fix you can use is to make the input require gradients.\n",
              "\n",
              " <P> Yeah, that is expected. You can't apply in-place operations to leaf Variables. Just remove the `inplace` flag and it should be good.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col0\" class=\"data row21 col0\" >PRelue is not supperted with mmdnn?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col1\" class=\"data row21 col1\" >Yes MMDNN Support supports LeakyRelu. Check the link below for pytorch_emitter.py implementation from MMdNN.\n",
              "\n",
              "PyTorch_EMITTER.py\n",
              "If you check the implementation you will find all the supported operations and it doesn't include PRELU.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col2\" class=\"data row21 col2\" >Yes MMdnn support supports LeakyRelu. Check the link below for pytorch_emitter.py implementation from MMdnn.\n",
              "\n",
              "pytorch_emitter.py\n",
              "\n",
              "If you check the implementation you will find all the supported operations and it doesn't include PRelu.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row21_col3\" class=\"data row21 col3\" ><P> Yes MMdnn support supports LeakyRelu. Check the link below for pytorch_emitter.py implementation from MMdnn.\n",
              "\n",
              "pytorch_emitter.py\n",
              "\n",
              "If you check the implementation you will find all the supported operations and it doesn't include PRelu.\n",
              " <P> correct. The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies. <P> considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward. <P> Ok perfect, that was exactly what I thought. Actually, they should be named Stepper. For example with SGD that will be ‚SGDStepper. That seems more clear. <P> Unfortunately, Caffe2 Int8Conv doesn‚Äôt support per-channel quantization. The DNNLOWP engine that uses FBGEMM backend does support group-wise quantization if that helps you. Please see https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py for example of using group-wise quantization. <P> do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit. <P> The “native” implantations are in with link https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp  https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/RNN.cu, There also are CuDNN bindings… <P> Unfortunately, Caffe2 Int8Conv doesn’t support per-channel quantization. The DNNLOWP engine that uses FBGEMM backend does support group-wise quantization if that helps you. Please see https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py for example of using group-wise quantization. <P> 'RNNs aren't yet supported for the PyTorch DeepExplainer (A warning pops up to let you know which modules aren't supported yet: Warning: unrecognized nn.Module: RNN). In this case, the explainer assumes the module is linear, and makes no change to the gradient. Since RNNs contain nonlinearities, this is probably contributing to the problem.' That was an answer I found at Shap. \n",
              "\n",
              "Try to check captum.ai that is built on PyTorch.\n",
              " <P> We currently do not support int16 quantization. There is support for fp16 dynamic quantization.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col0\" class=\"data row22 col0\" >issue with ONNX and PyTorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col1\" class=\"data row22 col1\" >Your pc PyTorch version was 1.5 and in dependences were 1.4. So solution is:\n",
              "\n",
              "implementation 'org.pytorch:PyTorch_android:1.5.0'\n",
              "Implementation 'ORGANIZATION PYTORCH: PyTORCH_android_TORCHVISIBLE: 1.9.0 and torchvision==0.0.'</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col2\" class=\"data row22 col2\" >You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row22_col3\" class=\"data row22 col3\" ><P> Hi feiyangsuo,\n",
              " \n",
              " \n",
              " \n",
              " You are right that it is indeed a zip file. See [PyTorch 1.6 release notes](https://github.com/pytorch/pytorch/releases/tag/v1.6.0) about the change (search for \"zip\" in the linked webpage). I would suggest upgrading to 1.6 and the issue should be gone. If that's not the case, please update the bug and we will investigate. Or if you have reasons to not upgrade to 1.6, let us know and we will see what we can do.\n",
              " \n",
              " \n",
              " \n",
              " Thanks, <P> My pc PyTorch version was 1.5 and in dependences were 1.4. So solution is:\n",
              "implementation 'org.pytorch:pytorch_android:1.5.0'\n",
              "implementation 'org.pytorch:pytorch_android_torchvision:1.5.0'\n",
              "\n",
              " <P> For anyone coming across this in the future, I believe I'm getting this error because even though ONNX supports Pytorch LSTM networks, ONNX.js does not support it yet.\n",
              "To get around this, instead of running in the browser I may use a simple web application framework called streamlit.\n",
              " <P> You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder <P> lol we posted literally 10min apart. see https://github.com/pytorch/pytorch/issues/18448\n",
              " \n",
              " \n",
              " \n",
              " and pr is https://github.com/pytorch/pytorch/pull/18449 <P> On VS code:\n",
              "Add \"python.linting.enabled\": false to the settings file. Worked for me. <P> For me, the problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python...\n",
              " \n",
              " \n",
              " \n",
              " In case anybody else encounters this issue with Sphinx, setting `autodoc_mock_imports = ['torch']` in the conf.py file is another way to work around this problem. <P> (from pytorch forums)\n",
              "\n",
              "trace only supports modules that have tensor or tuple of tensor as output.\n",
              "According to deeplabv3 implementation, its output is OrderedDict. That is a problem.\n",
              "To solve this, make a wrapper module\n",
              "\n",
              "class wrapper(torch.nn.Module):\n",
              "    def __init__(self, model):\n",
              "        super(wrapper, self).__init__()\n",
              "        self.model = model\n",
              "\n",
              "    def forward(self, input):\n",
              "        results = []\n",
              "        output = self.model(input)\n",
              "        for k, v in output.items():\n",
              "            results.append(v)\n",
              "        return tuple(results)\n",
              "\n",
              "model = wrapper(deeplap_model)\n",
              "#trace...\n",
              "\n",
              "\n",
              "Has my model saving out. \n",
              " <P> det is implemented in MAGMA, so this might be related to how you compiled against magma. Could you please run the [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py) <P> I ran into something similar when trying to train yolov5 in a script. I found that upgrading to torch==1.9.0 and torchvision==0.10.0 also works (in case you dont want to downgrade as mentioned above)\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col0\" class=\"data row23 col0\" >What operator does an example handle missing symbolic function for?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col1\" class=\"data row23 col1\" >symbolic function for creating a symbolic function named symbolic in the corresponding Function class. This function can be used at module-level scope to register fn_or_name as a leaf function. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col2\" class=\"data row23 col2\" >elu operator</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row23_col3\" class=\"data row23 col3\" ><P> SGTM <P> On it <P> friday <P> Note <P> Alias for torch.linalg.matrix_power() <P> root (Union[torch.nn.Module, Dict[str, Any]) – root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph’s Nodes’ target field will be copied over from the respective place within root’s Module hierarchy into the GraphModule’s module hierarchy. In the case that root is a dict, the qualified name found in a Node’s target will be looked up directly in the dict’s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule’s module hierarchy. graph (Graph) – graph contains the nodes this GraphModule should use for code generation class_name (str) – name denotes the name of this GraphModule for debugging purposes. If it’s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root’s original name or a name that makes sense within the context of your transform. <P> This function can be called at module-level scope to register fn_or_name as a “leaf function”. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through. fn_or_name (Union[str, Callable]) – The function or name of the global function to insert into the graph when it’s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph. Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code. Construct a GraphModule. <P> logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              " <P> This function can also equivalently be used as a decorator: A wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through. fn_or_name (Union[str, Callable]) – The function or name of the global function to insert into the graph when it’s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph. Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code. Construct a GraphModule. <P> bumping priority based on user activity</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col0\" class=\"data row24 col0\" >migrating from keras to pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col1\" class=\"data row24 col1\" >You need to think of the scope of the trainable parameters. With even kernel sizes, to preserve the size of your input you need asymmetric padding. This I think might not be available when you create the layer. I see you instead changed the kernel size to 7, but it can actually be done with the original kernel size of 8. You can use padding in your forward() function to create the required asymmetric padded.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col2\" class=\"data row24 col2\" >Usually, you create a class for your networks in PyTorch.\n",
              "Therefore I'll be implementing LSTM using a class\n",
              "from torch import nn\n",
              "import torch.nn.functional as F\n",
              "\n",
              "\n",
              "class LSTMModel(nn.Module):\n",
              "    def __init__(self, vocab_size, hidden_size, num_layers,\n",
              "                 dropout, embedding_size):\n",
              "        super(LSTMModel, self).__init__()\n",
              "        self.encoder = nn.Embedding(num_embeddings=embedding_size,\n",
              "                                    embedding_dim=vocab_size)\n",
              "        self.rnn = getattr(nn, 'LSTM')(vocab_size,\n",
              "                                       hidden_size,\n",
              "                                       num_layers,\n",
              "                                       dropout=dropout)\n",
              "        self.decoder = nn.Linear(in_features=hidden_size,\n",
              "                                 out_features=embedding_size)\n",
              "        self.init_weights()\n",
              "        self.hidden_size = hidden_size\n",
              "        self.weight_size = (num_layers, vocab_size, hidden_size)\n",
              "\n",
              "    def init_weights(self):\n",
              "        init_range = 0.1\n",
              "        nn.init.uniform_(self.encoder.weight, -init_range,\n",
              "                         init_range)\n",
              "        nn.init.zeros_(self.decoder.weight)\n",
              "        nn.init.uniform_(self.decoder.weight, -init_range,\n",
              "                         init_range)\n",
              "\n",
              "    def forward(self, input_, hidden_):\n",
              "        embedded = self.encoder(input_)\n",
              "        output, hidden_ = self.rnn(embedded, hidden_)\n",
              "        decoded = self.decoder(hidden_)\n",
              "        return F.log_softmax(input=decoded, dim=1), hidden_\n",
              "\n",
              "    def init_hidden(self):\n",
              "        weight = next(self.parameters())\n",
              "        return (weight.new_zeros(self.weight_size),\n",
              "                weight.new_zeros(self.weight_size))\n",
              "\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row24_col3\" class=\"data row24 col3\" ><P> I think your fundamental problem is that you confuse in_channels and out_channels with Keras shapes. Let's just take the first convolutional layer as an example. In Keras you have:\n",
              "\n",
              "Conv1D(filters=32, kernel_size=8, input_shape=(5000,1), strides=1, padding='same')\n",
              "\n",
              "\n",
              "The PyTorch equivalent should be (changing the kernel size to 7 like you did, we'll come back to it later):\n",
              "\n",
              "nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=3) # different kernel size\n",
              "\n",
              "\n",
              "Note that you don't need to give the shape of your input sequence for pytorch. Now let's see how it compares to what you did:\n",
              "\n",
              "nn.Conv1d(in_channels=1, out_channels=5000, kernel_size=7, stride=1, padding=0) # note padding\n",
              "\n",
              "\n",
              "You just created a huge network. While the correct implementation produces an output of [b, 32, 5000] where b is the batch size, your output is [b, 5000, 5000].\n",
              "\n",
              "Hope this example helps you to correct the rest of your implementation.\n",
              "\n",
              "Finally, some notes on replicating same padding in pytorch. With even kernel sizes, to preserve the size of your input you need asymmetric padding. This I think might not be available when you create the layer. I see you instead changed the kernel size to 7, but it can actually be done with the original kernel size of 8. You can use padding in your forward() function to create the required asymmetric padding.\n",
              "\n",
              "layer = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=8, stride=1, padding=0) # layer without padding\n",
              "x = torch.empty(1, 1, 5000).normal_()  # random input\n",
              "\n",
              "# forward run\n",
              "x_padded = torch.nn.functional.pad(x, (3,4))\n",
              "y = layer(x_padded).shape\n",
              "print(y.shape)  # torch.Size([1, 32, 5000])\n",
              "\n",
              " <P> Assuming each path have it's own weights, may be this could be done with grouped convolution, although pre fusion Linear can cause some trouble.\n",
              "    P = 20\n",
              "    self.features = nn.Sequential(\n",
              "        nn.Conv2d(1*P,10*P, kernel_size = 3, padding = 1, groups = P ),\n",
              "        nn.ReLU(),\n",
              "        nn.Conv2d(10*P, 14*P, kernel_size=3, padding=1, groups = P),\n",
              "        nn.ReLU(),\n",
              "        nn.Conv2d(14*P, 18*P, kernel_size=3, padding=1, groups = P),\n",
              "        nn.ReLU(),\n",
              "        nn.Conv2d(18*P, 256*P, kernel_size=28,          groups = P),  # not shure about this one\n",
              "        nn.Flatten(),\n",
              "        nn.Linear(256*P, 1024 )\n",
              "    )\n",
              "\n",
              " <P> You need to think of the scope of the trainable parameters.\n",
              "\n",
              "If you define, say, a conv layer in the forward function of your model, then the scope of this  \"layer\" and its trainable parameters is local to the function and will be discarded after every call to the forward method. You cannot update and train weights that are constantly being discarded after every forward pass.\n",
              "However, when the conv layer is a member of your model its scope extends beyond the forward method and the trainable parameters persists as long as the model object exists. This way you can update and train the model and its weights.\n",
              " <P> I see this is a nice question.\n",
              "\n",
              "self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "\n",
              "\n",
              "self.conv1 is indeed a variable, but it is also the as special class variable derived nn.Module (read: module).\n",
              "\n",
              "And it is also a callable, implemented __call__ method and this is why we can call it. \n",
              " <P> Keras treats as parameters (weights) many things that will be \"saved/loaded\" in the layer. \n",
              "\n",
              "While both implementations naturally have the accumulated \"mean\" and \"variance\" of the batches, these values are not trainable with backpropagation. \n",
              "\n",
              "Nevertheless, these values are updated every batch, and Keras treats them as non-trainable weights, while PyTorch simply hides them. The term \"non-trainable\" here means \"not trainable by backpropagation\", but doesn't mean the values are frozen. \n",
              "\n",
              "In total they are 4 groups of \"weights\" for a BatchNormalization layer. Considering the selected axis (default = -1, size=32 for your layer)\n",
              "\n",
              "\n",
              "scale (32) - trainable\n",
              "offset (32) - trainable    \n",
              "accumulated means (32) - non-trainable, but updated every batch\n",
              "accumulated std   (32) - non-trainable, but updated every batch\n",
              "\n",
              "\n",
              "The advantage of having it like this in Keras is that when you save the layer, you also save the mean and variance values the same way you save all other weights in the layer automatically. And when you load the layer, these weights are loaded together. \n",
              " <P> I'm assuming you use module interface nn.ReLU to create the acitvation layer instead of using functional interface F.relu. If so, setattr works for me. \n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "# This function will recursively replace all relu module to selu module. \n",
              "def replace_relu_to_selu(model):\n",
              "    for child_name, child in model.named_children():\n",
              "        if isinstance(child, nn.ReLU):\n",
              "            setattr(model, child_name, nn.SELU())\n",
              "        else:\n",
              "            replace_relu_to_selu(child)\n",
              "\n",
              "########## A toy example ##########\n",
              "net = nn.Sequential(\n",
              "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
              "            nn.ReLU(inplace=True),\n",
              "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
              "            nn.ReLU(inplace=True)\n",
              "          )\n",
              "\n",
              "########## Test ##########\n",
              "print('Before changing activation')\n",
              "for child in net.children():\n",
              "    if isinstance(child,nn.ReLU) or isinstance(child,nn.SELU):\n",
              "        print(child)\n",
              "# Before changing activation\n",
              "# ReLU(inplace=True)\n",
              "# ReLU(inplace=True)\n",
              "\n",
              "\n",
              "print('after changing activation')\n",
              "for child in net.children():\n",
              "    if isinstance(child,nn.ReLU) or isinstance(child,nn.SELU):\n",
              "        print(child)\n",
              "# after changing activation\n",
              "# SELU()\n",
              "# SELU(\n",
              "\n",
              " <P> The way these libraries work is that the connections/weights for a layer are represented as a \"tensor\" (i.e. a multi-dimensional array or matrix). This makes the application of a layer behave as a linear algebra operation (a matrix multiplication). PyTorch/Tensorflow aren't representing the individual neural connections as distinct objects in the code in a way that makes sense to think about them as something to be operated on or deleted individually.\n",
              "\n",
              "\n",
              "  1) How to remove specific neuron connections between the layers in NN using any of these frameworks? \n",
              "\n",
              "\n",
              "You could set one of the weights to zero, i.e. layer.weights[x, y]=0, though that doesn't actually \"remove\" it or prevent it from later being changed to non-zero.\n",
              "\n",
              "Maybe you could use a sparse tensor instead of dense, which is a coordinate format containing a list of all non-zero indices and values. Sparse is only going to be more efficient if you have a low percent non-zero values.\n",
              "\n",
              "\n",
              "  2) How to set the specific learning rule for some of the neurons in the layer? \n",
              "\n",
              "\n",
              "By learning rule do you mean optimizers? You can find other posts about multiple optimizers, which would probably work out to be similar to (3) below. E.g.\n",
              "\n",
              "https://discuss.pytorch.org/t/two-optimizers-for-one-model/11085\n",
              "\n",
              "\n",
              "  3) How to set the specific activating function for some of the neurons in the layer?\n",
              "\n",
              "\n",
              "Operators and activation functions are typically implemented to efficiently operate on a full tensor. You could split a layer into two separate smaller layers, and run them next to each other (at the same level in the network).\n",
              "\n",
              "E.g. if you had\n",
              "\n",
              "layer1=torch.nn.Linear(10, 10)\n",
              "\n",
              "\n",
              "but instead of just torch.relu(layer1(input)) you wanted to apply relu to some outputs and, say, sigmoid to others, you could just:\n",
              "\n",
              "layer1a = torch.nn.Linear(10, 5)\n",
              "layer2b = torch.nn.Linear(10, 5)\n",
              "\n",
              "\n",
              "and then \n",
              "\n",
              "torch.cat( (torch.relu(layer1a(x)), torch.sigmoid(layer1b(x)) ), 0)\n",
              "\n",
              "\n",
              "Similarly you can split any tensor into pieces, apply various functions to different ranges/values, and stitch the results back together with torch.cat.\n",
              " <P> You are missing exactly the last step in Keras transformation.\n",
              "There is also a Sequential() class in TensorFlow(Keras) that you can use to instantiate the model.\n",
              "I haven't checked for the exact match between TF and PyTorch, but this should be your starting point to solve your problem.\n",
              "model = tf.keras.Sequential([first_layer, core_layer, last_layer])\n",
              "y = model(x)\n",
              "\n",
              " <P> \n",
              "  How come in this examples and many examples online, they define the\n",
              "  convolutional layers and the fc layers in init, but the subsampling\n",
              "  and activation functions in forward?\n",
              "\n",
              "\n",
              "Any layer with trainable parameters should be defined in __init__. Subsampling, certain activations, dropout, etc.. don't have any trainable parameters so can be defined either in __init__ or used directly via the torch.nn.functional interface during forward.\n",
              "\n",
              "\n",
              "  What is the purpose of using torch.nn.functional for some functions, and torch.nn for others?\n",
              "\n",
              "\n",
              "The torch.nn.functional functions are the actual functions that are used at the heart of the majority of torch.nn layers, they call into C++ compiled code. For example nn.Conv2d subclasses nn.Module, as should any custom layer or model which contains trainable parameters. The class handles registering parameters and encapsulates some other necessary functionality required for training and testing. During forward it actually uses nn.functional.conv2d to apply the convolution operation. As mentioned in the first question, when performing a parameterless operation like ReLU there's effectively no difference between using the nn.ReLU class and the nn.functional.relu function.\n",
              "\n",
              "The reason they are provided is they give some freedom to do unconventional things. For example in this answer which I wrote the other day, providing a solution without nn.functional.conv2d would have been difficult.\n",
              "\n",
              "\n",
              "  Let's say I want to try different image sizes, like 28x28 (MNIST). The\n",
              "  tutorial recommends I resize MNIST. Is there a way to instead change\n",
              "  the values of LeNet? What happens if I don't change them?\n",
              "\n",
              "\n",
              "There's no obvious way to change an existing, trained model to support different image sizes. The size of the input to the linear layer is necessarily fixed and the number of features at that point in the model is generally determined by the size of the input to the network. If the size of the input differs from the size that the model was designed for then when the data progresses to the linear layers it will have the wrong number of elements and cause the program will crash. Some models can handle a range of input sizes, usually by using something like an nn.AdaptiveAvgPool2d layer before the linear layer to ensure the input shape to the linear layer is always the same. Even so, if the input image size is too small then the downsampling and/or pooling operations in the network will cause the feature maps to vanish at some point, causing the program to crash.\n",
              "\n",
              "\n",
              "  What is the purpose of num_flat_features? If you wanted to flatten the\n",
              "  features, couldn't you just do x = x.view(-1, 16*5*5)?\n",
              "\n",
              "\n",
              "When you define the linear layer you need to tell it how large the weight matrix is. A linear layer's weights are simply an unconstrained matrix (and bias vector). The shape of the weight matrix therefore is determined by the input shape, but you don't know the input shape before you run forward so it needs to be provided as an additional parameter (or hard coded) when you initialize the model.\n",
              "\n",
              "To get to the actual question. Yes, during forward you could simply use \n",
              "\n",
              "x = x.view(-1, 16*5*5)\n",
              "\n",
              "\n",
              "Better yet, use \n",
              "\n",
              "x = torch.flatten(x, start_dim=1)\n",
              "\n",
              "\n",
              "This tutorial was written before the .flatten function was added to the library. The authors effectively just wrote their own flatten functionality which could be used regardless of the shape of x. This was probably so you had some portable code that could be used in your model without hard coding sizes. From a programming perspective it's nice to generalize such things since it means you wouldn't have to worry about changing those magic numbers if you decide to change part of the model (though this concern didn't appear to extend to the initialization).\n",
              " <P> You can use a hook for that. Let's consider the following example demonstrated on VGG16:\n",
              "This is the network architecture:\n",
              "\n",
              "Say we want to monitor the input and output for layer (2)  in the features Sequential (that Conv2d layer you see above).\n",
              "For this matter we register a forward hook, named my_hook which will be called on any forward pass:\n",
              "import torch\n",
              "from torchvision.models import vgg16\n",
              "\n",
              "def my_hook(self, input, output):\n",
              "    print('my_hook\\'s output')\n",
              "    print('input: ', input)\n",
              "    print('output: ', output)\n",
              "\n",
              "# Sample net:\n",
              "net = vgg16()\n",
              "\n",
              "#Register forward hook:\n",
              "net.features[2].register_forward_hook(my_hook)\n",
              "\n",
              "# Test:\n",
              "img = torch.randn(1,3,512,512)\n",
              "out = net(img) # Will trigger my_hook and the data you are looking for will be printed\n",
              "\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col0\" class=\"data row25 col0\" >`nan_to_num` produces incorrect output for `BFloat16` on CUDA</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col1\" class=\"data row25 col1\" >See the row `-n` in results of `ULIMIT -a`. You might want to increase that number. FYI I have 500,000 in my system. Closing, please feel free to reopen if that doesn't solve your problem. \n",
              " \n",
              " I am actually trying to reduce the number of bits communicated during allreduce. Using 32bit int or 16bit floats would increase communication. I will try to find a workaround.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col2\" class=\"data row25 col2\" >Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row25_col3\" class=\"data row25 col3\" ><P> QR batching PR was done after 1.1, please allow me to investigate. <P> Hmm, replacing `==` with `torch.allclose` fixes the issue (and difference if default codepath between computed distribution and the actual value is 1e-16 ) <P> In triage, we decided that this relaxation seemed like a reasonable to do.\n",
              " \n",
              " \n",
              " \n",
              " (Unrelated to this issue: @fmassa was wondering if there wasn't a reason why BatchNorm didn't just take a sync argument. @mrshenli will investigate.) <P> Oh sorry I see it has been fixed. <P> Fixed by #59423 <P> Fixed by #41672. <P> `1e-2` is 0.01. So the default values do match. <P> @mrshenli Thank you for the clarification. \n",
              " \n",
              " \n",
              " \n",
              " I am actually trying to reduce the number of bits communicated during allreduce. Using 32bit int or 16bit floats would increase communication. I will try to find a workaround. \n",
              " \n",
              " \n",
              " \n",
              " Anyways I am closing this issue for now. <P> See the row `-n` in results of `ulimit -a`. You might want to increase that number. FYI I have 500,000 in my system. \n",
              " \n",
              " Closing, please feel free to reopen if that doesn't solve your problem. <P> Fixed in #3433</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col0\" class=\"data row26 col0\" > i found that 1.5 upgrade has change of autograd\n",
              "and also there is a tutorial for me who doesn’t know what’s wrong(https://github.com/pytorch/pytorch/releases)  issue:[torch.optim optimizers changed to fix in-place checks for the changes made by the optimizer]\n",
              "def model(input, target, param):\n",
              "    return `(input * param ** 2 - target).norm()`\n",
              "\n",
              "param = torch.randn(2, requires_grad=True)\n",
              "input = torch.randn(2)\n",
              "target = torch.randn(2)\n",
              "sgd = optim.SGD([param], lr=0.001)\n",
              "loss = model(input, target, param)\n",
              "loss.backward(retain_graph=True)\n",
              "sgd.step()\n",
              "loss.backward()\n",
              "param.grad\n",
              "\n",
              "before 1.5, ↑ codes works. but after 1.5\n",
              "def model(input, target, param):\n",
              "    return (input * param ** 2 - target).norm()\n",
              "\n",
              "param = torch.randn(2, requires_grad=True)\n",
              "input = torch.randn(2)\n",
              "target = torch.randn(2)\n",
              "sgd = optim.SGD([param], lr=0.001)\n",
              "loss = model(input, target, param.clone())\n",
              "loss.backward(retain_graph=True)\n",
              "sgd.step()\n",
              "loss.backward()\n",
              "param.grad\n",
              "\n",
              "i have to put param.clone() into model\n",
              "but in reality, i don’t use a model like above, i just put only inputs into my model.\n",
              "class test1(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(test1, self).__init__()\n",
              "        self.layer1 = nn.Linear(10, 1)\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = self.layer1(x)\n",
              "        return x\n",
              "\n",
              "t = test1().to(device)\n",
              "\n",
              "optimizer1 = torch.optim.Adam(t.parameters(), lr=0.001)\n",
              "\n",
              "for i, (images, labels) in enumerate(data_loader):\n",
              "    images = images.view(batch_size, -1)[:, :10].to(device)\n",
              "    labels = labels.float().to(device)\n",
              "\n",
              "    a = t(images)\n",
              "    loss = criterion(a, labels)\n",
              "\n",
              "    optimizer1.zero_grad()\n",
              "    loss.backward(retain_graph=True)\n",
              "    optimizer1.step()\n",
              "    loss.backward()\n",
              "\n",
              "and ↑ this is my test code.\n",
              "how can i change ↑ this code?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col1\" class=\"data row26 col1\" >If create_graph=True, backward() replaces.grad with a new tensor.grad + new grad, which attempts (but does not guarantee) matching the preexisting.grad’s strides. The default behavior (letting.grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.step() will not affect.grad layouts.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col2\" class=\"data row26 col2\" >\n",
              "The problem is that the original code here was computing wrong gradients.\n",
              "You can modify this quite easily by overriding the linear forward function for this case:\n",
              "class MyLinear(nn.Linear):\n",
              "    def forward(self, input):\n",
              "        return F.linear(input, self.weight.clone(), self.bias.clone())\n",
              "\n",
              "# And use this one later:\n",
              "self.layer1 = nn.MyLinear(10, 1)\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row26_col3\" class=\"data row26 col3\" ><P> Hi, You can check if torch.is_grad_enabled() in the backward and if gd.requires_grad.That will tell if you something wants the gradients to be computed for your function. Namely if grad mode is enabled and the input requires_grad then you should create the graph. Otherwise, it is not needed. <P> If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad’s strides. The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. <P> A dirty but working hack would be to zero out the gradient of these parts right before the optimizer step.\n",
              "thanks a lot.it works. <P> \n",
              "[UPDATED]\n",
              "You are not correctly passing the y1.grad into y1.backward in the second example. After the first backward all the intermediate gradient will be destroyed, you need a special hook to extract that gradients. And in your case you are passing the None value. Here is small example to reproduce your case:\n",
              "\n",
              "Code:\n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "\n",
              "torch.manual_seed(42)\n",
              "\n",
              "\n",
              "class Model1(nn.Module):\n",
              "\n",
              "    def __init__(self):\n",
              "        super().__init__()\n",
              "\n",
              "    def forward(self, x):\n",
              "        return x.pow(3)\n",
              "\n",
              "\n",
              "class Model2(nn.Module):\n",
              "\n",
              "    def __init__(self):\n",
              "        super().__init__()\n",
              "\n",
              "    def forward(self, x):\n",
              "        return x / 2\n",
              "\n",
              "\n",
              "model1 = Model1()\n",
              "model2 = Model2()\n",
              "criterion = nn.MSELoss()\n",
              "\n",
              "X = torch.randn(1, 5, requires_grad=True)\n",
              "y = torch.randn(1, 5)\n",
              "\n",
              "y1 = model1(X)\n",
              "y2 = model2(y1)\n",
              "\n",
              "loss = criterion(y2, y)\n",
              "# We are going to backprop 2 times, so we need to \n",
              "# retain_graph=True while first backward\n",
              "loss.backward(retain_graph=True)\n",
              "\n",
              "try:\n",
              "    y1.backward(y1.grad)\n",
              "except RuntimeError as err:\n",
              "    print(err)\n",
              "    print('y1.grad: ', y1.grad)\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "grad can be implicitly created only for scalar outputs\n",
              "y1.grad:  None\n",
              "\n",
              "\n",
              "So you need to extract them correctly:\n",
              "\n",
              "Code:\n",
              "\n",
              "def extract(V):\n",
              "    \"\"\"Gradient extractor.\n",
              "    \"\"\"\n",
              "    def hook(grad):\n",
              "        V.grad = grad\n",
              "    return hook\n",
              "\n",
              "\n",
              "model1 = Model1()\n",
              "model2 = Model2()\n",
              "criterion = nn.MSELoss()\n",
              "\n",
              "X = torch.randn(1, 5, requires_grad=True)\n",
              "y = torch.randn(1, 5)\n",
              "\n",
              "y1 = model1(X)\n",
              "y2 = model2(y1)\n",
              "\n",
              "loss = criterion(y2, y)\n",
              "y1.register_hook(extract(y1))\n",
              "loss.backward(retain_graph=True)\n",
              "\n",
              "print('y1.grad', y1.grad)\n",
              "\n",
              "y1.backward(y1.grad)\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "y1.grad:  tensor([[-0.1763, -0.2114, -0.0266, -0.3293,  0.0534]])\n",
              "\n",
              " <P> I find out that I made a mistake that I wrap these code in torch.no_grad()  <P> fk me. I wrapped it in with torch.no_grad() in caller function. <P> Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad’s strides. <P> If you need manual control over .grad’s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True. <P> The error is because the weight of the linear layer has changed (through optimizer.step).\n",
              "One might add that here, you have the gradient computation for addmm (which powers linear) pretend it would also want to compute the input derivative for which it would need the weight which, in this trivial use, is not actually the case. If you have a multiple layers, all but the first need the input gradient and you cannot actually change the weight and then compute the (correct) backward, not even with retain_graph.\n",
              "\n",
              " <P> The first time you call backward, the .grad attribute of the parameters of your model will be updated from None, to the gradients. If you do not reset the gradients to zero, future calls to .backward() will accumulate (i.e. add) gradients into the attribute (see the docs).\n",
              "When you call model.zero_grad() you are doing the reset.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col0\" class=\"data row27 col0\" >PyTorch - Change weights of Conv2d</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col1\" class=\"data row27 col1\" >When you declared nn.Conv2d the weights are initialized via this code.\n",
              "In particular, if you give bias it uses initialization as proposed by Kaiming et.al. It initializes as uniform distribution between (-bound, bound) where bound=\\sqrt{6/((1+a^2)fan_in)} (See here).\n",
              "\n",
              "You can initialize weight manually too.\n",
              "\n",
              "When you call optimizer.step and optimizer has parameters of convolutional filter registered they are updated.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col2\" class=\"data row27 col2\" >Try this :\n",
              " layer.load_state_dict({'weight': torch.tensor([[[[0.4738, -0.2197],\n",
              "                      [-0.3436, -0.0754]]],\n",
              "                    [[[0.1662, 0.4098],\n",
              "                      [-0.4306, -0.4828]]]])}, strict=False)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row27_col3\" class=\"data row27 col3\" ><P> When you declared nn.Conv2d the weights are initialized via this code. \n",
              "\n",
              "In particular, if you give bias it uses initialization as proposed by Kaiming et.al. It initializes as uniform distribution between (-bound, bound) where bound=\\sqrt{6/((1+a^2)fan_in)} (See here).\n",
              "\n",
              "You can initialize weight manually too. This has been answered elsewhere (See here) and I won't repeat it. \n",
              "\n",
              "When you call optimizer.step and optimizer has parameters of convolutional filter registered they are updated. \n",
              " <P> The element-wise operation always returns a FloatTensor. It is not possible to assign normal tensors as weight of layers. \n",
              "\n",
              "There are two possible options to deal with it. You can assign it to the data attribute of your weight, there it is possible assign normal tensors. \n",
              "\n",
              "Or alternatively you convert your result to an nn.Parameter itself, then you can assign it to wfx.weight.\n",
              "\n",
              "Here is an example which shows both ways:\n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "wfx = nn.Linear(10, 10)\n",
              "mask_use = torch.rand(10, 10)\n",
              "#wfx.weight = wfx.weight * mask_use #your example - this raises an error\n",
              "\n",
              "# Option 1: write directly to data\n",
              "wfx.weight.data = wfx.weight * mask_use\n",
              "\n",
              "# Option 2: convert result to nn.Parameter and write to weight\n",
              "wfx.weight = nn.Parameter(wfx.weight * mask_use)\n",
              "\n",
              "\n",
              "Disclaimer: When using an = (assignment) on the weights you are replacing the weights tensor of your parameter. This may have unwanted effects on the graph resp. the optimization step.\n",
              " <P> A 2D convolution layer contains one kernel per input channel, per output channel. So in your case, this will be 6*16=96 kernels. For 3x3 kernels, this corresponds to 3*3*96 = 864 parameters.\n",
              " import torch\n",
              "\n",
              " conv = torch.nn.Conv2d(6, 16, (3, 3))\n",
              " torch.numel(conv.weight)\n",
              "864\n",
              "\n",
              "For one image, one kernel per input channel is first applied. In your case, this results in 6 features maps, that are summed together (+ a possible bias) to form 1 of the output channel. Then, you repeat this 15 times to form the 15 other output channels.\n",
              " <P> \n",
              "self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
              "self.fc2 = nn.Linear(120, 84) # you can use any number instead of 120 play with this number and see which gives you best result.\n",
              "self.fc3 = nn.Linear(84, 10) \n",
              "\n",
              "120 is number of units in first layer after conv layer , 84 in second layer and 10 in last which probably is dimension of your output layer ie. 10 possible type of classification.\n",
              "You are correct the dimension of second and third layer is not fixed and you can try different value of num of units and choose one which gives you the best result. You can play with it but you can also look at some of the best performing models and follow the structure they use.\n",
              " <P> It matters because you are doing 2D convolution over the images which means the depth of the filter(kernel) must be equal to the number of in_channels(pytorch sets it for you) so the true kernel size is [in_channels,1,1]. On the other hands we can say that out_channels number is the number of kernels so the number of weights = number of kernels * size of kernel = out_channels * (in_channels * kernel_size). Here is 2D conv with 3D input\n",
              "\n",
              " <P> All the input channels are connected to each output channel (if group = 1, as by default) by convolution with filters (kernels) -- one for each output channel. Each kernel though has sub-kernels for each input channel.\n",
              "\n",
              "So in the first layer you have in_channels = 1 and out_channels = 64 meaning that there are 64 kernels (and sub-kernels). In the second layer you have in_channels = 64 and out_channels = 128 meaning that there are 128 kernels each having 64 * 128 sub-kernels.\n",
              "\n",
              "Here's a simple example of one conv layer taken from cs231n for clarification:\n",
              "\n",
              "\n",
              "\n",
              "And my implementation in Pytorch:\n",
              "\n",
              "import torch\n",
              "from torch import nn\n",
              "\n",
              "\n",
              "cnn = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3,\n",
              "                stride=2, padding=1, bias=True, groups=1)\n",
              "\n",
              "\n",
              "w0 = torch.FloatTensor([[[-1, -1,  0],\n",
              "                         [ 1,  1,  1],\n",
              "                         [ 1,  1,  0]],\n",
              "\n",
              "                        [[ 1,  1, -1],\n",
              "                         [ 0,  0,  0],\n",
              "                         [ 1,  1, -1]],\n",
              "\n",
              "                        [[ 0, -1,  0],\n",
              "                         [-1,  0, -1],\n",
              "                         [ 1,  0,  1]]])\n",
              "\n",
              "b0 = torch.FloatTensor([1])\n",
              "\n",
              "w1 = torch.FloatTensor([[[-1,  0,  0],\n",
              "                         [ 1,  1,  1],\n",
              "                         [-1, -1,  0]],\n",
              "\n",
              "                        [[ 1, -1, -1],\n",
              "                         [-1,  1, -1],\n",
              "                         [ 1, -1,  0]],\n",
              "\n",
              "                        [[ 1, -1,  0],\n",
              "                         [ 0,  1,  1],\n",
              "                         [ 1,  0,  1]]])\n",
              "\n",
              "b1 = torch.FloatTensor([0]) \n",
              "\n",
              "\n",
              "cnn.weight = torch.nn.Parameter(torch.stack((w0, w1), 0))\n",
              "cnn.bias = torch.nn.Parameter(torch.cat((b0, b1), 0))\n",
              "\n",
              "inpt = torch.FloatTensor([[[ 1, 2, 0, 1, 2],\n",
              "                           [ 1, 0, 2, 2, 0],\n",
              "                           [ 2, 0, 0, 2, 2],\n",
              "                           [ 0, 0, 2, 2, 0],\n",
              "                           [ 2, 2, 2, 1, 2]],\n",
              "\n",
              "                          [[ 2, 0, 0, 1, 1],\n",
              "                           [ 1, 0, 2, 1, 2],\n",
              "                           [ 2, 0, 2, 2, 1],\n",
              "                           [ 0, 2, 0, 0, 1],\n",
              "                           [ 1, 2, 1, 2, 0]],\n",
              "\n",
              "                          [[ 0, 0, 2, 1, 2],\n",
              "                           [ 0, 1, 0, 2, 0],\n",
              "                           [ 1, 1, 0, 0, 2],\n",
              "                           [ 0, 0, 0, 1, 1],\n",
              "                           [ 0, 1, 2, 0, 2]]])\n",
              "\n",
              "cnn(inpt.unsqueeze(0))\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "tensor([[[[ 7.,  9., 10.],\n",
              "          [ 0.,  6., 10.],\n",
              "          [ 2.,  5.,  2.]],\n",
              "\n",
              "         [[ 4.,  4.,  4.],\n",
              "          [ 5.,  1.,  2.],\n",
              "          [ 2.,  6.,  0.]]]])\n",
              "\n",
              " <P> The documentation at https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.conv2d seems to answer your question:\n",
              "\n",
              " input – input tensor of shape (minibatch,in_channels,iH,iW)\n",
              " weight – filters of shape (out_channels,in_channels/groups,kH,kW)\n",
              "\n",
              "\n",
              "so your x must be size (batch_size, in_channels, 16, 16)\n",
              "\n",
              "and your kernel (batch_size, out_channels/groups, kH, kW)\n",
              " <P> I think you'll find receptive field arithmetics useful for your understanding.\n",
              "\n",
              "Your net has 3 convolution layers each with kernel size of 3x3 and padding of 1 pixels, which means that the spatial output of your convolution layers is the same as their input.\n",
              "Each conv layer is followed by a max pooling with stride 2, that is, it reduces the spatial dimensions by a factor of 2.\n",
              "\n",
              "So, in the spatial domain, you have an input of size 32x32 after first conv and pool its dimensions are 16x16, after the second conv and pool it is 8x8 and after the third conv+pool it is 4x4.\n",
              "\n",
              "As for the \"feature\"/\"channel\" dimension: the input has 3 channels. The first conv layer has 16 filters (\"out_channels=16\") then 32 and finally 64.\n",
              "Thus, after three conv layers your feature map has 64 channels (per spatial location).\n",
              "Overall, an input of size 3x32x32 becomes 64x4x4 after the three conv+pooling layers defined by your network.\n",
              "\n",
              "a nn.Linear layer does not assign \"spatial\" meaning to its inputs and expects a 1D input (per entry in a minibatch), thus your forward function \"eliminates\" the spatial dimensions and converts x to a 1D vector using the x.view(-1, 64 * 4 * 4) command.\n",
              " <P> You can backtrack the size of the input image from the first fully connected layer i.e. fc1 = Linear(4*4*50, 500). The input to the fc1 is 50x4x4 (CxHxW) (here 50 is the channel dimension as evident from the previous conv2 layer). Thus, the output of conv2 (before max-pooling operation) is 50x8x8 as you're performing pooling using 2x2 - max_pool2d(x, 2, 2).\n",
              "\n",
              "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)\n",
              "\n",
              "\n",
              "Now, you can get the input size of image before the convolution operation using the formula (W-F+2P)/S + 1 = output size. Here, W is input size, F is filter/kernel size, and P is padding used, and S is the stride. Thus, (W-5+2*0)/1+1=8 => W=12. \n",
              "\n",
              "Hence, the input to conv2 is 20x12x12.\n",
              "\n",
              "In the same way, we can continue the process as follows:\n",
              "\n",
              "Output of conv1 (i.e. before max-pooling): 20x24x24\n",
              "Input to conv1: 1x28x28. ((W-5+2*0)/1+1=24 => W=28)\n",
              "\n",
              "Hence, the input image size is 1x28x28.\n",
              "\n",
              "\n",
              "\n",
              "The error is because the fully connected layers expect input of fixed size and that defines your network. In order to pass variable sized input, you may use need to transform the input to size that your network (fc layers) expects using transformations such as cropping.\n",
              "\n",
              "Also, there are networks that can take variable size input e.g. Fully Convolutional Networks (FCN), which doesn't contain fc layers, but only conv layers. You may also read about Spatial Pyramid Pooling (used in a network called DeepLab for semantic segmentation). Read more here.\n",
              " <P> I found an answer here:\n",
              "Efficient forward pass in nn.Linear #2159 \n",
              "\n",
              "It seems like there is no real reasoning behind this. However the transpose operation doesn't seem to be slowing down the computation.\n",
              "\n",
              "According to the issue mentioned above, during the forward pass the transpose operation is (almost) free in terms of computation. While during the backward pass leaving out the transpose operation would actually make computation less efficient with the current implementation.\n",
              "\n",
              "The last post in that issue sums it up quite nicely:\n",
              "\n",
              "\n",
              "  It's historical weight layout, changing it is backward-incompatible.\n",
              "  Unless there is some BIG benefit in terms of speed or convenience, we\n",
              "  wont break userland.\n",
              "\n",
              "\n",
              "https://github.com/pytorch/pytorch/issues/2159#issuecomment-390068272\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col0\" class=\"data row28 col0\" >Please fix all the related links format from http to https</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col1\" class=\"data row28 col1\" >It seems like you are using torchvision's Image Transforms. Some of these Transforms are expecting as input a PIL.Image object, rather than a Tensor or NumPy array.\n",
              "You are using io.imread to read ths image file, and I suspect this IO is not PL.Image resulting with a NumPy Array.\n",
              "Make sure you pass PILImage objects to Transforms and that your DogsDataset returns a 3D tensor for image (C-H-W shaped).</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col2\" class=\"data row28 col2\" >closed via </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row28_col3\" class=\"data row28 col3\" ><P> This seems to be the PIL/Pillow issue as described here . <P> Simply replacing the names does not help. Assistance is needed. <P> it'is not work.\n",
              " \n",
              " ![1](https://user-images.githubusercontent.com/5284540/69575091-a85a5180-0fda-11ea-9e7c-a4ec46201dd7.png) <P> uninstall the pyarrow installed by pip and then reinstall with conda works for me. <P> This issue got resolved once I uninstalled torch_tb_profiler and downgraded Tensorboard 2.5.0 to 1.15.0 as suggested in this answer\n",
              " <P> It seems like you are using torchvision's image transforms. Some of these transforms are expecting as input a PIL.Image object, rather than a tensor or numpy array.\n",
              "You are using io.imread to read ths image file, and I suspect this io is not PIL.Image resulting with a numpy array.\n",
              "Make sure you pass PIL.Image objects to transforms and that your DogsDataset returns a 3D tensor for image (C-H-W shaped).\n",
              " <P> actually, that file already has memory included.\n",
              " \n",
              " \n",
              " \n",
              " Are you following instructions from https://github.com/pytorch/pytorch#from-source\n",
              " \n",
              " Especially: \n",
              " \n",
              " \n",
              " \n",
              " MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " This part is important: `MACOSX_DEPLOYMENT_TARGET=10.9 ` <P> I am using Torch 1.4.0 on Windows and I had the same issue. Turns out I had installed the 2.x version of Tensorboard. I reverted back to 1.15.0 and it solved the issue.\n",
              " <P> Hello. I experiencing this issue only on one of my window systems. I think my problem might be visual studio related but no Idea what else to do to fix it. I first tried installing the VC Redist package. Then, I installed the community edition for the Visual Studio 2017. However, I am still getting:\n",
              " \n",
              " \n",
              " \n",
              " C:\\Users\\Mauricio>where api-ms-win-crt-utility-l1-1-0.dll\n",
              " \n",
              " INFO: Could not find files for the given pattern(s).\n",
              " \n",
              " \n",
              " \n",
              " When I try the dependency walker on the \"_C.cp35-win_amd64.pyd\", it marks that all references are correct and the \"api-ms-win-crt-utility-l1-1-0.dll\" is being mapped to \"C:\\Windows\\system32\\ucrtbase.dll\", but on the python console I am still getting the following error:\n",
              " \n",
              " \n",
              " \n",
              "  from torch._C import *\n",
              " \n",
              " ImportError: DLL load failed: The specified procedure could not be found.\n",
              " \n",
              " \n",
              " \n",
              " I even moved from the CUDA 9.2 to cpu-only to reduce the dependencies, but still no luck. Any suggestions? <P> I would recommend uninstalling Pillow, PIL (if they exist) and reinstalling pillow.\n",
              " \n",
              " Something like:\n",
              " \n",
              " \n",
              " \n",
              " pip uninstall Pillow\n",
              " \n",
              " pip uninstall PIL\n",
              " \n",
              " pip install Pillow\n",
              " \n",
              " \n",
              " \n",
              " In the future you should post your question to https://discuss.pytorch.org/ , we like to keep issues on github to PyTorch-only bugs.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col0\" class=\"data row29 col0\" >torch.hub does not close the resource before removing</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col1\" class=\"data row29 col1\" >The root cause is that `fork` inherits the current process state, including default Tensor type, while `spawn` doesn't. This is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. And that's my plan for fixing it in the short term.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col2\" class=\"data row29 col2\" >Add the \"cached_zipfile.close()\" call after the cached_zipfile.extractall(hub_dir) function in the _get_cache_or_reload function.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row29_col3\" class=\"data row29 col3\" ><P> tried change the saving path worked for me <P> Reading the code, I think this is just a simple data race. Patch coming. <P> @malfet It should work too. And that's my plan for fixing it in the short term. <P> Hi. i resolved the problem through reinstalling Pytorch version(1.5.1 > 1.0.0).\n",
              "thanks. <P> the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific.  <P> It works. Thanks. @mrshenli <P> Issue resolved by downgrad back to PyTorch 1.5.0So it looks like a PyTorch 1.6 issue <P> thanks for your help I've fixed it by my self wrong pytorch version was installed:) <P> Thanks for the fix <P> I love the option's name `--bad-jit` @kevinstephano ðŸ˜„ \n",
              " \n",
              " \n",
              " \n",
              " I think I know what the problem is here, I'll test the fix in the next few days.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col0\" class=\"data row30 col0\" >NO_DISTRIBUTED environment variable documentation needs to be updated</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col1\" class=\"data row30 col1\" >You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the CFX headers only in several months.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col2\" class=\"data row30 col2\" >I think we standardize on USE_... now, so the first variant would be it. ~~Maybe I'll tack this onto #17295.~~\n",
              " \n",
              " On second thought, maybe it's best to fix all of the variables in the description that aren't up to date in one go.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row30_col3\" class=\"data row30 col3\" ><P> You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the C++ headers only in several months. <P> We're running CI builds against CUDA 10, and I believe the current plan for pre-built binaries is the next release. <P> include_dirs now supports both absolute and relative paths. <P> Turns out the GPU build in was cached by setuptools (build/, dist/, etc.) and also installed!\n",
              " <P> We seem to have agreed to leave this as is. <P> torch.cuda._check_capability() <P> This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number. <P> A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available. Note <P> It looks like [that feature](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#skipping-members) already exists. I found a [simple example](https://stackoverflow.com/a/21449475) of how to include this, although it would need to check if distributed is available. <P> Update: CUDA 9.2 is added into our Windows AMI. We might want to consider adding a CUDA 9.2 Windows CI, or changing current CI to CUDA 9.2.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col0\" class=\"data row31 col0\" >What is the difference between these backward training methods in Pytorch?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col1\" class=\"data row31 col1\" >model.weight.data is the stored value of the parameter. Once loss.backward() is called, pytorch also calculates and stores the gradient. It is simply taking the the gradient value from the backward pass and updating the parameters to perform gradient descent. There is no reason an optimizer shouldn't work here either, but I can't help with that unless you give more info.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col2\" class=\"data row31 col2\" >First of all:\n",
              "loss.backward() backpropagates the error and assigns a gradient for every parameter along the way that has requires_grad=True.\n",
              "optimizer.step() updates the model parameters using their stored gradients\n",
              "optimizer.zero_grad() sets the gradients to 0, so that you can backpropagate your loss and update your model parameters for each batch without interfering with other batches.\n",
              "1 and 2 are quite similar, but if your model uses batch statistics or you have an adaptive optimizer they will probably perform differently. However, for instance, if your model doesn't use batch statistics and you have a plain old SGD optimizer, they will produce the same result, even though 1 would be faster since you do the backprop only once.\n",
              "3 is a completely different case, since you update your model parameters with loss_G_D.backward() and optimizer.step() before processing and backpropagating loss_G_C.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row31_col3\" class=\"data row31 col3\" ><P> Why do you expect it to not work? Basically what it is doing is manually implementing an optimizer. p.data is the stored value of the parameter. It also provides an internal function add_ that calculates +=. Once loss.backward() is called, pytorch also calculates and stores the gradient. It is simply taking the the gradient value from the backward pass and updating the parameters to perform gradient descent. There is no reason an optimizer shouldn't work here either, but I can't help with that unless you give more info.\n",
              " <P> \n",
              "I believe that the optimizer assigns None to all those places were you have set requires_grad=False. As a result, the gradients of all those parameters are not updating. An optimizer works with frozen params even when you do optim = torch.optim.AdamW(model.parameters(), lr=param['lr'], amsgrad=True),\n",
              "as essentially, the params have just requires_grad set as False. My suggestion is unfreeze the params and you will see a difference in training time \n",
              "For more info, check out this with link \"https://discuss.pytorch.org/t/trying-to-understand-optimizer-and-relation-to-requires-grad/7994\"\n",
              "If the optimizer is smart enough so that it looks at the requires_grad, why do people pass lambda functions into the optimizer, why not just pass in model.parameters()?\n",
              "As for my training time, because I was setting all the BatchNorm to be unfrozen, and those are scattered literally all throughout the model, backdrop still had to be done, even though it wasn’t updating the parameters, since everything with backpropogation is chained/dependent.  I tried to just freeze the classifier and saw a difference in speed, but since the actual updates to the weights is super quick, its the actual calculations that take all the time, that is why I wasn’t seeing a speed increase.\n",
              "So I guess I am still unclear though.  Is there any difference or benefit to passing in model.parameters() vs passing in filter(lambda p: p.requires_grad, model.parameters()) when you have parts of your model frozen?\n",
              "Assuming that you set all the requires_grad flags before training at all, there is very little difference in using filter.\n",
              "In the optimizer code, it iterates over each parameter and if the gradient is None, it continues. If requires_grad=False, the parameter’s gradient will never be updated thus it will always remains None. For this example,  filter adds very little performance boost.\n",
              ", it makes sense.  Perhaps the behavior of at least some optimizers was different in versions past?  I had read that the optimizer would throw an error if it encountered frozen parameters, that is why I was concerned that perhaps my parameters were not set right.\n",
              "same question in\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Parameters with requires_grad = False are updated during training with link \"https://discuss.pytorch.org/t/parameters-with-requires-grad-false-are-updated-during-training/90096\" autograd with link \"/c/autograd/7\"\n",
              "\n",
              "\n",
              "    . I’am trying to freeze front layers during training. \n",
              "Before starting optimization, the optimizer is constructed by \n",
              "optimizer = torch.optim.SGD(net.parameters(), lr, ...)\n",
              "\n",
              "Then, during training, i changed the front layers’ requires_grad=False. \n",
              "Specifically, \n",
              "for epoch in range(total_epoch):\n",
              "  if epoch == freeze_epoch:\n",
              "    net.conv1.weight.requires_grad = False\n",
              "  train_one_epoch() #update network in an epoch\n",
              "\n",
              "\n",
              "However, I found that the weights of the front layers are still updated. \n",
              "I als…\n",
              "  \n",
              "\n",
              "\n",
              "momentum, weight decay affect updating params with requires_grad=False\n",
              "Thanks, so it seems the lambda function has merits. <P> To make the gradient descent step, you normally use just optimizer.step(). Here is also an example taken from the documentation (same link at bottom), what it looks like in general:\n",
              "\n",
              "for input, target in dataset:\n",
              "    optimizer.zero_grad()\n",
              "    output = model(input)\n",
              "    loss = loss_fn(output, target)\n",
              "    loss.backward()\n",
              "    optimizer.step()\n",
              "\n",
              "\n",
              "I don't know where you got this model.step()? Does did you try it? \n",
              "\n",
              "If your model really possesses some kind of step()-functionality, it probably does something different.\n",
              "\n",
              "But unless you define something extra, your model gets its functions from nn.Module and this does not have step function!\n",
              "\n",
              "See this example from the the Pytorch Documentation:\n",
              "\n",
              "import torch.nn as nn\n",
              "import torch.nn.functional as F\n",
              "\n",
              "class Model(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Model, self).__init__()\n",
              "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = F.relu(self.conv1(x))\n",
              "        return F.relu(self.conv2(x))\n",
              "\n",
              "model = Model()\n",
              "model.step()\n",
              "\n",
              "\n",
              "Trying to call step() result in an AttributeError:\n",
              "\n",
              "---------------------------------------------------------------------------\n",
              "AttributeError                            Traceback (most recent call last)\n",
              "ipython-input-41-b032813f7eda in module\n",
              "     13 \n",
              "     14 model = Model()\n",
              "--- 15 model.step()\n",
              "\n",
              "~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in __getattr__(self, name)\n",
              "    530                 return modules[name]\n",
              "    531         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
              "-- 532             type(self).__name__, name))\n",
              "    533 \n",
              "    534     def __setattr__(self, name, value):\n",
              "\n",
              "AttributeError: 'Model' object has no attribute 'step'\n",
              "\n",
              "\n",
              "To sum it up, normally your model should not have a step() function, optimizer.step() is the way to go if you want to do the optimization step.\n",
              "\n",
              "See also here:\n",
              "https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
              " <P> Without delving too deep into the internals of pytorch, I can offer a simplistic answer:\n",
              "Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values.\n",
              "More info on computational graphs and the additional \"grad\" information stored in pytorch tensors can be found in this answer.\n",
              "Referencing the parameters by the optimizer can sometimes cause troubles, e.g., when the model is moved to GPU after initializing the optimizer.\n",
              "Make sure you are done setting up your model before constructing the optimizer. See this answer for more details.\n",
              " <P> Remember that the parameters are updated through the optimizer's step method.When the optimiser is initialised you pass it all the parameters it will modify. So yes, there are still some uncleared gradients in the discriminator parameters, however: loss_G.backward() optimizer_G.step() Wont be affected by them, as this will only modify the parameters of the generator based on its gradients wrt loss_G. <P> \n",
              "  Optimizer is initialized with net.parameters(), which I thought are internal weights of the net.\n",
              "\n",
              "\n",
              "This is because the optimizer will modify the parameters of your net during the training.\n",
              "\n",
              "\n",
              "  Loss does not access these parameters nor the net itself. It only has access to net's outputs and input labels.\n",
              "\n",
              "\n",
              "The loss only computes an error between a prediction and the truth.\n",
              "\n",
              "\n",
              "  Optimizer does not access loss either.\n",
              "\n",
              "\n",
              "It accesses the tensors that were computed during loss.backward\n",
              " <P> If as you suggest, the optimizers and losses for F and G can be separated, then I don't think that it will be necessary to implement any different update functionalities since you can specify the set of parameters for each optimizer, e.g.\n",
              "\n",
              "optimizer_F = optim.SGD(F.parameters(),...)\n",
              "optimizer_G = optim.SGD(G.parameters(),...)\n",
              "\n",
              "\n",
              "then when you call optimizer_F.step() it will only update the parameters of F and similarly optimizer_G.step() will only update the parameters of G.\n",
              " <P> First of all:\n",
              "loss.backward() backpropagates the error and assigns a gradient for every parameter along the way that has requires_grad=True.\n",
              "optimizer.step() updates the model parameters using their stored gradients\n",
              "optimizer.zero_grad() sets the gradients to 0, so that you can backpropagate your loss and update your model parameters for each batch without interfering with other batches.\n",
              "1 and 2 are quite similar, but if your model uses batch statistics or you have an adaptive optimizer they will probably perform differently. However, for instance, if your model doesn't use batch statistics and you have a plain old SGD optimizer, they will produce the same result, even though 1 would be faster since you do the backprop only once.\n",
              "3 is a completely different case, since you update your model parameters with loss_G_D.backward() and optimizer.step() before processing and backpropagating loss_G_C.\n",
              "Given all of these, it's up to you which one to choose depending on your application.\n",
              " <P> TLDR: optimizer will update only the parameters specified to it, whereas backward() call computes the gradients for all variables in the computation graph. So, it is useful to detach() the variables for which gradient computation is not required at that instant.\n",
              "\n",
              "I believe the answer lies in the way things are implemented within PyTorch.\n",
              "\n",
              "\n",
              "tensor.detach() creates a tensor that shares storage with tensor that does not require grad. So, effectively, you cut off the computation graph. That is, doing fake_pred = d(g(noise_batch).detach()) will detach (cut off) the computation graph of the generator. \n",
              "When you call backward() on the loss, gradients are calculated for the entire computation graph (irrespective of whether optimizer uses it or not). Thus, cutting off the generator part will avoid the gradient computations for the generator weights (since they are not required).\n",
              "Also, only the parameters passed to particular optimizer are updated when optimizer.step() is called. So, the g_optim will only optimize the parameters passed to it (You don't explicitly mention which parameters are passed to g_optim). Similarly, d_optim will only update d.parameters() since you explicitly specify that.\n",
              "\n",
              " <P>  it is the latter. model.eval() has effect on dropout, batchnorm etc. You can use model.eval() in combination with with torch.no_grad() during inference phase.\n",
              "from the docs:\n",
              "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n",
              "eval doesn’t turn off history tracking.\n",
              "thanks  and \n",
              "I also assumed that eval() mode automatically turns off gradient computation. Hopefully you can see why this might be confusing for us newcomers. I would request to emphasize this point in docs at nn.Module’s eval() function. Actually apart from FAQ, an article pointing out common mistakes and confusions would be great. Thanks.\n",
              "Exactly! Emm, so is this article available now?\n",
              "\n",
              "\n",
              "\n",
              " Shihab_Shahriar:\n",
              "\n",
              "Actually apart from FAQ, an article pointing out common mistakes and confusions would be great.\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col0\" class=\"data row32 col0\" >Keras learning rate decay in pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col1\" class=\"data row32 col1\" >I'm wondering why you don't have loss.backward() after the line that you compute the loss (i.e., loss = criterion(outputs,target)) in your training snippet. This will help backpropagating and ultimately updating the parameters of your network upon optimizer.step(). Also, try using lower learning rates as lr=1 normally is too much in training such networks. Try using learning rates in between 0.001-0.01 to see if your network is learning the mapping between input X and target Y.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col2\" class=\"data row32 col2\" >Based on the implementation in Keras I think your first formulation is the correct one, the one that contain the initial learning rate (note that self.lr is not being updated).\n",
              "\n",
              "However I think your calculation is probably not correct: since the denominator is the same, and lr_0 >= lr since you are doing decay, the first formulation has to result in a bigger number.\n",
              "\n",
              "I'm not sure if this decay is available in PyTorch, but you can easily create something similar with torch.optim.lr_scheduler.LambdaLR.\n",
              "\n",
              "decay = .001\n",
              "fcn = lambda step: 1./(1. + decay*step)\n",
              "scheduler = LambdaLR(optimizer, lr_lambda=fcn)\n",
              "Finally, don't forget that you will need to call .step() explicitly on the scheduler, it's not enough to step your optimizer. Also, most often learning scheduling is only done after a full epoch, not after every single batch, but I see that here you are just recreating Keras behavior.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row32_col3\" class=\"data row32 col3\" ><P> I'm wondering why you don't have loss.backward() after the line that you compute the loss (i.e., loss = criterion(outputs,target)) in your training snippet. This will help backpropagating and ultimately updating the parameters of your network upon optimizer.step(). Also, try using lower learning rates as lr=1 normally is too much in training such networks. Try using learning rates in between 0.001-0.01 to see if your network is learning the mapping between input X and target Y.\n",
              " <P> I replicated your model with data augmentation and tried to plot the Accuracy and Loss and it seems that the problem is the way you are plotting.\n",
              "In the following lines I attach my code and Loss and Accuracy plots:\n",
              "Code:\n",
              "# -- Imports -- #\n",
              "import torch\n",
              "\n",
              "from torch import nn, optim\n",
              "from torch.utils.data import DataLoader\n",
              "from torchvision import transforms, datasets\n",
              "import torch.nn.functional as F\n",
              "import matplotlib.pyplot as plt\n",
              "\n",
              "# -- Data Loader -- #\n",
              "my_transforms = transforms.Compose([\n",
              "    transforms.ToPILImage(),\n",
              "    transforms.RandomCrop((25,25)),\n",
              "    transforms.Resize((28,28)),\n",
              "    transforms.RandomRotation(degrees=45, fill=255),\n",
              "    transforms.RandomVerticalFlip(p=0.1),\n",
              "    transforms.RandomHorizontalFlip(p=0.5),\n",
              "    transforms.ToTensor()\n",
              "    ])\n",
              "dataset = datasets.MNIST('../data', train=True, download=True,\n",
              "                         transform = transforms.Compose([\n",
              "                                            transforms.RandomCrop((25,25)),\n",
              "                                            transforms.Resize((28,28)),\n",
              "                                            transforms.RandomRotation(degrees=45, fill=255),\n",
              "                                            transforms.RandomVerticalFlip(p=0.1),\n",
              "                                            transforms.RandomHorizontalFlip(p=0.5),\n",
              "                                            transforms.ToTensor()\n",
              "                                            ]))\n",
              "train_loader = DataLoader(dataset = dataset, batch_size = 1000, shuffle=True)\n",
              "\n",
              "\n",
              "# -- Define Model -- #\n",
              "class Net(nn.Module):\n",
              "    def __init__(self):\n",
              "        super().__init__()\n",
              "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
              "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
              "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
              "        self.fc1 = nn.Linear(20*4*4, 64)\n",
              "        self.fc2 = nn.Linear(64, 10)\n",
              "\n",
              "    def forward(self,x):\n",
              "            x = self.pool(F.relu(self.conv1(x)))\n",
              "            x = self.pool(F.relu(self.conv2(x)))\n",
              "            x = x.view(-1,20*4*4)\n",
              "            x = F.relu(self.fc1(x))\n",
              "            x = F.softmax(self.fc2(x), dim=1)\n",
              "\n",
              "            return x\n",
              "\n",
              "\n",
              "device = 'cuda'\n",
              "net = Net()\n",
              "net.to(device)\n",
              "\n",
              "loss_function=nn.NLLLoss()\n",
              "optimizer=optim.Adam(net.parameters())\n",
              "\n",
              "EPOCHS=2\n",
              "iteracion = 0\n",
              "accuracy = []\n",
              "loss_record = []\n",
              "for epoch in range(EPOCHS):\n",
              "    for data in train_loader:\n",
              "        inputs, labels = data\n",
              "        inputs = inputs.view(-1,1,28,28)\n",
              "        inputs, labels = inputs.to(device), labels.to(device)\n",
              "\n",
              "        # -- Forward -- #\n",
              "        net.zero_grad()\n",
              "        probabilities=net(inputs)\n",
              "        matches=[torch.argmax(i)==int(j) for i,j in zip(probabilities,labels)]\n",
              "        in_batch_acc=matches.count(True)/len(matches)\n",
              "        loss=loss_function(torch.log(probabilities), labels)\n",
              "\n",
              "        # -- Statistics -- #\n",
              "        accuracy.append(in_batch_acc)\n",
              "        loss_record.append(loss)\n",
              "        print('Loss:', round(float(loss), 3))\n",
              "        print('In-batch acc:', round(in_batch_acc, 2))\n",
              "\n",
              "        iteracion += 1\n",
              "\n",
              "        loss.backward()\n",
              "        optimizer.step()\n",
              "\n",
              "# -- Accuracy plot -- #\n",
              "iterations = range(0,120)\n",
              "plt.plot(iterations, accuracy, 'g', label='Accuracy')\n",
              "plt.title('Accuracy')\n",
              "plt.xlabel('Iterations')\n",
              "plt.ylabel('Accuracy')\n",
              "plt.legend()\n",
              "plt.show()\n",
              "\n",
              "# -- Loss plot -- #\n",
              "plt.plot(iterations, loss_record, label='Loss')\n",
              "plt.title('Loss')\n",
              "plt.xlabel('Iterations')\n",
              "plt.ylabel('Loss')\n",
              "plt.legend()\n",
              "plt.show()\n",
              "\n",
              "Plots:\n",
              "Accuracy plot\n",
              "Loss plot\n",
              "As you can see, there are not jumps when (iteration = 61) - Next epoch\n",
              " <P> In my laptop it worked ...\n",
              "since you are running it on just 10 epochs ...and using lr = 0.0001 ,you wont see it in just 10 epochs.\n",
              "i did this optimizer = torch.optim.SGD(our_model.parameters(), lr = 0.01) (increased lr )which actually decreased the loss in just 10 epochs\n",
              " <P> Use optimizer.step() before scheduler.step(). Also, for OneCycleLR, you need to run scheduler.step() after every step - source (PyTorch docs). So, your training code is correct (as far as calling step() on optimizer and schedulers is concerned).\n",
              "\n",
              "Also, in the example you mentioned, they have passed steps_per_epoch parameter, but you haven't done so in your training code. This is also mentioned in the docs. This might be causing the issue in your code.\n",
              " <P> You are optimizing many times (100 steps) on the first batch (first samples), then moving to the next samples. It means that your model will overfit your few samples before going to the next batch. Then, your training will be very non smooth, diverge and go far from your global optimum.\n",
              "\n",
              "Usually, in a training loop you should:\n",
              "\n",
              "\n",
              "go over all samples (this is one epoch)\n",
              "shuffle your dataset in order to visit your samples in a different order (set your pytorch training loader accordingly)\n",
              "go back to 1. until you reach the max number of epochs\n",
              "\n",
              "\n",
              "Also you should not define your optimizer each time (nor your criterion).\n",
              "\n",
              "Your training loop should look like this:\n",
              "\n",
              "criterion = nn.BCELoss()\n",
              "optim = torch.optim.SGD(model.parameters(), lr=0.001)\n",
              "n_epochs = 100\n",
              "\n",
              "def train_model():\n",
              "    for X, y in train_loader:\n",
              "        optim.zero_grad()\n",
              "        y_pred = model.forward(X)\n",
              "        loss = criterion(y_pred, y)\n",
              "        loss.backward()\n",
              "        optim.step()\n",
              "\n",
              "for epoch in range(n_epochs):\n",
              "    train_model()\n",
              "\n",
              " <P> have you tried adding momentum to your SGD optimizer?\n",
              "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0016, momentum=0.9)\n",
              "\n",
              "Or, a different optimizer such as Adam or AdaDelta, which will use adaptive learning rate?\n",
              "Also, it does not look like your training data is shuffled - can it happen than some batches have all cats and some batches have all dogs, pulling the gradient descent in different in opposite directions every few steps? It may be better to shuffle your training data after every epoch and do the batching on top of that. torch.utils.data.DataLoader class may be of some help in this.\n",
              "What's the naming scheme of your files? Do the variables 'isCat' and 'isDog' have correct values?\n",
              "What happens when you try to train on only 100 examples - is your model able to learn the train data in this simple case - this should hopefully rule out some obvious bugs.\n",
              " <P> So the issue is you're only training the first part of the classifier and not the second\n",
              "# this\n",
              "optimizer = torch.optim.Adam(RONANetv1.parameters(), lr=0.1)\n",
              "# needs to become this\n",
              "from itertools import chain\n",
              "optimizer = torch.optim.Adam(chain(RONANetv1.parameters(), RONANetv2.parameters()))\n",
              "\n",
              "and you need to incorportate the other cnn in training too\n",
              "intermediate_out = RONANetv1(images)\n",
              "out = RONANetv2(intermediate_out)\n",
              "loss = criterion(out, labels)\n",
              "batch_loss += loss.item()\n",
              "loss.backward()\n",
              "optimizer.step()\n",
              "\n",
              "Hope that helps best of luck!\n",
              " <P> considering that your training and dev loss are decreasing over time, it seems like your model is training correctly. With respect to your worry regarding your training and dev loss values, this is entirely dependent on the scale of your target values (how big are your target values?) and the metric used to compute the training and dev losses. If your target values are big and you want smaller train and dev loss values, you can normalise the target values.\n",
              "\n",
              "From what I gather with respect to your experiments as well as your R2 scores, it seems that you are looking for a solution in the wrong area. To me, it seems like your features aren't strong enough considering that your R2 scores are low, which could mean that you have a data quality issue. This would also explain why your architecture tuning has not improved your model's performance as it is not your model that is the issue. So if I were you, I would think about what new useful features I could add and see if that helps. In machine learning, the general rule is that models are only as good as the data that they are trained on. I hope this helps! \n",
              " <P> Your loss value 6.9077 is equal to -log(1/1000), which basically means your network produces random outputs out of all possible 1000 classes.\n",
              "It is a bit tricky to train VGG nets from scratch, especially if you do not include batch-norm layers.\n",
              "Try to reduce the learning rate to 0.01, and add momentum to your SGD.\n",
              "Add more input augmentations (e.g., flips color jittering, etc.).\n",
              " <P> The good news here is : \"The total loss values are all almost same,\" that means they are not always the same, and therefore, I think your network  does not output constant probabilities ! I can see many possible reasons why your training does not work as planned. Unfortunately, without debugging myself, I will not be able to say with certainty what happens. So here are my hypothesis :\n",
              "\n",
              "First, the hurtful one : maybe the task is too hard for a neural network. Have you tried classifying them by hand and did you find it easy to do ? There is not easy solution for this except accept that Machine Learning is not a magic wand and cannot solve everything.\n",
              "Maybe your learning rate is too high (or too low) try launching the training for values ranging from 10^-5 to 100 multiplying them by 10 each time. No need to let the training run for too long, just check how much your loss changes from an iteration to another.\n",
              "Maybe your training set is unbalanced : if you have 95% of True inputs and 5% of False ones, then, your network will naturally start by predicting True each time (with logits corresponding to a probability of ~95%). In this case, try to artificially balance it (at least temporarily) : you can do so by duplicating the False examples (ideally not in memory but directly in the code) or by removing some True examples (ideally only in the code also, not in the database).\n",
              "Maybe your architecture is too small (or too big) try adding (or removing) layers. I would start by removing layers since smaller networks tend to learn faster.\n",
              "\n",
              "Although testing all of these hypothesis may help you, I above all encourage you to understand the outputs of your network, print the outputs of the softmax layer : what probability does it output, and can you guess why ? (Sometimes you just can't, but often times, it is possible, like in the 95/5 probability case I talked about earlier in this answer) Check that the loss is what you expect it to be given this output (compute it manually if need be), in general, be curious to find out how does your code behaves, and check that it works as intended everywhere you can interpret your variables.\n",
              "It's one of the hard parts of Machine Learning, sailing through it is not easy ;) good luck !\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col0\" class=\"data row33 col0\" >By default, we decode byte strings as what?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col1\" class=\"data row33 col1\" >inputs to be partially specialized enable_cpatching – Enables C-level patching of functions (captures things like torch.randn) and trace the traceback back to the original tensor (i.e. the tensor you're trying to parse). This is by design, not a bug.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col2\" class=\"data row33 col2\" >utf-8</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row33_col3\" class=\"data row33 col3\" ><P> cc malfet <P> bumping priority based on user activity <P> `.data()` <P> friday <P> concrete_args allows you to partially specialize your function, whether it’s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this. f = fx.symbolic_trace(f, concrete_args={‘b’: False}) assert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn’t be specialized. For example: root (Union[torch.nn.Module, Callable]) – Module or function to be traced and converted into a Graph representation. concrete_args (Optional[Dict[str, any]]) – Inputs to be partially specialized enable_cpatching – Enables C-level patching of functions (captures things like torch.randn) <P> SGTM <P> You are right in the case of summarization and other tasks, it makes sense to build and use the same vocab for input and output\n",
              " <P> arr[[0,1,2,3], [0,2,1,2]]\n",
              "\n",
              "or if you prefer np.arange(4) for the 1st indexing array.\n",
              " <P> np.argpartition(probs,-5)[:,-5:]\n",
              "\n",
              " <P> logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col0\" class=\"data row34 col0\" >grad fn get whole graph in dot</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col1\" class=\"data row34 col1\" >forward[:, 0] = pi * obs_prob * backward[:, 1]\n",
              "\n",
              "\n",
              "This behaviour is expected.\n",
              "\n",
              "There is a detailed discussion in \n",
              "\n",
              ">>> torch.130a1dc1DC1DICTION = torch.autograd.grad()\n",
              ">>> module1 should create valid gradients as long as mask2 isn't full of zeros\n",
              ">>> The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col2\" class=\"data row34 col2\" >use this link    //github.com/szagoruyko/pytorchviz</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row34_col3\" class=\"data row34 col3\" ><P> backward[:, 0] = pi * obs_prob * backward[:, 1]\n",
              "\n",
              " <P> This behaviour is expected.\n",
              "There is a detailed discussion in https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219 <P> I use this:\n",
              "python\n",
              "class LayerNorm(nn.Module):\n",
              " def __init__(self, features, eps=1e-6):\n",
              " super().__init__()\n",
              " self.gamma = nn.Parameter(torch.ones(features))\n",
              " self.beta = nn.Parameter(torch.zeros(features))\n",
              " self.eps = eps\n",
              " def forward(self, x):\n",
              " mean = x.mean(-1, keepdim=True)\n",
              " std = x.std(-1, keepdim=True)\n",
              " return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
              " <P> The in-place operation seems to be on this line:\n",
              "File \"/home/anatole2/best/PCEN_pytorch.py\", line 30, in forward\n",
              "  filtered[i] = filtered[i] + (1-exp(self.log_s)) * filtered[i-1]\n",
              "\n",
              "Note that it is using the value from filtered[i] and then storing the result in filtered[i].  This is what in-place means; the new value overwrites the old one.\n",
              "To fix it, you'd need to do something like this:\n",
              "filtered_new = torch.zeros_like(filtered)\n",
              "...\n",
              "filtered_new[i] = filtered[i] + (1-exp(self.log_s)) * filtered[i-1]\n",
              "\n",
              "The part that makes this a bit complicated is that this seems to be inside a loop (I assume i is the loop counter) and it probably uses the values from the previous pass through the loop.  The modified version is not in-place, but probably won't produce the same results as the original either.  So you may have to do something like this:\n",
              "filtered_new[i] = filtered[i] + (1-exp(self.log_s)) * filtered_new[i-1]\n",
              "\n",
              "It's impossible to solve this without seeing more code around this, but basically - look around, and replace any operation which changes existing tensors with an operation which creates new tensors to store the results of the calculation.\n",
              " <P> Well, returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.\n",
              " \n",
              " So effectively, in your custom function here, you actually return the Tensors \"unpacked\" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either. <P> module1 should create valid gradients as long as mask2 isn't full of zeros <P> The output of function can contain non-Tensor values and gradient recording is only performed for the Tensor values. Note that if the output consists of nested structures (ex: custom objects, lists, dicts etc.) consisting of Tensors, these Tensors nested in custom structures will not be considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. Warning <P> I realized that this bug has been fixed in the latest master code (I am using pytorch 1.5.1) in \n",
              " \n",
              " https://github.com/pytorch/pytorch/blob/890b52e09ff1a758a2ac814f130a1dc1f1a46e9d/torch/nn/modules/activation.py#L869\n",
              " \n",
              " by changing the Linear class to _LinearWithBias class, which won't be passed to dynamic quantization function to pack the out_proj.\n",
              " \n",
              " I add this fix in my code and it works well.\n",
              " \n",
              " Please simply ignore this above error description then\n",
              " \n",
              " Thanks <P> fk me. I wrapped it in with torch.no_grad() in caller function. <P> You might want to use the Sequential class\n",
              "\n",
              "import torch.nn as nn\n",
              "\n",
              "class NNet(nn.Module):\n",
              "    def __init__(self, idim, hdim, odim, depth):\n",
              "        super().__init__()\n",
              "        layers = [nn.Linear(idim, hdim)]\n",
              "        layers += [nn.Linear(hdim, hdim)\n",
              "                   for i in range(depth)]\n",
              "        layers += [nn.Linear(hdim, odim)]\n",
              "        self.net = nn.Sequential(*layers)\n",
              "\n",
              "    def forward(self, x):\n",
              "        return self.net(x)\n",
              "\n",
              "\n",
              "This takes care of the parameters etc too.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col0\" class=\"data row35 col0\" >Exception in find_cuda_windows_lib</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col1\" class=\"data row35 col1\" >Enable the environment variable `export TORCH_CUDA_ARCH_LIST=7.0`, and this should be fixed. Basically, what's happening here is that you are building in a docker container that has CUDA, but doesn't have access to a GPU. So PyTorch is defaulting to building for all CUDA architectures, and I suspect it's including some old architectures. If you paste the full build log, we can tell what it's attempting to do.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col2\" class=\"data row35 col2\" >These functions are removed</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row35_col3\" class=\"data row35 col3\" ><P> @HuangJunJie2017 You may have to specify `include_dirs` of the `Extension` object, making it point to `%CUDA_PATH%/include`. <P> cc @janeyx99 <P> enable the environment variable `export TORCH_CUDA_ARCH_LIST=7.0`, and this should be fixed.\n",
              " \n",
              " \n",
              " \n",
              " Basically, what's happening here is that you are building in a docker container that has CUDA, but doesn't have access to a GPU. So PyTorch is defaulting to building for all CUDA architectures, and I suspect it's including some old architectures. If you paste the full build log, we can tell what it's attempting to do. <P> You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the C++ headers only in several months. <P> Yeah, I found that removing the build and torch/lib/build folders and rebuilding fixed it. I do not know why the normal install procedure doesn't pick up the change. <P> Turns out the GPU build in was cached by setuptools (build/, dist/, etc.) and also installed!\n",
              " <P> > Oh so basically cmake_cache_vars['USE_SYSTEM_NCCL'] is by default pointing to /usr/local/cuda for UNIX?\n",
              "\n",
              "Given the above code, it will look for system NCCL using env vars `NCCL_LIBRARIES` and `NCCL_INCLUDE_DIRS` <P> Try `export TORCH_CUDA_ARCH_LIST=\"8.6\"`. <P> @skyline75489 Yes, I guess TORCH_CUDA_SPLIT is only enabled on CUDA 11.1. <P> Closing this issue. I found what was wrong and **I fixed it**. The whole problem lies in the fact that Anaconda distribution comes with its own `ld` linker that is located in `/opt/anaconda/compiler_compat/` and it overshadows system `ld` residing at `/usr/bin`.\n",
              " \n",
              " \n",
              " \n",
              " You can see that the build command (which caused my error) uses Anaconda's `ld` instead of system `ld` as it specifies `-B` option which points to Anaconda's folder:\n",
              " \n",
              " bash\n",
              " \n",
              " g++ -pthread -shared -B /opt/anaconda/compiler_compat -L/opt/anaconda/lib -Wl,-rpath=/opt/anaconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/torch/csrc/stub.o -L/home/manjaro/Downloads/pytorch/torch/lib -lshm -ltorch_python -o build/lib.linux-x86_64-3.7/torch/_C.cpython-37m-x86_64-linux-gnu.so -Wl,-rpath,$ORIGIN/lib\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " To fix my error I ran `python setup.py clean` and then I temporarily renamed Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation. Removing `-B` option from the installation stage could fix it too. \n",
              " \n",
              " \n",
              " \n",
              " Probably, there is some incompatibility between my `ld` (version `GNU ld (GNU Binutils) 2.31.1\n",
              " \n",
              " `) and Anaconda's `ld` (version `GNU ld (crosstool-NG 1.23.0.444-4ea7) 2.31.1\n",
              " \n",
              " `) that caused the error. Or another explanation is that my system `g++` compiled `.o` object file that is incompatible with Anaconda's `ld` in the subsequent step of linking.\n",
              " \n",
              " \n",
              " \n",
              " What about changing `setup.py` to not to allow Anaconda's `ld` overshadow system `ld`?\n",
              " \n",
              " \n",
              " \n",
              " Thank you.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col0\" class=\"data row36 col0\" >Torch installed but repeat_interleave not found</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col1\" class=\"data row36 col1\" >Actually torch dtype object is already serializable. Closing....\n",
              "\n",
              "In [6]: b = copy.deepcopy(a) In [7]: id(b) Out[7]: 139818768678472\n",
              "Out[8]: 0. days for days not found not found.\n",
              "Then import torch followed by switching from inline to external plotting with %matplotlib auto.\n",
              "Note that this does not happen any more with\n",
              "ipython 7.2.0 and matplotlib 3.0.2</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col2\" class=\"data row36 col2\" >The torch.repeat_interleave operator was introduced in 1.1.0 release of pytorch, so please, consider updating to 1.1.0+ version of pytorch to use this method smoothly.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row36_col3\" class=\"data row36 col3\" ><P> Actually torch dtype object is already serializable. Closing....\n",
              "\n",
              "In [6]: b = copy.deepcopy(a)\n",
              "\n",
              "In [7]: id(b)\n",
              "Out[7]: 139818768678472\n",
              "\n",
              "In [8]: id(a)\n",
              "Out[8]: 139818768678472\n",
              "\n",
              "In [9]: import pickle\n",
              "\n",
              "In [10]: with open('/tmp/a', 'wb') as f:\n",
              "    ...:     pickle.dump(torch.float32, f)\n",
              "    ...:\n",
              "\n",
              " <P> Pytorch 1.5 using Tensor::index and Tensor::index_put_\n",
              "\n",
              "using namespace torch::indexing;\n",
              "\n",
              "auto myints = torch::Tensor({10, 20, 30, 40, 50, 60, 70});\n",
              "auto myvector = torch::ones({18});    \n",
              "myvector.index_put_({3, 7}, myints.index({0, 3}));  \n",
              "\n",
              "\n",
              "General translation for Tensor::index and Tensor::index_put_\n",
              "\n",
              "Python             C++ (assuming `using namespace torch::indexing`)\n",
              "-------------------------------------------------------------------\n",
              "0                  0\n",
              "None               None\n",
              "...                \"...\" or Ellipsis\n",
              ":                  Slice()\n",
              "start:stop:step    Slice(start, stop, step)\n",
              "True / False       true / false\n",
              "[[1, 2]]           torch::tensor({{1, 2}})\n",
              "\n",
              "\n",
              "Pytorch 1.4 alternative functions\n",
              "\n",
              "Tensor Tensor::narrow(int64_t dim, int64_t start, int64_t length) \n",
              "\n",
              "Tensor  Tensor::copy_(const Tensor  src, bool non_blocking=false)\n",
              "\n",
              "\n",
              "narrow is almost exactly like slice and using copy_ for assignment\n",
              "\n",
              "auto myints = torch::Tensor({10, 20, 30, 40, 50, 60, 70});\n",
              "auto myvector = torch::ones({18});\n",
              "\n",
              "myvector.narrow(0, 3, 4).copy_(myvector.narrow(0, 0, 3));\n",
              "\n",
              " <P> I have ipython 7.0.1 and matplotlib 2.0.2 and the same problem, it seems like ipython crashes after the following two commands: %matplotlib auto followed by import torch.\n",
              "\n",
              "This happens both in spyder as in jupyter notebook when the two commands are in seperate blocks.\n",
              "\n",
              "What worked for me was:\n",
              "First making sure that spyders backend graphics is set to inline: \n",
              "Tools -> Preferences -> IPython console -> Graphics backed to Inline.\n",
              "Then import torch followed by switching from inline to external plotting with %matplotlib auto.\n",
              "\n",
              "Note that this does not happen any more with\n",
              "ipython 7.2.0 and matplotlib 3.0.2\n",
              " <P> Ok, I solved the issue, I don't know if it has any unwanted  impact in any way but now Torch is working.\n",
              "Please, note I am using Mac Pycharm with a virtual environment so...\n",
              "I copied: libc++.1.dylib from usr/lib\n",
              "and I pace it in my environment: venv/lib/python3.7/site-packages/torch/lib\n",
              "\n",
              "I tested Torch with this script:\n",
              "\n",
              "from __future__ import print_function\n",
              "import torch\n",
              "x = torch.rand(5, 3)\n",
              "print(x)\n",
              "\n",
              "It devolved:\n",
              "\n",
              "tensor([[0.3633, 0.7173, 0.6055],\n",
              "        [0.3442, 0.6892, 0.2950],\n",
              "        [0.3909, 0.4718, 0.0202],\n",
              "        [0.1055, 0.8912, 0.0667],\n",
              "        [0.3819, 0.4413, 0.1544]])\n",
              "\n",
              "\n",
              "I hope it work for others. I have been 3 days trying to find solutions in Google but not any solution found it.\n",
              "\n",
              " <P> Reproduced locally.\n",
              " \n",
              " cmd\n",
              " \n",
              " C:\\Users\\peter>pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
              " \n",
              " Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
              " \n",
              " Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
              " \n",
              " Collecting torch===1.5.0\n",
              " \n",
              "  Downloading https://download.pytorch.org/whl/cu102/torch-1.5.0-cp37-cp37m-win_amd64.whl (899.1MB)\n",
              " \n",
              "  | | 61kB 75kB/s eta 3:18:16\n",
              " \n",
              " ERROR: Operation cancelled by user\n",
              " \n",
              " WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
              " \n",
              " You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
              " \n",
              " \n",
              " \n",
              " C:\\Users\\peter>pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
              " \n",
              " Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
              " \n",
              " Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
              " \n",
              " Collecting torch==1.5.0\n",
              " \n",
              "  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-win_amd64.whl (693.1MB)\n",
              " \n",
              "  | | 204kB 211kB/s eta 0:54:42\n",
              " \n",
              " ERROR: Operation cancelled by user\n",
              " \n",
              " WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
              " \n",
              " You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
              " \n",
              " \n",
              " \n",
              " Would you please fix the install commands? cc @seemethere @soumith <P> @tshadley \n",
              "> \n",
              "> import torch\n",
              "> t_b = t[...,(None,)*3]\n",
              "> print(t_b.shape)\n",
              "> \n",
              "To unsqueeze at the end, you could try to use `t_b = t[(..., ) + (None, ) * 3]` (works for me on version 1.1.0). <P> Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself.  <P> Hello. I experiencing this issue only on one of my window systems. I think my problem might be visual studio related but no Idea what else to do to fix it. I first tried installing the VC Redist package. Then, I installed the community edition for the Visual Studio 2017. However, I am still getting:\n",
              " \n",
              " \n",
              " \n",
              " C:\\Users\\Mauricio>where api-ms-win-crt-utility-l1-1-0.dll\n",
              " \n",
              " INFO: Could not find files for the given pattern(s).\n",
              " \n",
              " \n",
              " \n",
              " When I try the dependency walker on the \"_C.cp35-win_amd64.pyd\", it marks that all references are correct and the \"api-ms-win-crt-utility-l1-1-0.dll\" is being mapped to \"C:\\Windows\\system32\\ucrtbase.dll\", but on the python console I am still getting the following error:\n",
              " \n",
              " \n",
              " \n",
              "  from torch._C import *\n",
              " \n",
              " ImportError: DLL load failed: The specified procedure could not be found.\n",
              " \n",
              " \n",
              " \n",
              " I even moved from the CUDA 9.2 to cpu-only to reduce the dependencies, but still no luck. Any suggestions? <P> I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\n",
              "\n",
              "\n",
              "  template\n",
              "  static Vec256 arange_index_offset(double base = 0., step_t step = static_cast(1), int64_t index_offset = 0) {\n",
              "    return Vec256(\n",
              "        base + step * index_offset,\n",
              "        base + step * (index_offset + 1),\n",
              "        base + step * (index_offset + 2),\n",
              "        base + step * (index_offset + 3));\n",
              "  }\n",
              "\n",
              "\n",
              "to this:\n",
              "\n",
              "  template\n",
              "  static Vec256 arange_index_offset(double base = 0., step_t step = static_cast(1), int64_t index_offset = 0) {\n",
              "    Vec256 base_v(base, base, base, base);\n",
              "    Vec256 step_v(step, step, step, step);\n",
              "    Vec256 index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n",
              "    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\n",
              "  }\n",
              "\n",
              "\n",
              "Evidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\n",
              "\n",
              "\n",
              "  template\n",
              "  static Vec256 arange_index_offset(double base = 0., step_t step = static_cast(1), int64_t index_offset = 0) {\n",
              "    Vec256 index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n",
              "    Vec256 step_v(step, step, step, step);\n",
              "    Vec256 tmp = _mm256_mul_pd(step_v, index_v);\n",
              "    return Vec256(\n",
              "      base + tmp.values[0],\n",
              "      base + tmp.values[1],\n",
              "      base + tmp.values[2],\n",
              "      base + tmp.values[3]);\n",
              "  }\n",
              "\n",
              "\n",
              "This worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\n",
              "\n",
              "\n",
              "$ ATEN_CPU_CAPABILITY=avx2 python\n",
              ">>> import torch\n",
              ">>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\n",
              "tensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\n",
              ">>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\n",
              " -5.0\n",
              " -3.6\n",
              " -2.2\n",
              " -0.8000000000000007\n",
              " 0.5999999999999996\n",
              " 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\n",
              " 3.3999999999999986\n",
              " 4.799999999999999\n",
              "[torch.DoubleStorage of size 8]\n",
              "\n",
              "\n",
              "This result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\n",
              "\n",
              "I'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this? <P> This will get fixed when we move the Generic include inline into module.py... assuming I can fix the JIT bugs in this PR: https://github.com/pytorch/pytorch/pull/38211</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col0\" class=\"data row37 col0\" >Errow Showing :&#39;ResNet&#39; object has no attribute &#39;classifier&#39;</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col1\" class=\"data row37 col1\" >You can refer to this article to understand how to save the classifier. To make a tweaks to a model, what you can do is create a new model which is a child of the existing model.\n",
              "\n",
              "class NewModel( oldModelClass):\n",
              "    def __init__(self):\n",
              " super(NewModel, self).__init__()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col2\" class=\"data row37 col2\" >Assuming you want to train only the classifier, you can freeze the parameters you don't want to change. \n",
              "For your case, you can do\n",
              "\n",
              "for name, param in model.named_parameters() :\n",
              "    param.requires_grad = False\n",
              "    if name.startswith('classifier') : \n",
              "        param.requires_grad = True\n",
              "\n",
              "\n",
              "This will freeze all the parameters except of the classifier.\n",
              "\n",
              "And then you can  pass all parameters to the optimizer.\n",
              "\n",
              "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row37_col3\" class=\"data row37 col3\" ><P> You can refer to this article to understand how to save the classifier. To make a tweaks to a model, what you can do is create a new model which is a child of the existing model.\n",
              "\n",
              "class newModel( oldModelClass):\n",
              "    def __init__(self):\n",
              "        super(newModel, self).__init__()\n",
              "\n",
              "\n",
              "With this setup, newModel has all the layers as well as the forward function of oldModelClass. If you need to make tweaks, you can define new layers in the __init__ function and then write a new forward function to define it.\n",
              " <P> nn.Module don't have a predict function, just call the object for inference:\n",
              "prediction = model(img_reshape)\n",
              "\n",
              "This will call the object's __call__ function which, in turns, callsthe model forward function.\n",
              " <P> EfficentNet class doesn't have attribute classifier, you need to change in_features=model.classifier.in_features to in_features=model._fc.in_features.\n",
              "import torchvision.models as models\n",
              "\n",
              "NUM_CLASSES = 4\n",
              "\n",
              "#EfficientNet\n",
              "from efficientnet_pytorch import EfficientNet\n",
              "efficientnet = EfficientNet.from_pretrained('efficientnet-b3')\n",
              "efficientnet ._fc= torch.nn.Linear(in_features=efficientnet._fc.in_features, out_features=NUM_CLASSES, bias=True)\n",
              "\n",
              "#mobilenet_v2\n",
              "mobilenet = models.mobilenet_v2(pretrained=True)\n",
              "mobilenet.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=False),\n",
              "                  nn.Linear(in_features=mobilenet.classifier[1].in_features, out_features=NUM_CLASSES, bias=True))\n",
              "\n",
              "#inception_v3\n",
              "inception = models.inception_v3(pretrained=True)\n",
              "inception.fc =  nn.Linear(in_features=inception.fc.in_features, out_features=NUM_CLASSES, bias=True)\n",
              "\n",
              "\n",
              "\n",
              " <P> You are trying to embed the inputs, which are given as ints (torch.int). Only integers (torch.long) can be embedded, since they need to be indices, which cannot be float.\n",
              "\n",
              "inputs need to be converted to torch.long:\n",
              "\n",
              "inputs = inputs.to(torch.long)\n",
              "\n",
              "\n",
              "It seems like you removed the conversion to long, because in the notebook it's done within the model:\n",
              "\n",
              "# embeddings and lstm_out\n",
              "x = x.long()\n",
              "embeds = self.embedding(x)\n",
              "\n",
              "\n",
              "Whereas, in your stack trace, the line x = x.long() (same as using .to(torch.long)) is missing.\n",
              " <P> Your GCN is composed of two Graphconvlayer.\n",
              "As defined in the code you posted, Graphconvlayer's forward method expects only one input argument: inputfeaturedata. However, when GCN calls self.gcn1 or self.gcn2 (in its forward method) it passes 3 arguments: self.gcn1(adj,x,64) and self.gcn2(adj,x,7).\n",
              "Hence, instead of a single input argument, self.gcn1 and self.gcn2 are receiving 3 -- this is the error you are getting.\n",
              " <P> Plain Python containers, such as list and dict won't be properly registered inside your module, so use nn.ModuleDict in that case (or nn.ModuleList instead of list). <P> This problem is due to name mangling — where the interpreter changes the name of the variable in below way which makes it harder to create collisions when the class is extended later. where\n",
              "\n",
              "self.__private\n",
              "\n",
              "\n",
              "has been changed to (self._className__privateMethodName)\n",
              "\n",
              "self._Test__private\n",
              "\n",
              "\n",
              "As name mangling is not applied with dunder, where a name has to start and end with double underscores.\n",
              "\n",
              "So, to avoid name mangling add two more underscores at the end.\n",
              "\n",
              "Below snippet should solve your problem.\n",
              "\n",
              "import torch\n",
              "\n",
              "class Test:\n",
              "    def __init__(self):\n",
              "        self.a = min\n",
              "        self.b = max\n",
              "        self.c = self.__private__\n",
              "\n",
              "    def __private__(self):\n",
              "        return None\n",
              "\n",
              "test = Test()\n",
              "\n",
              "torch.save({\"test\": test}, \"file.pkl\")\n",
              "torch.load(\"file.pkl\")\n",
              "\n",
              " <P> There is a typo in the for loop. It should be labels instead of classes:\n",
              "# ...\n",
              "for inputs, labels in dataloaders[phase]:\n",
              "    inputs = inputs.to(CFG.device)\n",
              "    labels = labels.to(CFG.device)\n",
              "    # ...\n",
              "\n",
              " <P> You never assigned the models to any attribute of the object of the BertResNet class. There are in temporary variables in the __init__ method, but once that finishes, these variables are discarded. They should be assigned to self:\n",
              "\n",
              "def __init__(self):\n",
              "    super(BertResNet, self).__init__()\n",
              "    # resnet\n",
              "    self.resnet50 = models.resnet50(pretrained=True)\n",
              "    n_inputs = self.resnet50.fc.in_features\n",
              "    # compressed embedding space\n",
              "    self.classifier = nn.Sequential(OrderedDict([\n",
              "        ('fc1', nn.Linear(n_inputs, 512))\n",
              "    ]))\n",
              "\n",
              "    self.resnet50.fc = classifier # 512 out resnet \n",
              "\n",
              "\n",
              "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
              "\n",
              "    # final classification layer\n",
              "\n",
              "    self.classification = nn.Linear(512 + 768, 2)\n",
              "\n",
              " <P> This is because only the image inputs should be passed into the models, instead of both images and the ground truth targets. So instead of doing output = model(images, targets), you can do output = model(images).\n",
              "\n",
              "As for why the error message talks about being given 3 positional arguments, this is because forward is initiated with a default self keyword, which represents the class instance. So in addition to self, you should only give 1 more argument, which would be the input image.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col0\" class=\"data row38 col0\" >Warnings during compiling: floating-point value does not fit in required integral type</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col1\" class=\"data row38 col1\" >It's a duplicate of #33760, also I noticed it was only happening with certain flags on (i.e. in the logs above I had changed the `CFLAGS` to include `-GSPLIT-dwarf`.)\n",
              "Hmm, replacing `==` with `torch.allclose` fixes the issue (and difference if default codepath between computed distribution and the actual value is 1e-16 )</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col2\" class=\"data row38 col2\" >compiler-warnings arise during compilation of the templated function uniform_int()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row38_col3\" class=\"data row38 col3\" ><P> Thanks for the report, @yuandong-tian and the confirmation @reynoldscem. I think I found the bug. Fix incoming. <P> I agree that the type should be updated. `lr_lambda` returns a float though. Did you mean the following?\n",
              " \n",
              " \n",
              " \n",
              " python\n",
              " \n",
              " LRLambdaType = Callable[[int], float]\n",
              " \n",
              " \n",
              " \n",
              " class LambdaLR(_LRScheduler):\n",
              " \n",
              "  def __init__(self, optimizer: Optimizer, lr_lambda: Union[LRLambdaType, List[float]], last_epoch: int=...) -> None: ...\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " Please feel free to open a PR for this. <P> Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know.  <P> > hoe to slove it ?\n",
              "Just don't pass the same tensor 3 times as the same var. See the next my comment.\n",
              "\n",
              " <P> cc @wanchaol \n",
              "@SsnL the nan is a temporary stopgap. I believe the end state is indeed to support `at::optional` or something similar. <P> It's a duplicate of #33760, also I noticed it was only happening with certain flags on (i.e. in the logs above I had changed the `CFLAGS` to include `-gsplit-dwarf`.) <P> Hmm, replacing `==` with `torch.allclose` fixes the issue (and difference if default codepath between computed distribution and the actual value is 1e-16 ) <P> Ah, yes. This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`, and it comes back into `__torch_function__` unconditionally. This should be solved by #55093. <P> Thank you for reporting this issue, @shmsong. The team agrees it's a high priority. <P> Thanks to @iacolippo the issue is solved. Turns out the problem was due to  Pytorch 1.0.0. No problem with Pytorch 0.4.1. though.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col0\" class=\"data row39 col0\" >How can I assign a list to a torch.tensor?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col1\" class=\"data row39 col1\" >You can directly convert python list to a pytorch Tensor by defining the dtype. For example,\n",
              "import torch\n",
              "\n",
              "a_list = [3,23,53,32,53] \n",
              "A_Tensor = torch.Tensor(A_LIST)\n",
              "print(a_tensor.int())\n",
              "Here is the documentation:\n",
              "\n",
              "You can use x.item() to get a Python number from a tensor that has one element.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col2\" class=\"data row39 col2\" >use torch.Tensor(list).</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row39_col3\" class=\"data row39 col3\" ><P> You can directly convert python list to a pytorch Tensor by defining the dtype. For example,\n",
              "import torch\n",
              "\n",
              "a_list = [3,23,53,32,53] \n",
              "a_tensor = torch.Tensor(a_list)\n",
              "print(a_tensor.int())\n",
              "\n",
              " tensor([3,23,53,32,53])\n",
              "\n",
              " <P> You can use x.item() to get a Python number from a tensor that has one element.\n",
              " <P> You can use torch.stack:\n",
              "\n",
              "torch.stack(li, dim=0)\n",
              "\n",
              "\n",
              "after the for loop will give you a torch.Tensor of that size.\n",
              "\n",
              "Note that if you know in advance the size of the final tensor, you can allocate an empty tensor beforehand and fill it in the for loop:\n",
              "\n",
              "x = torch.empty(size=(len(items), 768))\n",
              "for i in range(len(items)):\n",
              "    x[i] = calc_result\n",
              "\n",
              "\n",
              "This is usually faster than doing the stack.\n",
              " <P> One way of doing this is:\n",
              "\n",
              "b = torch.IntTensor(list(zip(range(0, list(a.size())[0], 1), a.numpy())))\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [3, 1],\n",
              "        [4, 2]], dtype=torch.int32)\n",
              "\n",
              "\n",
              "Alternatively, you can also use torch.cat() as below:\n",
              "\n",
              "a = torch.tensor([1,2,0,1,2])\n",
              "indices = torch.arange(0, list(a.size())[0])\n",
              "res = torch.cat([indices.view(-1, 1), a.view(-1, 1)], 1) \n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 0],\n",
              "        [3, 1],\n",
              "        [4, 2]])\n",
              "\n",
              " <P> You could convert it to a list and then check via in operator:\n",
              "p = torch.arange(1, 5).reshape((2,2))\n",
              "k = torch.tensor([1,2])\n",
              "v = torch.tensor([1,3])\n",
              "\n",
              " k.tolist() in p.tolist()\n",
              "\n",
              "True\n",
              "\n",
              " v.tolist() in p.tolist()\n",
              "\n",
              "False\n",
              "\n",
              "Or if you want to do it via torch only\n",
              "torch.count_nonzero(k==p, 1) == len(k)\n",
              "\n",
              "\n",
              " <P> You should avoid using python built-ins (list) for your variable names.\n",
              "you can sort like following:\n",
              "list_tensors = [torch.tensor([1,2]), torch.tensor([3, 4, 5])]\n",
              "\n",
              "print(sorted(list_tensors, key=lambda x: x.size()[0]))\n",
              "\n",
              "which will output :\n",
              "[tensor([1, 2]), tensor([3, 4, 5])]\n",
              "Or in descending order :\n",
              "list_tensors = [torch.tensor([1,2]), torch.tensor([3, 4, 5])]\n",
              "\n",
              "print(sorted(list_tensors, key=lambda x: x.size()[0], reverse=True))\n",
              "\n",
              "output :\n",
              "[tensor([3, 4, 5]), tensor([1, 2])]\n",
              " <P> You can instantiate each tensor using pytorch inline or append to a list in a loop.\n",
              "Inline:\n",
              "mylist = [torch.rand(2), torch.rand(5), torch.rand(1)]\n",
              "\n",
              "In a loop:\n",
              "mylist = [torch.rand(i) for i in range(1, 5)]\n",
              "\n",
              "To create a custom tensor, use torch.tensor([[1., -1.], [1., -1.]]) for example.\n",
              "https://pytorch.org/docs/stable/tensors.html\n",
              " <P> Updated answer\n",
              "Here is a way to do this :\n",
              "(A == B[..., None]).any(axis=1).astype(bool)\n",
              "#  array([[False,  True],\n",
              "#          [ True, False],\n",
              "#          [False, False]])\n",
              "\n",
              "Previous answer\n",
              "You could also do it inside a list comprehension:\n",
              "[np.isin(a, b) for a,b in zip(A, B)]\n",
              "#  [array([False,  True]), array([ True, False]), array([False, False])]\n",
              "np.array([np.isin(a, b) for a,b in zip(A, B)])\n",
              "#  array([[False,  True],\n",
              "#          [ True, False],\n",
              "#          [False, False]])\n",
              "\n",
              "But, as @alex said it defeats the purpose of numpy.\n",
              " <P> There is no similar functions at the time of writing this answer. However, a workaround is using torch.from_numpy as in: \n",
              "\n",
              "In[2]: import numpy as np\n",
              "In[3]: a = np.array([[2], [7], [23]], dtype=np.uint8)\n",
              "In[4]: b = np.unpackbits(a, axis=1)\n",
              "In[5]: b\n",
              "Out[5]: \n",
              "array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 1]], dtype=uint8)\n",
              "In[6]: import torch\n",
              "In[7]: torch.from_numpy(b)\n",
              "Out[7]: \n",
              "tensor([[0, 0, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
              "        [0, 0, 0, 1, 0, 1, 1, 1]], dtype=torch.uint8)\n",
              "\n",
              " <P> If all elements are integer you can make integer torch tensor by defining dtype\n",
              "\n",
              " a_list = [1, 9903, 7876, 9971, 2770, 2435, 10441, 9370, 2]\n",
              " tmp2 = torch.tensor(a_list, dtype=torch.int)\n",
              " tmp2\n",
              "tensor([    1,  9903,  7876,  9971,  2770,  2435, 10441,  9370,     2],\n",
              "       dtype=torch.int32)\n",
              "\n",
              "\n",
              "While torch.Tensor returns torch.float32 which made it to print number in scientific notation\n",
              "\n",
              " tmp2 = torch.Tensor(a_list)\n",
              " tmp2\n",
              "tensor([1.0000e+00, 9.9030e+03, 7.8760e+03, 9.9710e+03, 2.7700e+03, 2.4350e+03,\n",
              "        1.0441e+04, 9.3700e+03, 2.0000e+00])\n",
              " tmp2.dtype\n",
              "torch.float32\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col0\" class=\"data row40 col0\" >If it's unset, all error messages will report as originating from what?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col1\" class=\"data row40 col1\" >ONNX ops or if it's unset, all error messages will report as originating from where. This is fixed in 1.7.1, 1.8.2, and master in master:\n",
              "\n",
              "1.1.0\n",
              "2.2.\n",
              "3.3.\n",
              "4.5.\n",
              "5.6.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col2\" class=\"data row40 col2\" >GraphModule</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row40_col3\" class=\"data row40 col3\" ><P> cc malfet <P> bumping priority based on user activity <P> friday <P> Note <P> On it <P> #ERROR! <P> Error in test codes <P> Because your script will be profiled, please ensure that it exits in a finite amount of time. Warning <P> upgrade your numpy <P> HIgh priority for silent wrong behavior and crashes</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col0\" class=\"data row41 col0\" >Pytorch custom dataset: ValueError: some of the strides of a given numpy array are negative</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col1\" class=\"data row41 col1\" >The problem with the last attempt is easy to solve. Change from torch.FloatTensor to torch.LongTensor:\n",
              "y = torch.tensor([test_dataloader.dataset[i][1] for i in [0,1,2]], dtype=torch.long)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col2\" class=\"data row41 col2\" >Thanks to the advice from @jodag and @UsmanAli, I sovled this by return torch.from_numpy(feature.copy()) and torch.tensor(label.astype(np.bool)) So the whole thing should be,\n",
              "\n",
              "class data_from_xlsx(Dataset):\n",
              "    def __init__(self, xlsx_fp, path_col, class_cols_list):\n",
              "        self.xlsx_file = pd.read_excel(xlsx_fp)\n",
              "        self.path_col = path_col\n",
              "        self.class_cols_list = class_cols_list\n",
              "\n",
              "    def __len__(self):\n",
              "        return get_xlsx_length(self.xlsx_file)\n",
              "\n",
              "    def __getitem__(self, index):\n",
              "        file_path = cols_from_xlsx(self.xlsx_file, index, 1, self.path_col) \n",
              "        feature = load_nii_file(file_path) # get 3D volume (x, y, z) \n",
              "        feature = np.expand_dims(feature, axis=0) # add channel (c, x, y, z)\n",
              "        label = cols_from_xlsx(self.xlsx_file, index, 1, self.class_cols_list) # get label\n",
              "        return torch.from_numpy(feature.copy()), torch.tensor(label.astype(np.bool))</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row41_col3\" class=\"data row41 col3\" ><P> The problem with the last attempt is easy to solve. Change from torch.FloatTensor to torch.LongTensor:\n",
              "y = torch.tensor([test_dataloader.dataset[i][1] for i in [0,1,2]], dtype=torch.long)\n",
              "\n",
              " <P> in the def sample_image, you have line that defines target labels for the generator:\n",
              "labels = np.array([num for _ in range(n_row) for num in range(n_row)]).\n",
              "\n",
              "Instead of using num which changes due to being sampled from range, use constant number you pass as an argument (class_id below):\n",
              "\n",
              "def sample_image(n_row, batches_done, class_id):\n",
              "   \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
              "   # Sample noise\n",
              "   z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
              "   # Get labels ranging from 0 to n_classes for n rows\n",
              "   labels = np.array([class_id for _ in range(n_row) for __ in range(n_row)])\n",
              "   labels = Variable(LongTensor(labels))\n",
              "   gen_imgs = generator(z, labels)\n",
              "   save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
              "\n",
              "\n",
              "This way you will get a rectangular array full of images of the class you have requested.\n",
              "\n",
              "Furthermore, to have just one image you can set n_row to 1. Note that you didn't provide code for save_image function, there may be some tricks to it.\n",
              " <P> When using np.loadtxt() method, make sure to add ndims = 2 as a parameter. \n",
              "Because the number of objects parameter num_obj becomes 10 even if it has only 1 object in it. \n",
              "\n",
              "It is because 1 object becomes a column vector which shows up as 10 objects. (representing 10 columns)\n",
              "\n",
              "ndims = 2, makes sure that the output of np.loadtxt() method does not give out any row or column vectors, only 2 dimensional outputs.\n",
              " <P> In case your data type is a tensor then you can use:\n",
              "\n",
              "import torch\n",
              " n_classes = len(torch.unique(Your_Target_Vector))\n",
              " <P> If the batch size can be derived from len(labels):\n",
              "\n",
              "def to_onehot(labels, n_categories, dtype=torch.float32):\n",
              "    batch_size = len(labels)\n",
              "    one_hot_labels = torch.zeros(size=(batch_size, n_categories), dtype=dtype)\n",
              "    for i, label in enumerate(labels):\n",
              "        # Subtract 1 from each LongTensor because your\n",
              "        # indexing starts at 1 and tensor indexing starts at 0\n",
              "        label = torch.LongTensor(label) - 1\n",
              "        one_hot_labels[i] = one_hot_labels[i].scatter_(dim=0, index=label, value=1.)\n",
              "    return one_hot_labels\n",
              "\n",
              "\n",
              "and you have 6 categories and want the output to be a tensor of integers:\n",
              "\n",
              "to_onehot(labels, n_categories=6, dtype=torch.int64)\n",
              "\n",
              "\n",
              "tensor([[1, 1, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 1],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 1, 1, 0],\n",
              "        [0, 0, 0, 1, 0, 0]])\n",
              "\n",
              "\n",
              "I would stick to torch.float32 in case you want to use label smoothing, mix-up or something along those lines later.\n",
              " <P> There are 2 hacks that can be used to sort out the problem, choose one way:\n",
              "\n",
              "By using the original batch sample Fast option:\n",
              "\n",
              "def my_collate(batch):\n",
              "    len_batch = len(batch) # original batch length\n",
              "    batch = list(filter (lambda x:x is not None, batch)) # filter out all the Nones\n",
              "    if len_batch  len(batch): # if there are samples missing just use existing members, doesn't work if you reject every sample in a batch\n",
              "        diff = len_batch - len(batch)\n",
              "        for i in range(diff):\n",
              "            batch = batch + batch[:diff]\n",
              "    return torch.utils.data.dataloader.default_collate(batch)\n",
              "\n",
              "\n",
              "Otherwise just load another sample from dataset at random Better option:\n",
              "\n",
              "def my_collate(batch):\n",
              "    len_batch = len(batch) # original batch length\n",
              "    batch = list(filter (lambda x:x is not None, batch)) # filter out all the Nones\n",
              "    if len_batch  len(batch): # source all the required samples from the original dataset at random\n",
              "        diff = len_batch - len(batch)\n",
              "        for i in range(diff):\n",
              "            batch.append(dataset[np.random.randint(0, len(dataset))])\n",
              "\n",
              "    return torch.utils.data.dataloader.default_collate(batch)\n",
              "\n",
              " <P> That code looks a bit complex... You can try the following:\n",
              "\n",
              "#Let there be 9 samples and 1 sample in class 0 and 1 respectively\n",
              "class_counts = [9.0, 1.0]\n",
              "num_samples = sum(class_counts)\n",
              "labels = [0, 0,..., 0, 1] #corresponding labels of samples\n",
              "\n",
              "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
              "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
              "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
              "\n",
              " <P> What worked for me is to declare empty list and then to fill them with the predictions\n",
              "    # TO GET ONLY THE WRONGLY LABELED ITEMS\n",
              "    wrong_ids.append(ids[~(predicted == targets ).to('cpu')])\n",
              "     \n",
              "    # TO ALSO STORE ALL TESTED LABELS PREDICTED AND TARGETS IN SEPARATE LIST\n",
              "    tested_labels.append(ids.to('cpu'))\n",
              "    pred_labels.append(predicted.to('cpu'))\n",
              "    true_labels.append(targets.to('cpu'))\n",
              "\n",
              "then after that to convert the resulting list of tensors to one list:\n",
              "wrongs =[]\n",
              "for i,j in enumerate(wrong_ids):\n",
              "    for k,l in enumerate(j):\n",
              "        wrongs.append(l.item())\n",
              "\n",
              "# and so on for the other lists of tensors\n",
              "\n",
              "To show all tested labels with the predictions and the true labels:\n",
              "df = pd.DataFrame({'id': ids,\n",
              "                   'predicted': pred_label,\n",
              "                   'true_label': true_label})\n",
              "print(df)\n",
              "\n",
              "\n",
              " <P> Only a tensor that contains a single value can be converted to a scalar with item(), try printing the contents of prediction, I imagine this is a vector of probabilities indicating which label is most likely. Using argmax on prediction will give you your actual predicted label (assuming your labels are 0-n).\n",
              " <P> The reason is our label should be integer scalar in F.nll_loss method read here.\n",
              "Let me give one example, let say you want to do image classification , you cannot give your labels as 0.1 ,0.2 etc, rather it should be 0,1,2 ..\n",
              "And I also see that , your labels are more that your model's output\n",
              "working code should be something like this :\n",
              "import os; os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
              "import torch\n",
              "import torchvision\n",
              "import torch.nn as nn\n",
              "import torch.nn.functional as F\n",
              "import matplotlib.pyplot as plt\n",
              "import time\n",
              "\n",
              "#####################################################\n",
              "#             Create the neural network             #\n",
              "#####################################################\n",
              "\n",
              "class Net(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Net, self).__init__()\n",
              "        self.fc1 = nn.Linear(1, 10)\n",
              "        self.fc2 = nn.Linear(10, 10)\n",
              "        self.fc3 = nn.Linear(10, 2) #  CHANGED TO 2 CLASS\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = F.relu(self.fc1(x))\n",
              "        x = F.relu(self.fc2(x))\n",
              "        x = self.fc3(x)\n",
              "        return x\n",
              "\n",
              "\n",
              "net = Net()\n",
              "\n",
              "#####################################################\n",
              "#                   Create the datasets             #\n",
              "#####################################################\n",
              "\n",
              "\n",
              "trainset = [torch.tensor([1., 0.]), torch.tensor([2., 0.]), torch.tensor([3., 0.]), torch.tensor([4., 0.]), torch.tensor([5., 1.]), torch.tensor([6., 1.]), torch.tensor([7., 1.]), torch.tensor([8., 1.])]\n",
              "\n",
              "testset = [torch.tensor([1.1, 0.]), torch.tensor([2.3, 0.]), torch.tensor([3.1, 0.]), torch.tensor([4.5, 0.]), torch.tensor([5.9, 1.]), torch.tensor([6.1, 1.]), torch.tensor([7.3,1.]), torch.tensor([8.01, 1.])]\n",
              "\n",
              "#####################################################\n",
              "#               Optimize the parameters             #\n",
              "#####################################################\n",
              "\n",
              "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
              "\n",
              "\n",
              "EPOCHS = 3\n",
              "\n",
              "for epoch in range(EPOCHS):\n",
              "    for data in trainset:\n",
              "        x, y = data\n",
              "        net.zero_grad()\n",
              "        #print(x.view(-1,1).shape)\n",
              "        y = torch.tensor([y.type(torch.LongTensor)])\n",
              "        #print(y.shape)\n",
              "        #print(y)\n",
              "        output = net(x.view(-1,1))\n",
              "        loss = F.nll_loss(output, y)\n",
              "        loss.backward()\n",
              "        optimizer.step()\n",
              "    print(loss)\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col0\" class=\"data row42 col0\" >How do I extract only subset of classes from torchvision.datasets.CIFAR10?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col1\" class=\"data row42 col1\" >There is no generic way to do this efficiently, as the Datasets class is only implements a __getitem__ and __len__ method, and don't necessarily have any \"stored\" information about the labels.\n",
              "In the case of the MNIST dataset class however you can sort the dataset from the label list.\n",
              "For example when you want to list the indices that have the label 5.\n",
              "\n",
              "mnist = torchvision.datasets.MNIST(\"/\")\n",
              "labels = MNIST.train_labels\n",
              "fives = (labels == 5).nonzero()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col2\" class=\"data row42 col2\" >By inspecting the code of CIFAR10, you can see that the data is stored as numpy array and the labels are stored as a list. You can therefore subclass this and filter the two arrays adequately. An example is below:\n",
              "\n",
              "class SubLoader(torchvision.datasets.CIFAR10):\n",
              "    def __init__(self, *args, exclude_list=[], **kwargs):\n",
              "        super(SubLoader, self).__init__(*args, **kwargs)\n",
              "\n",
              "        if exclude_list == []:\n",
              "            return\n",
              "\n",
              "        if self.train:\n",
              "            labels = np.array(self.train_labels)\n",
              "            exclude = np.array(exclude_list).reshape(1, -1)\n",
              "            mask = ~(labels.reshape(-1, 1) == exclude).any(axis=1)\n",
              "\n",
              "            self.train_data = self.train_data[mask]\n",
              "            self.train_labels = labels[mask].tolist()\n",
              "        else:\n",
              "            labels = np.array(self.test_labels)\n",
              "            exclude = np.array(exclude_list).reshape(1, -1)\n",
              "            mask = ~(labels.reshape(-1, 1) == exclude).any(axis=1)\n",
              "\n",
              "            self.test_data = self.test_data[mask]\n",
              "            self.test_labels = labels[mask].tolist()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row42_col3\" class=\"data row42 col3\" ><P> \n",
              "I do not know about x-shot learning but if you want to get a subset of your dataset, you just need to change the __len__() function to return maximum size you want.\n",
              "This function return the length of your dataset, so if you put a constant number or a function the current number, you get as much as you wanted.\n",
              " def __len__(self):\n",
              "        return len(self.inputFolderDataset.imgs)/10 # any number will work\n",
              "\n",
              "\n",
              "About your second question, DataLoader class can solve your problem. It ‘shuffle’ argument which can create random batches of your dataset.\n",
              "Here is a snippet you can use.\n",
              "from torch.utils.data import DataLoader\n",
              "# your CustomData is defined here\n",
              "custom_data = CustomData(*args)\n",
              "train_loader = DataLoader(dataset=custom_data,\n",
              "                          batch_size= 16 # batch size,\n",
              "                          shuffle=True,  # this line do the random thing\n",
              "                          num_workers=0)\n",
              "\n",
              "Based on PyTorch, every epoch, it will generate different random numbers so you’ll get different images.\n",
              "Good luck <P> import torchvision, torch\n",
              "from torchvision import datasets, models, transforms\n",
              "\n",
              "def load_training(root_path, dir, batch_size, kwargs):\n",
              "\n",
              "    transform = transforms.Compose(\n",
              "        [transforms.Resize([256, 256]),\n",
              "         transforms.RandomCrop(224),\n",
              "         transforms.RandomHorizontalFlip(),\n",
              "         transforms.ToTensor()])\n",
              "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
              "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
              "\n",
              "    return train_loader\n",
              "\n",
              "\n",
              "I hope it'll work ...\n",
              " <P> There is no generic way to do this efficiently, as the dataset class is only implements a __getitem__ and __len__ method, and don't necessarily have any \"stored\" information about the labels.\n",
              "\n",
              "In the case of the MNIST dataset class however you can sort the dataset from the label list.\n",
              "\n",
              "For example when you want to list the indices that have the label 5.\n",
              "\n",
              "mnist = torchvision.datasets.mnist.MNIST(\"/\")\n",
              "labels = mnist.train_labels\n",
              "fives = (labels == 5).nonzero()\n",
              "\n",
              "\n",
              " <P> PyTorch DataSets can return tuples of values, but they have no inherent \"features\"/\"target\" distinction. You can create your modified DataSet like so:\n",
              "labeled_data = [*zip(dataset, labels)]\n",
              "data_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=False)\n",
              "\n",
              "for imgs, labels in data_loader: # per batch\n",
              "    ...\n",
              "\n",
              " <P> If your DataLoader is something like this:\n",
              "test_loader = DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True)\n",
              "\n",
              "it is giving you a batch of size batch_size, and you can pick out a single random example by directly indexing the batch:\n",
              "for test_images, test_labels in test_loader:  \n",
              "    sample_image = test_images[0]    # Reshape them according to your needs.\n",
              "    sample_label = test_labels[0]\n",
              "\n",
              "\n",
              "Alternative solutions\n",
              "\n",
              "You can use RandomSampler to obtain random samples.\n",
              "\n",
              "Use a batch_size of 1 in your DataLoader.\n",
              "\n",
              "Directly take samples from your DataSet like so:\n",
              " mnist_test = datasets.MNIST('../MNIST/', train=False, transform=transform)\n",
              "\n",
              "Now use this dataset to take samples:\n",
              " for image, label in mnist_test:\n",
              "      # do something with image and other attributes\n",
              "\n",
              "\n",
              "(Probably the best) See here:\n",
              " inputs, classes = next(iter(dataloader))   \n",
              "\n",
              "\n",
              "\n",
              " <P> from torch.utils.data import Dataset, DataLoader\n",
              "training_set = Dataset(xtrain, ytrain)\n",
              "test_set = Dataset(xtest, ytest)\n",
              "params = {'batch_size': 64,\n",
              "        'shuffle': True}\n",
              "train_loader = DataLoader(training_set, **params)\n",
              "\n",
              " <P> You can use the SKLearn's train_test_split + SubSetRandomSampler from torch as follow :    \n",
              "\n",
              "from torch.utils.data.sampler import SubsetRandomSampler\n",
              "from sklearn.model_selection import train_test_split\n",
              "from torch.utils.data.sampler import SubsetRandomSampler\n",
              "valid_size_and_test = 0.4 # then I will keep 0.1 for testing \n",
              "\n",
              "## split the dataset into train and rest data\n",
              "targets = dataset.targets\n",
              "\n",
              "train_idx, rest_idx = train_test_split(np.arange(len(targets)), test_size= \n",
              "                    valid_size_and_test, random_state=42, shuffle=True, stratify=targets)\n",
              "test_size = int(len(rest_idx) * valid_size_and_test)\n",
              "valid_idx, test_idx = rest_idx[:test_size], rest_idx[test_size:]\n",
              "\n",
              "dataloaders = {'trainLoader' : torch.utils.data.DataLoader(dataset,batch_size=32,sampler=SubsetRandomSampler(train_idx)),\n",
              "               'validLoader' : torch.utils.data.DataLoader(dataset,batch_size=32,sampler=SubsetRandomSampler(valid_idx)),\n",
              "               'testLoader' :  torch.utils.data.DataLoader(dataset,batch_size=32,sampler=SubsetRandomSampler(test_idx)),\n",
              "              }\n",
              "\n",
              " <P> You can use torch.utils.data.Subset to split your ImageFolder dataset into train and test based on indices of the examples.\n",
              "For example:\n",
              "\n",
              "orig_set = torchvision.datasets.Imagefolder(...)  # your dataset\n",
              "n = len(orig_set)  # total number of examples\n",
              "n_test = int(0.1 * n)  # take ~10% for test\n",
              "test_set = torch.utils.data.Subset(orig_set, range(n_test))  # take first 10%\n",
              "train_set = torch.utils.data.Subset(orig_set, range(n_test, n))  # take the rest   \n",
              "\n",
              " <P> From torchvision.datasets.ImageFolder documentation:\n",
              "\"Returns:   (sample, target) where target is class_index of the target class.\"\n",
              "So, quite simply, the dataset object you're currently using returns a tuple with 2 items. You'll get an error if you try to store this tuple in 3 variables. The correct line would be:\n",
              "for i, (batch, targets) in enumerate(val_loader):\n",
              "\n",
              "If you really need the names (which I assume is the file path for each image) you can define a new dataset object that inherits from the ImageFolder dataset and overload the __getitem__ function to also return this information.\n",
              " <P> If you want a DataLoader where you just want to define the class label for each sample then you can make use of the torch.data.utils.Subset class. Despite its name it doesn't necessarily need to define a subset of dataset. For example\n",
              "import torch\n",
              "import torchvision\n",
              "import torchvision.transforms as T\n",
              "from itertools import cycle\n",
              "\n",
              "mnist = torchvision.datasets.MNIST(root='./', train=True, transform=T.ToTensor())\n",
              "\n",
              "# not sure what \"...and so on\" implies, but define this list however you like\n",
              "target_classes = [1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3]\n",
              "\n",
              "# create cyclic iterators of indices for each class in MNIST\n",
              "indices = dict()\n",
              "for label in torch.unique(mnist.targets).tolist():\n",
              "    indices[label] = cycle(torch.nonzero(mnist.targets == label).flatten().tolist())\n",
              "\n",
              "# define the order of indices in the new mnist subset based on target_classes\n",
              "new_indices = []\n",
              "for t in target_classes:\n",
              "    new_indices.append(next(indices[t]))\n",
              "\n",
              "# create a Subset of MNIST based on new_indices\n",
              "mnist_modified = torch.utils.data.Subset(mnist, new_indices)\n",
              "dataloader = torch.utils.data.DataLoader(mnist_modified, batch_size=1, shuffle=False)\n",
              "\n",
              "for idx, (x, y) in enumerate(dataloader):\n",
              "    # training loop\n",
              "    print(f'Batch {idx+1} labels: {y.tolist()}')\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col0\" class=\"data row43 col0\" >LibTorch Windows binaries appear to not be built with CuDNN</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col1\" class=\"data row43 col1\" >The __CXX11 superficially looks related to the C++ ABI switch in gcc 5. You could either try to compile both PyTorch from source and your extension with the same compiler or share which version you used for your extension. IIRC, the nervanagpu submodule is optional. Here are steps to build:\n",
              " \n",
              " \t \n",
              " 1. `git clone --branch v0.4.1</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col2\" class=\"data row43 col2\" >Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row43_col3\" class=\"data row43 col3\" ><P> Turns out the GPU build in was cached by setuptools (build/, dist/, etc.) and also installed!\n",
              " <P> We're running CI builds against CUDA 10, and I believe the current plan for pre-built binaries is the next release. <P> Hi,\n",
              "even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`. <P> You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the C++ headers only in several months. <P> Sorry the 0.4.1 source build is broken. IIRC, the nervanagpu submodule is optional. Here are steps to build:\n",
              " \n",
              " \n",
              " \n",
              " 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`\n",
              " \n",
              " 2. `cd pytorch-0.4.1`\n",
              " \n",
              " 3. `git rm --cached third_party/nervanagpu`\n",
              " \n",
              " 4. Delete these lines:\n",
              " \n",
              " \n",
              " \n",
              " https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21\n",
              " \n",
              " \n",
              " \n",
              " 5. `git submodule update --init --recursive`\n",
              " \n",
              " 6. `python setup.py install`\n",
              " \n",
              " \n",
              " \n",
              " Be aware that git submodules have some weird state, so if you run into trouble I would recommend deleting your checkout and starting again. <P> cc  who knows the best about c++ extension\n",
              "The __cxx11 superficially looks related to the C++ ABI switch in gcc 5. You could either try to compile both PyTorch from source and your extension with the same compiler or share which version you used for your extension.\n",
              "If you have a very recent cuda, gcc 6 is a great choice, I have used gcc 5 successfully since PyTorch 0.1.2, but I always self-compiled so I didn’t have consistency issues.\n",
              "\n",
              "\n",
              "Hi tom,\n",
              "I have tested on Debian testing (no cuda) with g++5, 6 and 7, as well as with clang++ 6 and on Ubuntu 18.04 (with cuda) with g++5. The same symbol remains undefined.\n",
              "I will try asap to rebuild v0.4.1 from source, to test if the symbol is recovered.\n",
              "Indeed, it works with the built version (branch: master commit: 0d5e4a2c6)… I’m not sure it is really a good news though. Asking an end-user to rebuild pytorch from source is, for our project, a bit too much.\n",
              "I hope next python package will not suffer the same problem…\n",
              "Anyway.  for your kind and quick answer\n",
              "At some point of time, PyTorch was compiled with g++ 4.8 or somesuch (before the C++ abi switch) and g++ 5,6,7 are all after (I don’t know whether you can use a switch for compatibility).\n",
              "Personally, I’d hope that one could move to a newer gcc, but even the\"manylinux 2010\" seems to decree gcc 4 or something like that. \n",
              "\n",
              " <P> `conda install pytorch=0.1.12 cuda80 -c soumith`\n",
              "We'll update the docs soon to give links to older versions. <P> Can you share a few more details how you've built gcc-5.4.0?\n",
              "By default, GCC is coupled with a version of `libstdc++`, but there is a way to configure the build to use version already available in the system path. \n",
              "Otherwise, it sort of expected behaviour: shared library/executable compiled against newer version of libstdc++ is not compatible with an old one.\n",
              "PyTorch binary is build using `devtoolset-7`, which can be installed using something like the following \n",
              "https://github.com/pytorch/builder/blob/589a615fc8a8ee24690a1037ba583d32f22bc3a3/manywheel/Dockerfile#L12-L14\n",
              "Following article describes process of installing newer toolchain on CentOS in a bit more detail: https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/ <P> updated.\n",
              " \n",
              " \n",
              " \n",
              " It seems like your do not need to re-install anaconda, find `libTH*` in your directory, you may find\n",
              " \n",
              " \n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTH.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n",
              " \n",
              " /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n",
              " \n",
              " /opt/anaconda/lib/libTH.so.1\n",
              " \n",
              " /opt/anaconda/lib/libTHNN.so.1\n",
              " \n",
              " /opt/anaconda/lib/libTHPP.so.1\n",
              " \n",
              " /opt/anaconda/lib/libTHS.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHS.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHPP.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHNN.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTH.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTH.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n",
              " \n",
              " /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " So, it is ok to uninstall `libtorch` from conda, and everything goes ok\n",
              " \n",
              " \n",
              " \n",
              " conda uninstall libtorch\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " Thanks for updating to the exciting `0.2.0`. <P> What is your cmake version? I can compile on a mac and my version is 3.9.4. You can see it via `cmake --version`</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col0\" class=\"data row44 col0\" >segfault when using xarray, only when &#39;import torch&#39;</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col1\" class=\"data row44 col1\" >As explicitly stated by the error message, you made a typo : you should have written\n",
              "torch::kInt16 instead of torch::KInt16. The k should not be capitalized.\n",
              "\n",
              "The message is pretty straight forward, PyTorch is not installed. You can download it from Pytorch website here.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col2\" class=\"data row44 col2\" >Installing netcdf4 via pip solves the problem.\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row44_col3\" class=\"data row44 col3\" ><P> Update your opencv to a newer version. It should help.\n",
              "\n",
              "pip install opencv-python==4.1.0.25\n",
              "\n",
              " <P> As explicitly stated by the error message, you made a typo : you should have written\n",
              "torch::kInt16 instead of torch::KInt16. The k should not be capitalized.\n",
              " <P> The message is pretty straight forward, Pytorch is not installed. You can download it from Pytorch website here.\n",
              " <P> Probably pycocotools is missing, which can be installed via:\n",
              "# install pycocotools\n",
              "cd $INSTALL_DIR\n",
              "git clone https://github.com/cocodataset/cocoapi.git\n",
              "cd cocoapi/PythonAPI\n",
              "python setup.py build_ext install\n",
              " <P> In jupyter notebook simply restarting kernel works fine\n",
              " <P> Solved\n",
              "Turns out somehow torchvision 0.2.2 had been installed instead of the latest 0.9.1 (which my other environment used).\n",
              "This was solved by uninstalling torchvision using\n",
              "conda remove torchvision\n",
              "\n",
              "then installing torchvision using pip (using conda install gave me version 0.2.2)\n",
              "pip install torchvision\n",
              "\n",
              "I also had to reinstall six using pip.\n",
              " <P> Alright, turns out this is an issue with pyinstaller.\n",
              "if Pytorch is installed using Conda, it requires the CUDANN , and it won't work with it (ie without that environment)\n",
              "if you want it to work every where, Pytorch has to be installed using pip.\n",
              "For reference,\n",
              "https://github.com/pyinstaller/pyinstaller/issues/2666#issuecomment-508013383\n",
              " <P> There is some discussion regarding this here. I had the same issue but using cuda 11.1 resolved it for me.\n",
              "This is the exact pip command\n",
              "pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
              "\n",
              " <P> It's happening because, by default, conda prefers packages from a higher priority channel over any version from a lower priority channel.  -- conda docs\n",
              "You can solve this problem by setting the priority of pytorch channel higher than the default channel by changing the order in .condarc -- more here\n",
              "channels:\n",
              "  - pytorch\n",
              "  - defaults\n",
              "  - conda-forge\n",
              "\n",
              "channel_priority: true\n",
              "\n",
              "or you can upgrade it by specifying as option:\n",
              "conda update --all -c pytorch\n",
              "\n",
              " <P> I am using Torch 1.4.0 on Windows and I had the same issue. Turns out I had installed the 2.x version of Tensorboard. I reverted back to 1.15.0 and it solved the issue.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col0\" class=\"data row45 col0\" >\"undefined symbol: PySlice_Unpack\" of pytorch 1.0.0 with Python 3.6.0</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col1\" class=\"data row45 col1\" >I solved the same issue by installing a `libpython3.6-dev` package on Ubuntu system. <P> @JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/PyTorch/torch/lib/` (libiomp5.so dir) fixes it for me</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col2\" class=\"data row45 col2\" >Upgrading to Python 3.6.1 was suggested in \n",
              " \n",
              " \n",
              " \n",
              " Duplicate of #14931.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row45_col3\" class=\"data row45 col3\" ><P> I solved the same issue by installing a `libpython3.6-dev` package on Ubuntu system. <P> @JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir) fixes it for me <P> Try a clean re-install. \n",
              "bash\n",
              "rm -rf build\n",
              "rm -rf torch/lib/build\n",
              "\n",
              "Then install again <P> it's very likely that you installed x64 libnccl, instead of ppc64 version <P> @szagoruyko Could you please try opening torch in any directory other than repo's root? It's trying to load the `torch` dir instead of the python package and gives you this error.\\n <P> Yes, it's probably picking up a header/library from the old install. <P> Does it work if you do \"FULL_CAFFE2=1 python setup.py install\" instead? <P> I had a similar issue with the error message: ImportError: cannot import name 'nan'. The fix was to do: \n",
              " \n",
              " \n",
              " \n",
              " `pip3 uninstall torch`\n",
              " \n",
              " \n",
              " \n",
              " multiple times (3, in my case) untill the message `Cannot uninstall requirement torch, not installed` appeared. After this, a fresh install via:\n",
              " \n",
              " \n",
              " \n",
              " `pip3 install torch==0.3.1`\n",
              " \n",
              " \n",
              " \n",
              " seemed to fix the problem. I realise we are using different versions of pytorch, but hopefully this helps. <P> I fixed the problem by importing opencv before torch. <P> Add the path by: export PATH=~/anaconda3/bin:$PATH  \n",
              "before opening the python.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col0\" class=\"data row46 col0\" > all, if my code is\n",
              "n=10\n",
              "learning_rate=1e-2\n",
              "criterion= nn.BCEWithLogitsLoss()\n",
              "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-6)\n",
              "loss= criterion(outputs, targets)\n",
              "loss=loss / n\n",
              "loss.backward()\n",
              "\n",
              "Does the loss  equivalent with the below by using learning rate / n?\n",
              "n=10\n",
              "learning_rate=1e-3  #1e-2 /n\n",
              "criterion= nn.BCEWithLogitsLoss()\n",
              "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-6)\n",
              "loss= criterion(outputs, targets)\n",
              "loss.backward()\n",
              "\n",
              "I am using SGD. IF not, what should I change in the first code without using loss=loss/n? Thanks</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col1\" class=\"data row46 col1\" >Are you using the sigmoid for both layers?\n",
              "Note that nn.BCEWithLogitsLoss does not expect probabilities, but raw logits.\n",
              "Could you post the training code, so that we can have a look?\n",
              "Ah yes you are completely right! I’ll fix that!</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col2\" class=\"data row46 col2\" >If you’re using momentum and/or weight decay (or another optimizer) it won’t work.\n",
              "For example SGD+weight decay (with learning rate lr and weight_decay wd will do the following update: w = w - lr * (dL/dw + wd*w). You can see that scaling your loss: dL/dw -> 1/n*dL/dw will not have the same effect as changing the learning rate lr -> 1/n*lr: the weight decay term will not be scaled the same way.\n",
              "For other optimizer, you will need to check the formula of the update and see if scaling of the gradient and of the learning rate have the same effect on the update or not (it is really unlikely).</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row46_col3\" class=\"data row46 col3\" ><P> Are you using the sigmoid for both layers?\n",
              "Note that nn.BCEWithLogitsLoss does not expect probabilities, but raw logits.\n",
              "Could you post the training code, so that we can have a look?\n",
              "Ah yes you are completely right! I’ll fix that! <P> The loss changes for random input data using your code snippet:\n",
              "train_data = torch.randn(64, 6)\n",
              "train_out = torch.empty(64, 17).uniform_(0, 1)\n",
              "\n",
              "so I would recommend to play around with some hyperparameters, such as the learning rate.\n",
              "That being said, I’m not familiar with your use case, but a softmax output in L1Loss doesn’t seem to be the usual use case.\n",
              "If you are dealing with a multi class classification use case, I would recommend to try out nn.CrossEntropyLoss and pass the raw logits to it.\n",
              "Thanks !! removing softmax layer worked\n",
              "I want to Trouble you again .\n",
              "Sorry for this ,Plz help me\n",
              "I am facing same issue with another model.I am building simple digit classifier using ANN\n",
              "code=\n",
              "class NET(nn.Module):\n",
              "    def __init__(self):\n",
              "        super().__init__()\n",
              "        self.model=nn.Sequential(\n",
              "            nn.Linear(784, 128),\n",
              "            nn.ReLU(),\n",
              "            nn.Linear(128, 256),\n",
              "            nn.ReLU(),\n",
              "            nn.Linear(256, 512),\n",
              "            nn.ReLU(),\n",
              "            nn.Linear(512, 10),\n",
              "        \n",
              "        )\n",
              "    def forward(self,x):\n",
              "            return self.model(x)\n",
              "\n",
              "\n",
              "loss_fn=nn.CrossEntropyLoss()\n",
              "opt=optim.Adam(net.parameters() , lr=0.01)\n",
              "\n",
              "loss_epoch=[]\n",
              "epochs=5\n",
              "for i in range(epochs):\n",
              "        \n",
              "        opt.zero_grad()\n",
              "        output=net(x_train)\n",
              "        loss=loss_fn(output, y_train)\n",
              "        loss.backward()\n",
              "        opt.step()\n",
              "        loss_epoch.append(loss.item())\n",
              "        \n",
              "        print(\"Epochs: {}/{} , Loss:{}\".format(i,epochs,loss))\n",
              "\n",
              "OUTPUT=\n",
              " Epochs: 0/5, Loss:2.301159143447876\n",
              "\n",
              " Epochs: 1/5, Loss:2.301161289215088\n",
              "\n",
              " Epochs: 2/5, Loss:2.3011562824249268\n",
              "\n",
              " Epochs: 3/5, Loss:2.3011701107025146\n",
              "\n",
              " Epochs: 4/5, Loss:2.3011698722839355\n",
              "\n",
              "data- Mnist Digit data\n",
              "Again, I would recommend to play around with hyperparameters, such as the learning rate.\n",
              "Since the loss changes and no dropout (or other “random” operations are used), it would mean that the parameters get some updates but the overall training isn’t able to reduce the loss.\n",
              "Start by using a lower learning rate (e.g. 1e-3) and try to overfit the current data sample.\n",
              " very much .It was due to high LR.\n",
              "Loss starts decreasing when LR is 1e-7 <P> Looking at PyTorch's torch.optim.lr_scheduler code here, I can see that they set the parameter of the optimizer. Thus, that will be the best approach. The exact place I can see this is in step function of class _LRScheduler (in the above link).\n",
              "\n",
              "You can do the same by\n",
              "\n",
              "optimizer.param_groups[0]['lr'] = lr\n",
              "\n",
              "\n",
              "as you had mentioned yourself.\n",
              " <P> I think it's because you are using cross entropy loss function which in PyTorch combines log-softmax and negative log likelihood. Since your model already performs softmax before returning the output, you actually end up calculating the negative log likelihood for softmax of softmax. Try removing the final softmax from your model.\n",
              "PyTorch documentation for cross entropy loss: https://pytorch.org/docs/stable/nn.functional.html#cross-entropy\n",
              " <P> You could create a feature request for get_last_lr for this scheduler and meanwhile use the 'lr' attribute of the parameter group of the optimizer directly:\n",
              "print(optimizer.param_groups[0]['lr'])\n",
              " <P> If you want to decay the learning rate every 30 epochs you could use\n",
              "torch.optim.lr_scheduler.StepLR (optimizer , 30 , gamma=0.01)\n",
              "Thanks for the response, but the decay has to be exponentially. So can I do the same in ExponentialLR ?\n",
              "If I’ve understood it correctly you want to use exponential lr rate scheduler and starting at 0.01 you want it to decrease to 0.0001 after 30 epochs. We can do some calculations where we should have:\n",
              "0.01 * gamma^30 = 0.0001, from this we can imply that gamma = 0.01^(1/30) = 0.85769…\n",
              "So set gamma approximately to 0.86 and it should work as you wanted (approximately).\n",
              " your lr_scheduler.step() should be inside the epoch for-loop. <P> Actually, I found out. It turns out that BCELoss and log_loss behaves differently when the weights sum up to more than the dimension of the input array. Interesting.  \n",
              " <P> Based on the implementation in Keras I think your first formulation is the correct one, the one that contain the initial learning rate (note that self.lr is not being updated).\n",
              "\n",
              "However I think your calculation is probably not correct: since the denominator is the same, and lr_0 >= lr since you are doing decay, the first formulation has to result in a bigger number.\n",
              "\n",
              "I'm not sure if this decay is available in PyTorch, but you can easily create something similar with torch.optim.lr_scheduler.LambdaLR.\n",
              "\n",
              "decay = .001\n",
              "fcn = lambda step: 1./(1. + decay*step)\n",
              "scheduler = LambdaLR(optimizer, lr_lambda=fcn)\n",
              "\n",
              "\n",
              "Finally, don't forget that you will need to call .step() explicitly on the scheduler, it's not enough to step your optimizer. Also, most often learning scheduling is only done after a full epoch, not after every single batch, but I see that here you are just recreating Keras behavior.\n",
              " <P> I cannot tell with certainty without seeing your training code, but it's most likely your model was trained with cross-entropy loss and as such it outputs logits rather than class probabilities. You can turn them into proper probabilities by applying the softmax function.\n",
              " <P> Your learning rate schedule is sub-optimal for this dataset. Try to first figure out the best learning rate for this network and dataset with \n",
              "LRFinder. This can be done by exploring the loss behavior for different learning rates with \n",
              "\n",
              "learn.lr_find()\n",
              "learn.recorder.plot()\n",
              "\n",
              "\n",
              "Edit:\n",
              "\n",
              "It looks like you are re-training the last layer in your network. Instead try training more layers from scratch. as:\n",
              "\n",
              "learn.unfreeze(2)\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col0\" class=\"data row47 col0\" >Calling .backward() function for two different neural networks but getting retain_graph=True error</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col1\" class=\"data row47 col1\" >use retain_graph = True on the second backwards pass. This flag is making your code store the computation graphs for each batch, conceivably in perpetuity. Only the learned parameters (parameters given to the optimiser) should have requires_grad=True and generally, you don't have to set it manually either.Parameter handles that automatically.\n",
              "You should also ensure that the tensors you create from NumPy data have type torch.float (float32) as NumPy's float array will usually be float64, which are mostly unnecessary and just slower compared to float32, especially on the GPU.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col2\" class=\"data row47 col2\" >you should propagate through parts of the graph.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row47_col3\" class=\"data row47 col3\" ><P> Don't use retain_graph = True on the second backwards pass. This flag is making your code store the computation graphs for each batch, conceivably in perpetuity.\n",
              "You should only use retain_graph for all but the last call to backward() that back-propagate through the same variables/parameters.\n",
              " <P> Presumably, you haven't re-run the data creation code again after setting retain_graph=True, as it is in a IPython REPL. It would work with that, but in nearly all cases, setting retain_graph=True is not the appropriate solution.\n",
              "\n",
              "The problem in your case, is that you have set requires_grad=True for U, which means that everything that involved U in the data creation, will be recorded in the computational graph and when loss.backward() is called, the gradients will propagate through all of these up to U. After the first time, all buffers for the gradients of those will have been freed, and a second backward will fail.\n",
              "\n",
              "Neither U nor Z should have requires_grad=True, as they are not being optimised/learned. Only the learned parameters (parameters given to the optimiser) should have requires_grad=True and generally, you don't have to set it manually either, since nn.Parameter handles that automatically.\n",
              "\n",
              "You should also ensure that the tensors you create from NumPy data have type torch.float (float32) as NumPy's float array will usually be float64, which are mostly unnecessary and just slower compared to float32, especially on the GPU.\n",
              "\n",
              "U = torch.tensor(U, dtype=torch.float)\n",
              "\n",
              "Z = torch.tensor(Z, dtype=torch.float)\n",
              "\n",
              "\n",
              "And remove the retain_graph=True from the backward call:\n",
              "\n",
              "loss.backward()\n",
              "\n",
              " <P> In such a case, one can detach the computation graph to exclude the parameters that don't need to be optimized. In this case, the computation graph should be detached after the second forward pass with gru1 i.e.\n",
              "....\n",
              "gru1_opt.step()\n",
              "gru1_output, _ = gru1(vector)\n",
              "gru1_output = gru1_output.detach()\n",
              "....\n",
              "\n",
              "This way, you won't \"try to backward through the graph a second time\" as the error mentioned.\n",
              " <P> loss_G.backward() should be  loss_G.backward(retain_graph=True) this is because when you use backward normally it doesn't record the operations it performs in the backward pass, retain_graph=True is telling to do so.\n",
              " <P> As the error message reads, you need to specify the retain_graph=True option on the first .backward call, not the second:\n",
              "c.backward(retain_graph=True)\n",
              "e = d*c\n",
              "e.backward()\n",
              "\n",
              "If you do not retain the graph, the second backward pass will not be able to reach the nodes c, a, and b because the activations will have been cleared by the first backward pass.\n",
              " <P> It is as the error says. The loss tensor doesn't require grad and doesn't have a gread_fn function. Your out in forward in RNN_pytorch  should also already have a grad_fn, start checking there. See if you somehow disabled gradients by for example having .eval() or .no_grad() somewhere.\n",
              " <P> This worked because the loss calculation has happened before the no_grad and you keep calculating the gradients according to that loss calculation (which calculation had gradient enabled). \n",
              "\n",
              "Basically, you continue update the weights of your layers using the gradients calculated outside of the no_grad. \n",
              "\n",
              "When you actually use the no_grad: \n",
              "\n",
              "for epoch in range(epochs):\n",
              "    # convert numpy to tensor\n",
              "    inputs = torch.from_numpy(x_train)\n",
              "    targets = torch.from_numpy(y_train)\n",
              "    with torch.no_grad():  # no_grad used here\n",
              "    # forward\n",
              "        out = model(inputs)\n",
              "        loss = criterion(out, targets)\n",
              "        model.zero_grad()\n",
              "        loss.backward()\n",
              "        optimizer.step()\n",
              "        print('inputs grad : ', inputs.requires_grad)\n",
              "    if epoch % 5 == 0:\n",
              "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
              "\n",
              "\n",
              "Then you will get the proper error, saying: \n",
              "\n",
              "element 0 of tensors does not require grad and does not have a grad_fn.\n",
              "\n",
              "That is, you use no_grad where is not appropriate to use it. \n",
              "\n",
              "If you print the .requires_grad of loss, then you will see that loss has requires_grad. \n",
              "\n",
              "That is, when you do this: \n",
              "\n",
              "for epoch in range(epochs):\n",
              "    # convert numpy to tensor\n",
              "    inputs = torch.from_numpy(x_train)\n",
              "    targets = torch.from_numpy(y_train)\n",
              "\n",
              "    # forward\n",
              "    out = model(inputs)\n",
              "    loss = criterion(out, targets)\n",
              "\n",
              "    # backward\n",
              "    with torch.no_grad():\n",
              "        model.zero_grad()\n",
              "        loss.backward()\n",
              "\n",
              "        optimizer.step()\n",
              "        print('inputs grad : ', inputs.requires_grad)\n",
              "        print('loss grad : ', loss.requires_grad)  # Prints loss.require_rgad\n",
              "    if epoch % 5 == 0:\n",
              "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
              "\n",
              "\n",
              "You will see: \n",
              "\n",
              "inputs grad :  False\n",
              "loss grad :  True\n",
              "\n",
              "\n",
              "Additionally, the \n",
              "\n",
              "print('inputs grad : ', inputs.requires_grad)\n",
              "\n",
              "\n",
              "Will always print False. That is, if you do\n",
              "\n",
              "for epoch in range(epochs):\n",
              "    # convert numpy to tensor\n",
              "    inputs = torch.from_numpy(x_train)\n",
              "    targets = torch.from_numpy(y_train)\n",
              "\n",
              "    print('inputs grad : ', inputs.requires_grad). # Print the inputs.requires_grad\n",
              "\n",
              "    # forward\n",
              "    out = model(inputs)\n",
              "    loss = criterion(out, targets)\n",
              "\n",
              "    # backward\n",
              "    with torch.no_grad():\n",
              "        model.zero_grad()\n",
              "        loss.backward()\n",
              "\n",
              "        optimizer.step()\n",
              "        print('inputs grad : ', inputs.requires_grad)\n",
              "        print('loss grad : ', loss.requires_grad)\n",
              "    if epoch % 5 == 0:\n",
              "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
              "\n",
              "\n",
              "You will get: \n",
              "\n",
              "inputs grad :  False\n",
              "inputs grad :  False\n",
              "loss grad :  True\n",
              "\n",
              "\n",
              "That is, you are using wrong things to check what you did wrong. The best thing that you can do is to read again the docs of PyTorch on gradient mechanics. \n",
              " <P> The error is because the weight of the linear layer has changed (through optimizer.step).\n",
              "One might add that here, you have the gradient computation for addmm (which powers linear) pretend it would also want to compute the input derivative for which it would need the weight which, in this trivial use, is not actually the case. If you have a multiple layers, all but the first need the input gradient and you cannot actually change the weight and then compute the (correct) backward, not even with retain_graph.\n",
              "\n",
              " <P> Zero the grad before the backward, bot after.\n",
              "\n",
              "\n",
              "Great . It looks like the error is decreasing now. Thanks a lot.\n",
              "Other than that is the model built correctly? <P> You are missing .zero_grad() operation. Add that to the loop and your code will work fine without retain_graph= True. \n",
              "\n",
              "loss.backward()\n",
              "opt.step()\n",
              "opt.zero_grad()\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col0\" class=\"data row48 col0\" >In LSTM, if user passes what, hidden, function should use the first input as activation and the second input as hidden?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col1\" class=\"data row48 col1\" >the LSTM processes a sequences of words, each words is represented by a vector of size input_size. This representation is needed for the model representation since it allows you to map the values of words to the corresponding tensors. For example, if user passes what, hidden, function should use the first input as activation and the second input as hidden.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col2\" class=\"data row48 col2\" >activation</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row48_col3\" class=\"data row48 col3\" ><P> SGTM <P> logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              " <P> Double-backward <P> The Network defined as having two layers, hidden and output.\n",
              "Roughly speaking, the function of the hidden layer is to hold parameters you can optimize during training.\n",
              " <P> LGTM, will test <P> functions – A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially. segments – Number of chunks to create in the model input – A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) – Omit stashing and restoring the RNG state during each checkpoint. Output of running functions sequentially on *inputs Example <P> \n",
              "it seems to me by default the output of a PyTorch model's forward pass\n",
              "is logits\n",
              "\n",
              "As I can see from the forward pass, yes, your function is passing the raw output\n",
              "    def forward(self, x):\n",
              "        x = self.pool(F.relu(self.conv1(x)))\n",
              "        x = self.pool(F.relu(self.conv2(x)))\n",
              "        x = x.view(-1, 16 * 5 * 5)\n",
              "        x = F.relu(self.fc1(x))\n",
              "        x = F.relu(self.fc2(x))\n",
              "        x = self.fc3(x)\n",
              "        return x\n",
              "\n",
              "So, where is softmax? Right here:\n",
              "criterion = nn.CrossEntropyLoss()\n",
              "\n",
              "It's a bit masked, but inside this function is handled the softmax computation which, of course, works with the raw output of your last layer\n",
              "This is softmax calculation:\n",
              "\n",
              "where z_i are the raw outputs of the neural network\n",
              "So, in conclusion, there is no activation function in your last input because it's handled by the nn.CrossEntropyLoss class\n",
              "Answering what's the raw output that comes from nn.Linear: The raw output of a neural network layer is the linear combination of the values that come from the neurons of the previous layer\n",
              " <P> Very crudely speaking, number of features refer to the size of the vectors/tensors.\n",
              "Outside end-to-end neural networks, the term feature had a more tangible meaning, since feature engineering was an import processing step you had to do “manually”. For example, to use a SVM to classify a text document had extract the text document into a set of features. This could some very naive features such as #words, #characters. In this case, each document would be represented by 2 numerical features. In practice, you would have more meaningful features but they would have a clear semantic meaning.\n",
              "With end-to-end neural networks, this semantic meaning is usually latent and not obvious. For example, each word in a text may be represented by a 300-dim vector of numerical values, i.e., the word has 300 features. This vector places a word in a 300-dim space in relation to other words. However, you usually don’t really know what an individual value in the vector means. For example, The 25th entry in the 300-dim vector does not tell you that the word is a noun, verb, adjective, etc.\n",
              "So in case you use an LSTM for text processing where the LSTM processes a sequences of words, each words is represented by a vector of size input_size. This representation is needed since word are symbolic representations. For example, the words “cat” and “kitten” are only similar to you because your mental models of both concepts (animal, 4 legs, furry, meows, etc.) are similar. For a computer, these are completely different things. A vector representation now allows to map “cat” and “kitten” (and all other word) to a numerical representation where the vector for “cat” and the vector for “kitten” are closer together compared to, say, the vector of “cat” and the vector of “train”.\n",
              "In contrast, if you use an LSTM for time series prediction of already scalar numerical values, input_size is just 1.\n",
              "For the resulting dimensions of the hidden state, you best consult the PyTorch docs, but it definitely does not depend on the input_size.\n",
              "I’m afraid I don’t quite understand your explanation. I’m sorry!\n",
              "I’ve scoured the PyTorch RNN docs, and there is little to no explanation of how data should be formatted for input, and certainly no information about how the shape of the data is changing as it moves through the network.\n",
              "For example,  my forward pass I need to have my data in the shape (batch_size, seq_length, input_size), but there are no concrete examples of what seq_length is or what input_size is. How are they different?\n",
              "Since you posted in nlp, I assume you work with text.\n",
              "A very common application is sentence classification (e.g., for sentence classification), where each sentence is a sequence of words. Let’s say you have a batch 3 sentences, each containing 10 words (nn.LSTM and nn.GRU require by default sequences of the same length; you can look up padding and packing with link \"https://discuss.pytorch.org/t/tensorflow-esque-bucket-by-sequence-length/41284/15\")\n",
              "That means your batch has the shape (batch_size, seq_len), i.e., (3, 10) with the numbers above. Not that each sentence/sequence is a vector of integers reflecting the index of a word in your vocabulary.\n",
              "The next step is to push the batch through a nn.Embedding layer to map words (represented by ther indices) to word vectors of size, say, 100. The output shape after the embedding layer is then (batch_size, seq_len, embed_dim), i.e., (3, 10, 100) with the numbers above.\n",
              "This tensor can now serve as input for your nn.LSTM or nn.GRU which expect as input (batch_size, seq_len, input_size) – not that by default, they actually expect (seq_len, batch_size, input_size); so either you transform() for tensor or you define your RNN layer with batch_first=True.\n",
              "Anyway, embed_dim, i.e., the size of your word vectors defines input_size, 100 in the example above. Summing up\n",
              "\n",
              "\n",
              "batch_size is the number of sentences in your batch (e.g., 3)\n",
              "\n",
              "seq_len is the number of items in your sequences such as words in a sentence (e.g., 10)\n",
              "\n",
              "input_size is the size of the tensor/vector that represents a single(!) item in your sequence such as 100-dim word vectors for each word in a sentence.\n",
              "\n",
              "The shape of inputs and outputs are very well defined; see, for example, for nn.LSTM with link \"https://pytorch.org/docs/master/generated/torch.nn.LSTM.html\". <P> The difference lies in how nn.Linear initializes weights and bias:\n",
              "\n",
              "class Linear(Module):\n",
              "\n",
              "    def __init__(self, in_features, out_features, bias=True):\n",
              "        super(Linear, self).__init__()\n",
              "        self.in_features = in_features\n",
              "        self.out_features = out_features\n",
              "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
              "        if bias:\n",
              "            self.bias = Parameter(torch.Tensor(out_features))\n",
              "        ...\n",
              "\n",
              "\n",
              "So, when you write nn.Linear(300, 10) the weight is (10, 300) and bias is (10). But, in the ModelOne, weights has dimension (300, 10).\n",
              "\n",
              "You can confirm it using\n",
              "\n",
              "for name, param in mo.named_parameters():\n",
              "    print(name, param.shape)\n",
              "\n",
              "\n",
              "The output in ModelOne:\n",
              "\n",
              "weights torch.Size([300, 10])\n",
              "bias torch.Size([10])\n",
              "\n",
              "In ModelTwo:\n",
              "\n",
              "linear.weight torch.Size([10, 300])\n",
              "linear.bias torch.Size([10])\n",
              "\n",
              "\n",
              "\n",
              "Now, the reason you're getting [300, 10] in first case and [10, 10] in second case is because if you print the length of a 2d Tensor, then it'll only give its first dimension i.e.\n",
              "\n",
              "a = torch.Tensor(10, 300)\n",
              "b = torch.Tensor(10)\n",
              "print(len(a), len(b))\n",
              "\n",
              "\n",
              "(10, 10)\n",
              " <P> Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col0\" class=\"data row49 col0\" >What is sampled from the multinomial probability distribution located in the corresponding row of tensor input?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col1\" class=\"data row49 col1\" >the sum of gradients of A tensor with respect to the corresponding row in the corresponding TF. This can be either a 1-dimensional tensor or a vector of size (1) where the boundaries of A represent the greatest common divisor of A, and the corresponding column of A represents the cumulative product of A in the dimension dim.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col2\" class=\"data row49 col2\" >num_samples indices</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row49_col3\" class=\"data row49 col3\" ><P> Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs. <P> Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other. <P> Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other. <P> Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor <P> the categorical crossentropy loss is typically used in a multiclass classification setting in which the outputs are interpreted as predictions of class membership probabilities the target y is a vector of n elements that represents the true multinomial distribution 4  over all the classes if only one class is correct this vector is a onehot vector the networks output  is also a vector of n elements but represents the networks prediction of the multinomial distribution categorical cross entropy will compare these two vectors y to measure the loss <P> Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col0\" class=\"data row50 col0\" >Returns the maximum value of all elements in the input tensor. Returns the minimum value of each slice of the input tensor</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col1\" class=\"data row50 col1\" >the sum of all elements, treating Not a Numbers (NaNs) as zero. This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles Q of equal elements. If equal elements in input evaluate True, Bessel’s correction will be used.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col2\" class=\"data row50 col2\" >True</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row50_col3\" class=\"data row50 col3\" ><P> Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel’s correction will be used. <P> Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. <P> Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero. <P> Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor. <P> Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor. <P> Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor. <P> Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1. <P> Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements. <P> torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim. <P> nnz mean number non zero elements. In this example nnz = 3.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col0\" class=\"data row51 col0\" >Pytorch can&#39;t convert np.ndarray of type numpy.object</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col1\" class=\"data row51 col1\" >The problem can be solved by changing:\n",
              "\n",
              "train_y = np.array(train_labels) == 'fake'\n",
              "test_y.THRESH_BINARY, it follows this function:\n",
              "from torch.utils.data import Dataset\n",
              "from dataset import DataLoader\n",
              "from DataLoader import Tensor\n",
              "from Tensor import Variable\n",
              "tensor = Tensor([[-0.4287, -1.1934, -2.1562, -0.1574, -3.9775,  0.406,  1.3984,</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col2\" class=\"data row51 col2\" >\n",
              "def get_imgs(path_to_imgs):\n",
              "\n",
              "    imgs = []\n",
              "    for path in path_to_imgs:\n",
              "        imgs.append(torch.Tensor(cv2.imread(path)))\n",
              "\n",
              "    return imgs\n",
              "\n",
              "\n",
              "\n",
              "class Dataset(torch.utils.data.Dataset):\n",
              "    def __init__(self, path_to_imgs, path_to_label):\n",
              "        'Initialization'\n",
              "        self.path_to_imgs = path_to_imgs\n",
              "        self.path_to_label = path_to_label\n",
              "\n",
              "        self.imgs = get_imgs(path_to_imgs)\n",
              "        self.label = get_pts(path_to_label)\n",
              "\n",
              "        # padding ops here (\n",
              "        # for img in self.imgs:\n",
              "        #     ...\n",
              "\n",
              "        self.label = torch.Tensor(self.label)\n",
              "\n",
              "        self.len = len(self.imgs)\n",
              "\n",
              "    def __len__(self):\n",
              "        'Denotes the total number of samples'\n",
              "        return self.len\n",
              "\n",
              "    def __getitem__(self, index):\n",
              "\n",
              "        return self.imgs, self.label\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row51_col3\" class=\"data row51 col3\" ><P> It is difficult to answer properly since you do not show us how you try to do it. From your error message I can see that you try to convert a numpy array containing objects to a torch tensor. This does not work, you will need a numeric data type:\n",
              "\n",
              "import torch\n",
              "import numpy as np\n",
              "\n",
              "# Your test array without 'dtype=object'\n",
              "a = np.array([\n",
              "   np.array([-0.4287  , -1.193   , -2.156   , -0.2264  , -1.978   , -1.101   ,   -3.395   ,  0.2974  ], dtype=np.float16),\n",
              "   np.array([-0.3386 ,  1.398  , -1.083  ,  0.2961 , -0.7354 , -1.326  , -4.33   ,  0.6284 ], dtype=np.float16)\n",
              "])\n",
              "\n",
              "print(a.dtype) # This should not be 'object'\n",
              "\n",
              "b = torch.from_numpy(a)\n",
              "\n",
              "print(b)\n",
              "\n",
              "\n",
              "Output\n",
              "\n",
              "float16\n",
              "tensor([[-0.4287, -1.1934, -2.1562, -0.2264, -1.9775, -1.1006, -3.3945,  0.2974],\n",
              "        [-0.3386,  1.3984, -1.0830,  0.2961, -0.7354, -1.3262, -4.3281,  0.6284]],\n",
              "       dtype=torch.float16)\n",
              "\n",
              " <P> Given that the last 0 in your threshold call means cv.THRESH_BINARY, it follows this function:\n",
              "\n",
              "\n",
              "\n",
              "As your maxval is set to 1, you can replace this threshold call with something like this:\n",
              "\n",
              "(y[0,:,:,0]  0.4).float()\n",
              "\n",
              "\n",
              "I am casting to float, but you can change that as you wish, ofc. Or even to something like:\n",
              "\n",
              "(y[0,:,:,0]  0.4).to(dtype=y.dtype)\n",
              "\n",
              "\n",
              "so that it will remain with the same data type.\n",
              " <P> Didn't see any function native to pytorch for this but you can use np.block:\n",
              "import numpy as np\n",
              "item = np.array(item) # need to convert item from tensor to ndarray\n",
              "x = np.block([[item, item], [item, item]]\n",
              "x = torch.from_numpy(x) # if you want to change it back to tensor\n",
              "\n",
              "might not be the fastest if it's really big sinec you're converting between the types a lot. Note in this way there's no need to inialize x with zeros.\n",
              " <P> Try converting to numpy and then to tensors:\n",
              "\n",
              "x_train = torch.from_numpy(np.array(x_train).astype(np.float32))\n",
              "\n",
              " <P> I can't confirm but I believe your problem will be solved by changing:\n",
              "\n",
              "train_y = np.array(train_labels) == 'fake'\n",
              "test_y = np.array(test_labels) == 'fake'\n",
              "\n",
              "\n",
              "to:\n",
              "\n",
              "train_y = (np.array(train_labels) == 'fake').astype(int)\n",
              "test_y = (np.array(test_labels) == 'fake').astype(int)\n",
              "\n",
              "\n",
              "The train_y data is currently an array of type Bool (True or False) and the tensor needs and int (0 or 1).\n",
              " <P> The problem is that int(string_id, 16) converts your 32 char long hash into a single integer. This is really a very VERY large number.\n",
              "You can, instead, convert it to an array:\n",
              "\n",
              "tensor_id = torch.tensor([int(c, 16) for c in string_id])\n",
              "\n",
              "\n",
              "Resulting with (in your example):\n",
              "\n",
              "\n",
              "tensor([14, 12,  2, 12,  1, 12, 12,  2,  4,  1,  0, 10,  4, 14,  2,  5,  9, 10,\n",
              "        10,  9, 12,  1,  2,  7,  5,  6, 14,  1, 13,  6, 14])\n",
              "\n",
              "\n",
              "\n",
              "You can also group the hex digits to 8 at a time (for int64 tensor):\n",
              "\n",
              "torch.tensor([int(string_id[i:i+8], 16) for i in range(0, len(string_id), 8)], dtype=torch.int64)\n",
              "\n",
              " <P> The type 32SC3 means that your data are 32bits (4 bytes) signed integers, i.e ints. Pytorch kByte type means unsigned char (1 byte, values between 0 and 255). Therefore you are actually reading a matrix of ints as if it were a matrix of uchars.\n",
              "Try with\n",
              "auto tensor_image = torch::from_blob(in_img.data, {1, in_img.rows, in_img.cols, 3}, torch::kInt32);\n",
              "\n",
              "The conversion to kLong was bound to fail because long means int64. So there are just not enough bytes in your opencv int32 matrix to read it as a int64 matrix with the same size.\n",
              " <P> Well, I found a workaround after reading the data array from FITS\n",
              "\n",
              "data = data.astype(np.float32)\n",
              "a = torch.from_numpy(data)\n",
              "\n",
              "\n",
              "No error is thrown and everything is ok...\n",
              " <P> The problem is that the input you give to your network is of type ByteTensor while only float operations are implemented for conv like operations. Try the following\n",
              "\n",
              "my_img_tensor = my_img_tensor.type('torch.DoubleTensor')\n",
              "# for converting to double tensor\n",
              "\n",
              "\n",
              "Source PyTorch Discussion Forum\n",
              "\n",
              "Thanks to AlbanD\n",
              " <P> There is no string tensor so you cannot directly convert to pytorch tensor of strings. \n",
              "\n",
              "Alternative, you can convert the string to ASCII char values and save that as a Tensor. \n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col0\" class=\"data row52 col0\" >understanding seed of a ByteTensor in PyTorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col1\" class=\"data row52 col1\" >If your column contains NaN it will always be of float type. The presence of NaN values explains therefore why the column is float.\n",
              "\n",
              "You must have a dimension problem, your predictions variable is two dimensional, 32 x 10, you should flatten it first and then add your 24 elements, see documentions here :\n",
              "\n",
              "predictions = predictions.flatten()\n",
              "to_append = logits.argmax(1) # this is your array with 24 elements</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col2\" class=\"data row52 col2\" >It sounds like you're thinking of the seed and the state as equivalent. For older pseudo-random number generators (PRNGs) that was true, but with more modern PRNGs tend to work as described here. (The answer in the link was written with respect to Mersenne Twister, but the concepts apply equally to other generators.)\n",
              "\n",
              "Why is it a good idea to not have a 32- or 64-bit state space and report the state as the generator's output? Because if you do that, as soon as you see any value repeat the entire sequence will repeat. PRNGs were designed to be \"full cycle,\" i.e., to iterate through the maximum number of values possible before repeating. This paper showed that the birthday problem could quickly (O(sqrt(cycle-length)) identify such PRNGs as non-random. This meant, for instance, that with 32-bit integers you shouldn't use more than ~50000 values before a statistician could call you out with a better than 99% level of confidence. The solution, used by many modern PRNGs, is to have a larger state space and collapse it down to output a 32- or 64-bit result. Since multiple states can produce the same output, duplicates will occur in the output stream without the entire stream being replicated. It looks like that's what PyTorch is doing.\n",
              "\n",
              "Given the larger state space, why allow seeding with a single integer? Convenience. For instance, Mersenne Twister has a 19,937 bit state space, but most people don't want to enter that much info to kick-start it. You can if you want to, but most people use the front-end which populates the full state space from a single integer input.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row52_col3\" class=\"data row52 col3\" ><P> Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray. <P> Because you're using float32 dtype. If you convert these two numbers to binary, you will find they are actually the same. Strictly speaking, the most accurate representations of those two numbers in float32 format are the same. \n",
              "\n",
              "0.7575408585008059\n",
              "Most accurate representation = 7.57540881633758544921875E-1\n",
              "\n",
              "0.7575408816337585\n",
              "Most accurate representation = 7.57540881633758544921875E-1\n",
              "\n",
              "Binary: 00111111 01000001 11101110 00110011\n",
              "\n",
              " <P> If your column contains NaN it will always be of float type. The presence of NaN values explains therefore why the column is float.\n",
              "\n",
              "You must have a dimension problem, your predictions variable is two dimensional, 32 x 10, you should flatten it first and then add your 24 elements, see documentions here : \n",
              "\n",
              "predictions = predictions.flatten()\n",
              "\n",
              "to_append = logits.argmax(1) # this is your array with 24 elements \n",
              "predictions = predictions.append(to_append)\n",
              "\n",
              "\n",
              "What happens here is that whan you append a row of 24 elements to a DataFrame with 32 columns, the last columns from 23 to 32 will exist and will automatically be filled with NaN. See some examples here and here.\n",
              "\n",
              "Why do you want to transform NaN to blank? If blank means an empty string, you shouldn't do that, as you will mix float and string in your column. \n",
              "\n",
              "If you want Integer values. You should make an imputation of the NaN values with a constant Integer value (let us say 888):\n",
              "\n",
              "df = df.fillna(888)\n",
              "\n",
              "\n",
              "Then you can transform all to int using the function astype : \n",
              "\n",
              "df = df.astype('int16')\n",
              "\n",
              " <P> Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1). <P> \n",
              "First of all, What is channel last format? Is it NHWC?\n",
              "\n",
              "Yes, additionally when you move from NCHW to NHWC in PyTorch the underlying memory is reordered and stride changes...\n",
              "\n",
              "For the following code, how stride is calculated?\n",
              "\n",
              "numpy introduction\n",
              "stride defines how many bytes you have to jump to get to the next element in this axis, your snippet:\n",
              "import numpy as np\n",
              "\n",
              "N, C, H, W = 10, 3, 32, 32\n",
              "x = np.empty((N, C, H, W))\n",
              "print(x.strides)  # Ouputs: (24576, 8192, 256, 8)\n",
              "\n",
              "So to get from element [0, 0, 0, 0] to element [0, 0, 0, 1] you need to make a jump of 8 bytes as that is the size of np.float64 (default is double). To jump from [0, 0, 0, x] to [0, 0, 1, x] you need to go all over all elements in previous dimension so it is 32 * 8 = 256. For the second dimension you have to go over 32 * 32 * 8 = 8192 and to jump across N you need to traverse 32 * 32 * 8 * 3 = 24576 bytes.\n",
              "In PyTorch\n",
              "In Pytorch you don't jump across bytes, you jump across array elements instead (which IMO is easier and is independent of underlying type).\n",
              "In standard with channels as the second dimension, you need to jump 1 element to move from [0, 0, 0, 0] to [0, 0, 0, 1] (self-explanatory, this is movement across on pixel in W), 32 elements to jump in the second to last dimension (H) and so on.\n",
              "In NHWC format you have (3072, 1, 96, 3) for (N, C, H, W). This means you jump 1 element to move across channels (as this is channels last), 3 elements to jump across W, 3 * 32 across H and 3 * 32 * 32 for N.\n",
              " <P> Depends what your goals are.\n",
              "\n",
              "\n",
              "If you want to simulate your quantized model:\n",
              "\n",
              "\n",
              "You may stick to existing float data type and only introduce truncation as needed, i.e.:\n",
              "\n",
              " x = torch.floor(x * 2**8) / 2**8\n",
              "\n",
              "\n",
              "assuming x is a float tensor.\n",
              "\n",
              "\n",
              "If you want to simulate your quantized model efficiently:\n",
              "\n",
              "\n",
              "Then, I am afraid PyTorch will be not very useful since the low-level convolutional operator is implemented only for float type.\n",
              " <P> As hkchengrex's answer points out, the PyTorch documentation does not explain what rule is used by adaptive pooling layers to determine the size and locations of the pooling kernels. (In fact, there is a fixme in the PyTorch code indicating the documentation needs to be improved.)\n",
              "However, the calculation of the kernel sizes and locations is implemented by this cpp function and the key logic is actually in the calls to the functions start_index and end_index, which define the location and offset of the kernels.\n",
              "I believe this Python code re-implements that code and shows how kernels are calculated:\n",
              "from typing import List\n",
              "import math\n",
              "def kernels(ind,outd) - List:\n",
              "    \"\"\"Returns a List [(kernel_offset_start,kernel_length)] defining all the pooling kernels for a 1-D adaptive pooling layer that takes an input of dimension `ind` and yields an output of dimension `outd`\"\"\"\n",
              "    def start_index(a,b,c):\n",
              "        return math.floor((float(a) * float(c)) / b)\n",
              "    def end_index(a,b,c):\n",
              "        return math.ceil((float(a + 1) * float(c)) / b)\n",
              "    results = []\n",
              "    for ow in range(outd):\n",
              "        start = start_index(ow,outd,ind)\n",
              "        end = end_index(ow,outd,ind)\n",
              "        sz = end - start\n",
              "        results.append((start,sz))\n",
              "    return results\n",
              "\n",
              "def kernel_indexes(ind,out) - List:\n",
              "    \"\"\"Returns a List [[*ind]] containing the indexes of the pooling kernels\"\"\"\n",
              "    startsLengths = kernels(ind,out)\n",
              "    return [list(range(start,start+length)) for (start,length) in startsLengths]\n",
              "\n",
              "Here are the key points to notice.\n",
              "First, it matters a lot whether the input dimension (ind) is an integer multiple of the output dimension (outd).\n",
              "Second, when this is the case, then the adaptive layer's kernels are equally-sized and non-overlapping, and are exactly what would be produced by defining kernels and a stride based on the following rule:\n",
              "stride = ind // outd\n",
              "kernel_size = ind - (outd-1)*stride\n",
              "padding = 0\n",
              "\n",
              "In other words, in this case it is possible to reproduce the effect of an adaptive pooling layer by using a non-adaptive pooling layer defined with suitable stride, kernel_size, and padding. (Example further below.)\n",
              "Finally, when instead it is the case that the input size is not an integer multiple of the output size, then PyTorch's adaptive pooling rule produces kernels which overlap and are of variable size.\n",
              "Since the non-adaptive pooling API does not allow for variably-sized kernels, in this case it seems to me there is no way to reproduce the effect of adaptive pooling by feeding suitable values into a non-adaptive pooling layer.\n",
              "Here's an example which shows both cases. This helper function lets us compare what's happening with adapative average pooling layer and an ordinary average pooling layer which uses fixed stride and kernel:\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "def compare1DAdaptivity(ind,outd,inputpattern):\n",
              "    c = 1\n",
              "    padding = 0\n",
              "\n",
              "    input = torch.Tensor(inputpattern).view(1,c,ind)\n",
              "\n",
              "    stride = ind // outd\n",
              "    kernel_size = (ind - (outd-1)*stride)\n",
              "    avg_pool = nn.AvgPool1d(stride=stride,kernel_size=kernel_size,padding=padding)\n",
              "    avg_out = avg_pool(input)\n",
              "\n",
              "    adap_avg_pool = torch.nn.AdaptiveAvgPool1d(outd)\n",
              "    adap_avg_out = adap_avg_pool(input)\n",
              "    \n",
              "    try:\n",
              "        equal_output = torch.allclose(avg_out,adap_avg_out)\n",
              "    except:\n",
              "        equal_output = False\n",
              "\n",
              "    print(\"input.shape: {}\".format(input.shape))\n",
              "    print(\"in_dims: {}\".format(ind))\n",
              "    print(\"out_dims: {}\".format(outd))\n",
              "    print(\"\")\n",
              "    print(\"AAL strides: {}\".format(stride))\n",
              "    print(\"AAL kernel_sizes: {}\".format(kernel_size))\n",
              "    print(\"AAL pad: {}\".format(padding))\n",
              "    print(\"\")\n",
              "    print(\"outputs equal: {}\".format(equal_output))\n",
              "    print(\"\")\n",
              "    print(\"AAL input - output: {} - {}\".format(input,avg_out))\n",
              "    print(\"adap input - output: {} - {}\".format(input,adap_avg_out))\n",
              "    return equal_output\n",
              "\n",
              "So, to give an example of the first case, where the input dimension is a multiple of the output dimension, we can go from 6 to 3. We can see that the approximate adaptive layer and the true adaptive layer give the same output:\n",
              "compare1DAdaptivity(6,3,[1,0,0,0,0]) # = Tue\n",
              "AAL input - output: tensor([[[1., 0., 0., 0., 0., 0.]]]) - tensor([[[0.5000, 0.0000, 0.0000]]])\n",
              "adap input - output: tensor([[[1., 0., 0., 0., 0., 0.]]]) - tensor([[[0.5000, 0.0000, 0.0000]]])\n",
              "\n",
              "However, this no longer works if we go from 5 to 3.\n",
              "compare1DAdaptivity(5,3,[1,0,0,0,0]) # = False\n",
              "AAL input - output: tensor([[[1., 0., 0., 0., 0.]]]) - tensor([[[0.3333, 0.0000, 0.0000]]])\n",
              "adap input - output: tensor([[[1., 0., 0., 0., 0.]]]) - tensor([[[0.5000, 0.0000, 0.0000]]])\n",
              "\n",
              "But we can reproduce the result of the adaptive layers by manually computing over the indexes:\n",
              "t = [1,0,0,0,0]; [sum( [t[x] for x in xs] ) / len(xs) for xs in kernel_indexes(5,3)]\n",
              "# = [0.5,0.0,0.0]\n",
              "\n",
              " <P> Any time you write a = sign in Python you are creating a new object.\n",
              "\n",
              "So the right-hand side of your expression in the second case uses the original a and then evaluates to a new object i.e. a + 1, which replaces this original a. b still points to the memory location of the original a, but now a points to a new object in memory.\n",
              "\n",
              "In other words, in a = a + 1, the expression a + 1 creates a new object, and then Python assigns that new object to the name a.\n",
              "\n",
              "Whereas, with a += 1, Python calls a's in-place addition method (__iadd__) with the argument 1.\n",
              "\n",
              "The numpy code: np.add(a,1,out=a) , in the first case takes care of adding that value to the existing array in-place.\n",
              "\n",
              "(Thanks to @Engineero and @Warren Weckesser for pointing out these explanations in the comments)\n",
              " <P> Tensor.bernoulli_ Tensor.bernoulli_ fills each location of self with an independent sample from Bernoulli(p). self can have integral dtype. p should either be a scalar or tensor containing probabilities to be used for drawing the binary random number. If it is a tensor, the ith element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i]). In this case p must have floating point dtype. See also bernoulli() and torch.bernoulli() <P> Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col0\" class=\"data row53 col0\" >PyTorch with yolov5: color channel and result display</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col1\" class=\"data row53 col1\" >You can't just reshape the image you need to transpose the channels. As a remark for the future, if you get a stripy result like you did it's most likely some permutation/transposition or reshaping operation that's not correct.\n",
              "\n",
              "Other than that I also scaled the input image to [0, 1] to show it properly. Below is the working code:\n",
              "\n",
              "import numpy as np\n",
              "import torch\n",
              "Import torchvision.transforms as transforms\n",
              "\n",
              "import torch.nn as nn\n",
              "import matplotlib.pyplot as plt\n",
              "import imageio\n",
              "import sys</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col2\" class=\"data row53 col2\" >The following worked: result = model(cv2.cvtColor(scr, cv2.COLOR_BGR2RGB), size=400)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row53_col3\" class=\"data row53 col3\" ><P> You are right. There are several setting for your ConvTranpose layer to achieve the same result.\n",
              "The same goes for the opposite direction: you can get the same output shape using different settings for a Conv layer.\n",
              "This tutorial with link \"http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic\" and also these visualizations with link \"https://github.com/vdumoulin/conv_arithmetic\" might make things clearer.\n",
              "This is what I want.\n",
              "Thanks so much !!!\n",
              "If this is of any help, I wrote a post on ConvTranspose1d which is extendable to 2d simply:\n",
              "\n",
              "\n",
              "\n",
              "Medium – 27 Jul 18 with link \"https://medium.com/.pdp/how-pytorch-transposed-convs1d-work-a7adac63c4a5\"\n",
              "\n",
              "\n",
              "\n",
              "How PyTorch Transposed Convs1D Work – Santi Pdp – Medium with link \"https://medium.com/.pdp/how-pytorch-transposed-convs1d-work-a7adac63c4a5\"\n",
              "WARNING: I’ll be assuming you know what neural networks and convolutional neural networks are. Also, this post is written in PyTorch…\n",
              "Reading time: 9 min read\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "If anyone in the community sees a misunderstanding I’ll be greateful about pointers  <P> The issue with your code is this line\n",
              "\n",
              "image_d = torch.FloatTensor(np.asarray(A.reshape(1, 3, A.shape[0] , A.shape[1])))\n",
              "\n",
              "\n",
              "You can't just reshape the image you need to transpose the channels. As a remark for the future, if you get a stripy result like you did it's most likely some permutation/transposition or reshaping operation that's not correct.\n",
              "\n",
              "Other than that I also scaled the input image to [0, 1] to show it properly. Below is the working code:\n",
              "\n",
              "import numpy as np\n",
              "import torch\n",
              "import torchvision\n",
              "import torchvision.transforms as transforms\n",
              "import torch.nn as nn\n",
              "import matplotlib.pyplot as plt\n",
              "import imageio\n",
              "import sys\n",
              "\n",
              "A = imageio.imread('LiT.png')\n",
              "# Define how the convolution operation works\n",
              "conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
              "\n",
              "# from [H, W, C] to [C, H, W]\n",
              "transposed_image = A.transpose((2, 0, 1))\n",
              "# add batch dim\n",
              "transposed_image = np.expand_dims(transposed_image, 0)\n",
              "\n",
              "image_d = torch.FloatTensor(transposed_image)\n",
              "fc = conv2(image_d)\n",
              "fc1 = fc.permute(0, 2, 3, 1)[0]\n",
              "result = fc1.data.numpy()\n",
              "max_ = np.max(result)\n",
              "min_ = np.min(result)\n",
              "result -= min_\n",
              "result /= max_\n",
              "\n",
              "plt.figure(figsize=(16,8))\n",
              "plt.subplot(1,2,1)\n",
              "plt.imshow(A)\n",
              "plt.subplot(1,2,2)\n",
              "plt.imshow(result)\n",
              "\n",
              "plt.show()\n",
              "\n",
              " <P> Hey, you want to go from 41x41 to 321x321? If you upsample 8 times with conv transpose 2d you get 328. Then a conv filter of size 8x8, you should get 321x321\n",
              "Hey! Thanks for the suggestion. This would work for my training scheme where I know that my input is 321x321.\n",
              "But in the inference phase, the input image size is not fixed and I want a cleaner way where I can upsample by simply specifying the target size.\n",
              "For example, this is the behavior I want\n",
              "\n",
              "feature_map = net(input) // The size of the feature map depends on the size of input\n",
              "output = torch.nn.Upsample(size = input_size) // input_size is the size of input\n",
              "\n",
              "Please write back if I didn’t make something clear. <P> From the docs:\n",
              "\n",
              "The input dimensions are interpreted in the form: mini-batch x\n",
              "channels x [optional depth] x [optional height] x width.\n",
              "\n",
              "Currently the first 28 in your shape, and the 14 in the output size, are interpreted as the number of channels/colors, not the image height. Therefore the scaling does not happen in that dimension. Unsqueeze your input to be of shape (1, 1, 28, 28) to get the correct behavior.\n",
              " <P> The first convolution doesn't use padding.\n",
              "\n",
              "nn.Conv2d(3, 64, kernel_size=3, bias=False)\n",
              "\n",
              "\n",
              "Therefore the spatial dimensions will be reduced by 2. In the case of CIFAR the input has size [batch_size, 3, 32, 32] and the output would be [batch_size, 64, 30, 30]. For all other convolutions the spatial dimensions are unchanged, but the max pooling will halve them (integer division). Since you have 5 max pooling layers, the height/width change as follows:\n",
              "\n",
              "30 - 15 - 7 - 3 - 1 - 0 (error)\n",
              "\n",
              "\n",
              "In the Keras version you are using padding in the max pooling layers as well, which is presumably only applied if the input is not strictly divisible by 2. If you wanted to replicate that behaviour in PyTorch you would have to set the padding of the max pooling layers manually for the ones that receive an input with an odd height/width.\n",
              "\n",
              "I don't think that using padding in max pooling with a kernel size of 2 is beneficial, especially as you are using ReLU before them, meaning that the padded max pooling just preserves the border values (it's a different story for bigger kernel sizes).\n",
              "\n",
              "The simplest solution is to use padding in the first convolution, such that the spatial dimensions are unchanged:\n",
              "\n",
              "nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
              "\n",
              "\n",
              "Another option would be to remove the last max pooling layer, since the height/width are already 1, but that also means that the last three convolutions are applied to only one value, since the input sizes would be [batch_size, 512, 1, 1], which kind of defeats the purpose of using a convolution.\n",
              " <P> decoder is returning a linear output of shape BATCH_SIZE x 355008. First, we need to reshape the second dimension to 3 dimensions of shape 3 x 344 x 344 before applying transpose on it. Replacing decodeddd with following should do the trick:\n",
              "\n",
              "decodeddd = np.transpose(decoded.cpu()[0].view(3, 344, 344),(1,2,0))\n",
              "\n",
              " <P> You are not upsampling enough via ConvTranspose2d, shape of your encoder is only 1 pixel (width x height), see this example:\n",
              "\n",
              "import torch\n",
              "\n",
              "layer = torch.nn.ConvTranspose2d(8, 64, kernel_size=3, stride=1)\n",
              "print(layer(torch.randn(64, 8, 1, 1)).shape)\n",
              "\n",
              "\n",
              "This prints your exact (3,3) shape after upsampling.\n",
              "\n",
              "You can:\n",
              "\n",
              "\n",
              "Make the kernel smaller - instead of 4 in first Conv2d in decoder use 3 or 2 or even 1\n",
              "Upsample more, for example: torch.nn.ConvTranspose2d(8, 64, kernel_size=7, stride=2) would give you 7x7\n",
              "What I would do personally: downsample less in encoder, so output shape after it is at least 4x4 or maybe 5x5. If you squash your image so much there is no way to encode enough information into one pixel, and even if the code passes the network won't learn any useful representation.\n",
              "\n",
              " <P> You can use nn.AdaptiveAvgPool2d to reduce the spatial dimensions to 1x1(HxW) and then use 1x1 convolution to bring num_filters = num_classes. Use this after the last conv output from vgg.\n",
              "self.classifier = nn.Sequential(\n",
              "          nn.AdaptiveAvgPool2d((1, 1)), \n",
              "          nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1),\n",
              "          nn.ReLU(inplace=True)\n",
              "         )\n",
              "\n",
              "Given what you have done above, I am not sure how you were able to calculate loss since the output will be num_classes, H//32, W//32. However one important thing to note is, do not use nn.Softmax at the end of the network when using nn.CrossEntropy(it combines softmax and nll loss). That can cause problems.\n",
              "\n",
              "thanks for the tip, I have removed Softmax from classification layer. Also, I found my mistake: it was a problem of weight initialization of last 1x1 convolution layer. Now the network is learning properly.\n",
              "many thanks <P> You are asking two different questions, I will try to answer both.\n",
              "\n",
              "Indeed, you should first reshape to (c, h, w) where c is the channel dimension In most cases, you will need that extra dimension because most 'image' layers are built to receive 3d dimensional tensors - not counting the batch dimension - such as nn.Conv2d, BatchNorm2d, etc... I don't believe there's anyways around it, and doing so would restrict yourself to one-layer image datasets.\n",
              "You can broadcast to the desired shape with torch.reshape or Tensor.view:\n",
              "X = X.reshape(1, *X.shape)\n",
              "\n",
              "Or by adding an additional dimension using torch.unsqueeeze:\n",
              "X.unsqueeze(0)\n",
              "\n",
              "\n",
              "About normalization. Batch-normalization and dataset-normalization are two different approaches.\n",
              "The former is a technique that can achieve improved performance in convolution networks. This kind of operation can be implemented using a nn.BatchNorm2d layer and is done using learnable parameters: a scale factor (~ std) and a bias (~ mean). This type of normalization is applied when the model is called and is applied per-batch.\n",
              "The latter is a pre-processing technique which allows making different features have the same scale. This normalization can be applied inside the dataset per-element. It requires you measure the mean and standard deviation of your training set.\n",
              "\n",
              "\n",
              " <P> First, the weird shape you get for your mean and std ([600]) is unsuprising, it is due to your data having the shape [8, 600, 800, 3]. Basically, the channel dimension is the last one here, so when you try to flatten your images with\n",
              "# (N, 600, 800, 3) - [view] - (N, 600, 2400 = 800*3)\n",
              "images_data = images.view(batch_samples, images.size(1), -1)\n",
              "\n",
              "You actually perform a weird operation that fuses together the width and channel dimensions of your image which is now [8, 600, 2400]. Thus, applying\n",
              "# (8, 600, 2400) - [mean(2)] - (8, 600) - [sum(0)] - (600) \n",
              "data.mean(2).sum(0)\n",
              "\n",
              "Creates a tensor of size [600] which is what you indeed get.\n",
              "There are two quite simple solutions :\n",
              "Either you start by permuting the dimensions to make the 2nd dimension the channel one :\n",
              "batch_samples = images.size(0)\n",
              "# (N, H, W, C) - (N, C, H, W)\n",
              "reordered = images.permute(0, 3, 1, 2)\n",
              "# flatten image into (N, C, H*W)\n",
              "images_data = reordered.view(batch_samples, reordered.size(1), -1)\n",
              "# mean is now (C) = (3)\n",
              "mean += images_data.mean(2).sum(0)\n",
              "\n",
              "Or you changes the axis along which to apply mean and sum\n",
              " batch_samples = images.size(0)\n",
              "# flatten image into (N, H*W, C), careful this is not what you did\n",
              "images_data = images.view(batch_samples, -1, images.size(1))\n",
              "# mean is now (C) = (3)\n",
              "mean += images_data.mean(1).sum(0)\n",
              "\n",
              "Finally, why did dataloaderand trainloader behave differently ? Well I think it's because one is using dataset while the other is using transformedDataset. In TransformedDataset, you apply the toTensortransform which cast a PIL image into a torch tensor, and I think that pytorch is smart enough to permute your dimensions during this operation (and put the channels in the second dimension). In other word, your two datasets just do not yield images with identical format, they differ by a permutation of the axis.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col0\" class=\"data row54 col0\" >Hyper-parameter tuning and Over-fitting with Feed-Forward Neural Network - Mini-Batch Epoch and Cross Validation</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col1\" class=\"data row54 col1\" >If you don't split your data previously, the trainloader will use the entire train folder. You can specify the amount of training by splitting your data, see:\n",
              "\n",
              "from torchvision import datasets\n",
              "\n",
              "# convert data to a normalized torch.FloatTensor\n",
              "transform = transforms.Compose([ transforms.ToTensor(),\n",
              "    transforms.Normalize((0.5, 0.5), (0. 5, 0))\n",
              ")\n",
              "For each batch val_loss you can choose not to run the validation step for each of the batch (then you'd have to do it 1000 times!) but</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col2\" class=\"data row54 col2\" >•\tk-fold cross validation is generally useful when you have a very small dataset. Thus, if you are training on a dataset like CIFAR10 (which is large, 60000 images), then you don't require k-fold cross validation.\n",
              "•\tThe idea of k-fold cross validation is to see how model performance (generalization) varies as different subsets of data is used for training and testing. This becomes important when you have very less data. However, for large datasets, the metric results on the test dataset is enough to test the generalization of the model.\n",
              "•\tThus, whether you require k-fold cross validation depends on the size of your dataset. It does not depend on what model you use.\n",
              "Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.\n",
              "•\tSo, yes, mini-batch training will have a regularizing effect (reduce overfitting) to some extent.\n",
              "•\tThere is no inbuilt hyperparameter tuning (at least at the time of writing this answer), but many developers have developed tools for this purpose .You can find more such tools by searching for them. </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row54_col3\" class=\"data row54 col3\" ><P> The question is way too broad, I think. However, this is a common practice, especially in case of a small training set. I would rank possible methods like this:\n",
              "\n",
              "\n",
              "smaller learning rate\n",
              "more/different data augmentation\n",
              "add noise to train set (related to data augmentation, indeed)\n",
              "fine-tune on subset of the training set.\n",
              "\n",
              "\n",
              "The very last one is indeed a very powerful method to finalize the model that performs poor on some corner cases. You can then make a 'difficult' train subset in order to bias model towards it. I personally use it very often. \n",
              " <P> Well, you're right that's the way to do it \"run the whole validation step after each training batch and keeping track of those\" and also as you've thought it's pretty time-consuming and would be overkill. However, If that's something you really need then there's a way you can do it. What you can do is, let's say you've 1000  batches in your data. Now to calculate per batch val_loss you can choose not to run the validation step for each of the batch (then you'd have to do it 1000 times!) but for a small subset of those batches, let's say 50/100 (choose as you please or find feasible). Now, you can use some statistical power so that your calculation for 50/100 batches becomes very very close to that of 1000 batches (meaning this val_loss for a small number of batches must be as close as to those of 1000 batches if you had calculated that), so to achieve it you can introduce some randomness in your batch selection.\n",
              "This means you randomly select 100 batches from your 1000 batches for which you'll run the validation step.\n",
              " <P> Your loss is fluctuating means your network is not powerful enough to extract meaningful embeddings. I can recommend trying one of these few things.\n",
              "\n",
              "\n",
              "Add more layers.\n",
              "Use a smaller learning rate.\n",
              "Use a larger dataset or use a pre-trained model if you only have a small dataset.\n",
              "Normalize your dataset.\n",
              "Shuffle training set.\n",
              "Play with hyperparameters. \n",
              "\n",
              " <P> your model's overfitting wont depend on the no. of epochs you set.....\n",
              "since you hav made a val split in your data, make sure that your train loss - val loss OR train acc - val acc is nearly the same.This will assure that your model is not overfitting\n",
              " <P> I have some suggestions, what I would try, maybe you've already done it:\n",
              "\n",
              "increase the probability of dropout, that could decrease overfitting,\n",
              "I did not see or I missed it but if you don't do it, shuffle all the samples,\n",
              "there is not so much data, did you thought about using other NN to generate more data of the classes which are having the least score? I am not sure if it is the case here but even randomly rotating, scaling the images can produce more training examples,\n",
              "another approach you can take, if you haven't done it already, use transfer learning using another popular CNN net and see how it is doing the job, then you can have some comparison, whether it is something wrong with your architecture or it's lack of examples :)\n",
              "I know these are just suggestions but maybe, if you haven't try some of them, they will bring you closer to the solution.\n",
              "Good luck!\n",
              "\n",
              " <P> If you don't split your data previously, the trainloader will use the entire train folder. You can specify the amount of training by splitting your data, see:\n",
              "\n",
              "from torchvision import datasets\n",
              "\n",
              "# convert data to a normalized torch.FloatTensor\n",
              "transform = transforms.Compose([\n",
              "    transforms.ToTensor(),\n",
              "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
              "    ])\n",
              "\n",
              "# choose the training and test datasets\n",
              "train_data = datasets.CIFAR10('data', train=True,\n",
              "                              download=True, transform=transform)\n",
              "test_data = datasets.CIFAR10('data', train=False,\n",
              "                             download=True, transform=transform)\n",
              "valid_size = 0.2\n",
              "\n",
              "# obtain training indices that will be used for validation\n",
              "num_train = len(train_data)\n",
              "indices = list(range(num_train))\n",
              "np.random.shuffle(indices)\n",
              "split = int(np.floor(valid_size * num_train))\n",
              "train_idx, valid_idx = indices[split:], indices[:split]\n",
              "\n",
              "# define samplers for obtaining training and validation batches\n",
              "train_sampler = SubsetRandomSampler(train_idx)\n",
              "valid_sampler = SubsetRandomSampler(valid_idx)\n",
              "\n",
              "# prepare data loaders (combine dataset and sampler)\n",
              "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
              "    sampler=train_sampler, num_workers=num_workers)\n",
              "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
              "    sampler=valid_sampler, num_workers=num_workers)\n",
              "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
              "    num_workers=num_workers)\n",
              "\n",
              "\n",
              "The Batch size is the numbers of files that you catch by iteration (epoch). For example, if your training_size is 1000, and you have a batch_size of 10, then each epoch would contain 100 iterations.\n",
              "\n",
              "The number of workers is used to preprocess the data of batch. More workers will consume more memory usage and workers are helpful to speed up the Input and Output process.\n",
              "num_workers = 0 means that will do the data loading when needed,\n",
              "num_workers > 0 means your data will be preprocessed with the number of workers you defined.\n",
              " <P> This is a borderline question; you should still be able to extract this understanding from the basic literature ... eventually.\n",
              "Your insight is exactly correct: you are measuring execution time per epoch, rather than total Time-to-Train (TTT).  You have also carried the generic \"smaller batches\" advice ad absurdum: a batch size of 1 is almost guaranteed to be sub-optimal.\n",
              "The mechanics are very simple at a macro level.\n",
              "With a batch size of 60k (the entire training set), you run all 60k images through the model, average their results, and then do one back-propagation for that average result.  This tends to lose the learning you can get from focusing on little-seen features.\n",
              "With a batch size of 1, you run each image individually through the model, average the one result (a very simple operation :-) ), and do a back propagation.  This tends to over-emphasize individual effects, especially retaining superstitious effects from each single image.  It also gives too much weight to the initial assumptions of the first few images.\n",
              "The most obvious effect of the tiny batch size is that you're doing 60k back-props instead of 1, so each epoch takes much longer.\n",
              "\n",
              "Either of these approaches is an extreme case, usually absurd in application.\n",
              "You need to experiment to find the \"sweet spot\" that gives you the fastest convergence to acceptable (near-optimal) accuracy.  There are a few considerations in choosing your experimental design:\n",
              "\n",
              "Memory size: you want to be able to ingest the entire batch into memory at once.  This allows your model to pipeline reading and processing.  If you exceed available memory, you will lose a lot of time to swapping.  If you under-use the memory, you leave some potential performance untapped.\n",
              "Processors: if you're on a multi-processor chip, you want to keep them all busy.  If you care to assign processors through your OS controls, you'll also want to play with how many to assign to model computation, and how many to assign to I/O and system use.  For instance, in one project I did, our group found that our 32 cores were best used with 28 allocated to computation, 4 reserved for I/O and other system functions.\n",
              "Scaling: some characteristics work best in powers of 2.  You may find that a batch size that is 2^n or 3 * 2^n for some n, works best, simply because of block sizes and other system allocations.\n",
              "\n",
              "The experimental design that has worked best for me over the years is to start with a power of 2 that is roughly the square root of the training set size.  For you, there's an obvious starting guess of 256.  Thus, you'd run experiments at perhaps 64, 128, 256, 512, and 1024.  See which ones give you the fastest convergence.\n",
              "Then do one step of refinement, using that factor of 3.  For instance, if you find that the best performance comes at 128, also try 96 and 192.\n",
              "You will likely see very little difference between your \"sweet spot\" and the adjacent batch sizes; this is the nature of most complex information systems.\n",
              " <P> When the validation loss is larger than the training loss, It is usually a sign of overfitting. There area few things you can do:\n",
              "\n",
              "\n",
              "Add Dropout of Batch Normalisation:\n",
              "\n",
              "\n",
              "This makes the model more robust.\n",
              "\n",
              "\n",
              "Make the model deeper:\n",
              "\n",
              "\n",
              "Add more layers to the model for a better comprehension of the patterns.\n",
              "\n",
              "\n",
              "Use better optimizers:\n",
              "\n",
              "\n",
              "Adaptive optimizers such as Adam, Adagrad and RMSprop are usually effective.\n",
              " <P> \n",
              "Try augmentations like RandomHorizontalFlip, RandomResizedCrop,\n",
              "RandomRotate, Normalize etc from torchvision transforms. These always help a lot in classification problems.\n",
              "Label smoothing and/or Mixup precision training.\n",
              "Simply try using a more optimized architecture, like EfficientNet.\n",
              "Instead of OneCycle, a longer, more manual training approach may help. Try Stochastic Gradient Descent with a weight decay of 5e-4 and a Nesterov Momentum of 0.9. Use Warmup Training of around 1-3 epochs, and then regular training of around 200 epochs. You could set a manual learning rate schedule or cosine annealing or some other scheme. This entire method will consume a lot more time and effort than the usual onecycle training, and should be explored only if other methods don't show considerable gains.\n",
              "\n",
              " <P> You have a large gap between training and validation performance, and between validation and test performance. There are two issues to explore:\n",
              "\n",
              "\n",
              "Differences in the distribution. We assume that train / val / test sets are all drawn from the same distribution, and so have similar characteristics. A well trained model should perform equally well on the val and test datasets. If you are dataset really is only 10 samples for test and 18 for val, there is a high chance that the samples selected will skew one/both of these datasets, so that they no longer have similar characteristics. Therefore the difference between your val and test performance could just be chance: Your test set just happens to be much harder. You could test this by manual inspection.\n",
              "Overfitting to val: However, I think it is more likely that you have experimented with different architectures, training regimes, etc, and have tweaked parameters to get the best performance on you validation set. This means that you have overfit your model to your val set. The test set is a truer reflection of your model's accuracy.\n",
              "\n",
              "\n",
              "Your training accuracy is very high for both problems, and there is a large gap between training and validation performance. You are therefore overfitting to the training data, so need to train less, or introduce more stringent regularisation.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col0\" class=\"data row55 col0\" >nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col1\" class=\"data row55 col1\" >This OOM exception comes from the Python API implement of Conv2d_weight actually. In backprop weight calculation, the output gradients need to be expanded with output channel times. When default CUDA implement this with data prefetch block and block (not allocate more memory), Python API uses a repeat that will allocate a huge size of memory on output gradens tensor with unnecessary duplication of data. You can easily fix this by convert the repeat into a loop function at Conv2D_weight.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col2\" class=\"data row55 col2\" >It seems your performances gets a hit by handling denormal values.\n",
              "Try to set [torch.set_flush_denormal(True)]( and profile the code again.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row55_col3\" class=\"data row55 col3\" ><P> Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\n",
              "\n",
              ">>> from random import *\n",
              ">>> n = 100000\n",
              ">>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\n",
              ">>> V = torch.randn(n)\n",
              ">>> size = torch.Size([1000, 1000, 1000])\n",
              ">>> S = torch.sparse_coo_tensor(I.t(), V, size)\n",
              "\n",
              ">>> %timeit S.coalesce()\n",
              "10 loops, best of 3: 30.7 ms per loop\n",
              "\n",
              ">>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\n",
              ">>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\n",
              "100 loops, best of 3: 9.59 ms per loop\n",
              " <P> The difference is well below the order of 1e-5, which is expected for fp32 arithmetics. Afterall, floating point computation is not exact. <P> Hi, This OOM exception comes from the python api implement of conv2d_weight actually. In backprop weight calculation, the output gradients need to be expanded with output channel times. When default cudnn implement this with data prefetch block and block (not allocate more memory), python api uses a repeat that will allocate a huge size of memory on output gradients tensor with unnecessary duplication of data. you can easily fix this by convert the repeat into a loop function at conv2d_weight. <P> \n",
              " this is a known limitation of amp. You can see this issue on their repo for more info: https://github.com/NVIDIA/apex/issues/503 <P> I have figured something addition to @unlut's post.\n",
              "\n",
              "The convolution method are in separate files for  different implementations. You may find cudnn_convoluton_backward or mkldnn_convolution_backward easily. One tricky thing is that the final native fall function is hard to find. It is because currently Pytorch Teams are porting Thnn function to ATen, you could refer to PR24507.\n",
              "\n",
              "The native function could be find as thnn_con2d_backward.\n",
              "\n",
              "The convolution backward is not calculated via autograd, rather, there must a conv_backward function and this must be recorded in derivatives.yaml. If you want to find specific backward function, refer to that file is a good start.\n",
              "\n",
              "About this code, if you want to directly call thnn_backward function, you need to explicitly construct finput and fgrad_input. These are two empty tensor offering as a buffer.\n",
              "\n",
              "\n",
              "at::Tensor finput = at::empty({0},input.options()); \n",
              "at::Tensor fgrad_input = at::empty({0}, input.options());\n",
              "auto kernel_size = weight.sizes().slice(2);\n",
              "auto result = at::thnn_conv2d_backward(grad_output, input, weight,kernel_size , stride, padding, \n",
              "finput, fgrad_input, output_mask);\n",
              "\n",
              " <P> Ran into the same issue. This seems to be a feature of underlying algorithm (GESDD) and can be fixed by improving condition number to be better than 8000. GESVD seems to not have this problem, so if you care about tiny singular values, a workaround is to use scipy with lapack_driver `gesvd` \n",
              " \n",
              " https://github.com/pytorch/pytorch/issues/25978 <P> The differences you are seeing are approx. 1e-6 which comes most likely due to the limited floating point precision of float32.\n",
              "The order of operations might yield different results as seen here:\n",
              "x = torch.randn(10, 10, 10)\n",
              "s1 = x.sum()\n",
              "s2 = x.sum(0).sum(0).sum(0)\n",
              "print((s1 - s2).abs().max())\n",
              "> tensor(3.8147e-06)\n",
              "\n",
              "I assume the difference in CPU architecture between both posted CPUs might explain the difference, but that’s just by best guess.\n",
              "Also, the implementation of the pseudo-random number generator might be different for different hardware devices (e.g. CPU vs. GPU).\n",
              "Thanks for your answer, I tried increasing model complexity and printing out the maximum difference between GPU and CPU results and  you are correct, the highest difference I received was 2.3543834686279297e-06.\n",
              "Architecture difference is also a good guess. <P> Adam is more stateful than SGD, so it is expected that it uses more memory (proportional to the total size of the optimized parameters). <P> This problem comes from CuDNN.\n",
              "\n",
              "CuDNN has multiple algorithms to do convolutions, that they can choose from. They have a heuristic function that picks a particular algorithm on a particular GPU, for specific sizes of convolution / dilation etc.\n",
              "\n",
              "In CuDNN 7.4, which is the latest version, and which is what PyTorch ships with, it looks like they introduced performance regression in this version compared to CuDNN 7.1 (which PyTorch used before).\n",
              "\n",
              "`torch.backends.cudnn.benchmark=True` tells PyTorch / CuDNN to try every algorithm for a given {input size, kernel size, kernel stride, dilation, padding}, and pick the fastest.\n",
              "\n",
              "The reason it fixes the problem for @knazeri is because they must be doing something like image classification, where the `input size` does not change across iterations. So, CuDNN only searches all algorithms (for the fastest algorithm) once in training -- when it encounters `input size` for the first time.\n",
              "\n",
              "The reason it makes @zimenglan-sysu-512 's problem much worse is because they must be doing object detection or something similar, where images of different `input size` are passed in at every iteration, so CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer. <P> The g4dn.8xlarge use NVIDIA T4 GPU which have 16GB of RAM. So I think PyTorch shows the memory of the GPU, not the instance.\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col0\" class=\"data row56 col0\" >Multi label classification in pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col1\" class=\"data row56 col1\" >The best metric to evaluate your resulting algorithm, it's up to you, what are you looking for. But you may want to investigate ideas like precision and recall or f1 score. Personally, I would probably pick the top 3 for each genre (since that's at max number of genres assigned to each poster) and look if the ones to be expected show up with high probability and if the unexpected ones (in case of a movie with 2 \"ground truth\" genres) show at the last places, with significantly less probability assigned.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col2\" class=\"data row56 col2\" >You are looking for torch.nn.BCELoss. Here's example code:\n",
              "\n",
              "import torch\n",
              "\n",
              "batch_size = 2\n",
              "num_classes = 11\n",
              "\n",
              "loss_fn = torch.nn.BCELoss()\n",
              "\n",
              "outputs_before_sigmoid = torch.randn(batch_size, num_classes)\n",
              "sigmoid_outputs = torch.sigmoid(outputs_before_sigmoid)\n",
              "target_classes = torch.randint(0, 2, (batch_size, num_classes))  # randints in [0, 2).\n",
              "\n",
              "loss = loss_fn(sigmoid_outputs, target_classes)\n",
              "\n",
              "# alternatively, use BCE with logits, on outputs before sigmoid.\n",
              "loss_fn_2 = torch.nn.BCEWithLogitsLoss()\n",
              "loss2 = loss_fn_2(outputs_before_sigmoid, target_classes)\n",
              "assert loss == loss2</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row56_col3\" class=\"data row56 col3\" ><P> \n",
              "You're right, you're looking to perform binary classification (is poster X a drama movie or not? Is it an action movie or not?) for each poster-genre pair. BinaryCrossEntropy(WithLogits) is the way to go.\n",
              "Regarding the best metric to evaluate your resulting algorithm, it's up to you, what are you looking for. But you may want to investigate ideas like precision and recall or f1 score. Personally, I would probably pick the top 3 for each genre (since that's at max number of genres assigned to each poster) and look if the ones to be expected show up with high probability and if the unexpected ones (in case of a movie with 2 \"ground truth\" genres) show at the last places, with significantly less probability assigned.\n",
              "\n",
              " <P> There are a few open-ended questions in how this is implemented. For instance, do you want each class to be equally represented regardless of that class's actual frequency? Note that this may give better performance on minority classes at the expense of performance on majority classes.\n",
              "Also, do you want each example to be used at most once per epoch, or at least once per epoch?\n",
              "In any case, this will likely be difficult to accomplish with the standard getitem method because it returns an example with no regard for the other examples returned in the same batch. You'll likely need to define a custom dataloader object to ensure good data distribution and usage properties, which is a bit unfortunate because pytorch's dataloader and dataset objects work together quite nicely and efficiently for most simple use cases. Perhaps someone else has a solution that uses these objects.\n",
              "Here's a solution that uses random sampling with replacement after each batch, so there's no guarantee that every example will be used. Also, it uses looping so you could probably do better with parallelization.\n",
              "class ImageFolderLoader(Dataset):\n",
              "  def __init__(self, root, transform=None, loader=default_loader,):\n",
              "    classes, class_to_idx = find_classes(root)\n",
              "    imgs = make_dataset(root, class_to_idx)\n",
              "\n",
              "    #currently, imgs items are of the form (path,class)\n",
              "\n",
              "    data_dict = {}\n",
              "    for item in imgs:\n",
              "       cls = item[1]\n",
              "       if cls not in data_dict.keys():\n",
              "           data_dict[cls] = [item]\n",
              "       else:\n",
              "           data_dict[cls].append(item)  \n",
              "   \n",
              "    # each class is the key for a list of all items belonging to that class\n",
              "    self.data_dict = data_dict \n",
              "\n",
              "    self.root = root\n",
              "    self.imgs = imgs\n",
              "    self.classes = classes\n",
              "    self.class_to_idx = class_to_idx\n",
              "    self.transform = transform\n",
              "    self.loader = loader\n",
              "    \n",
              "  def get_batch(self):\n",
              "    img_batch = []\n",
              "    label_batch = []\n",
              "    \n",
              "    classes = random.sample((0,300),15) \n",
              "    for cls in classes:\n",
              "        class_data = self.data_dict[cls]\n",
              "        selection = random.sample((0,len(class_data),5)\n",
              "        for idx in selection:\n",
              "           img = self.loader(os.path.join(self.root, class_data[idx][0]))\n",
              "           if self.transform is not None:\n",
              "               img = self.transform(img)\n",
              "           img_batch.append(img)\n",
              "           label_batch.append(cls)\n",
              "   \n",
              "    img_batch = torch.stack(img_batch)\n",
              "    label_batch = torch.stack(label_batch)\n",
              "\n",
              "    return img_batch, label_batch\n",
              "\n",
              "  def __len__(self):\n",
              "    return len(self.imgs)\n",
              "\n",
              " <P> Here is a slightly modified(direct) approach using sklearn's confusion_matrix:-\n",
              "\n",
              "from sklearn.metrics import confusion_matrix\n",
              "\n",
              "nb_classes = 9\n",
              "\n",
              "# Initialize the prediction and label lists(tensors)\n",
              "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
              "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
              "\n",
              "with torch.no_grad():\n",
              "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
              "        inputs = inputs.to(device)\n",
              "        classes = classes.to(device)\n",
              "        outputs = model_ft(inputs)\n",
              "        _, preds = torch.max(outputs, 1)\n",
              "\n",
              "        # Append batch prediction results\n",
              "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
              "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
              "\n",
              "# Confusion matrix\n",
              "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
              "print(conf_mat)\n",
              "\n",
              "# Per-class accuracy\n",
              "class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
              "print(class_accuracy)\n",
              "\n",
              " <P> \n",
              "On top of that, how would I obtain predictions for this?\n",
              "\n",
              "If it's a multilabel task and you are outputting logits (as you are) then simply do:\n",
              "output = model(data.float())\n",
              "labels = output  0\n",
              "\n",
              "\n",
              "point out whats wrong with it?\n",
              "\n",
              "It is hard and opinionated, what I would do in order:\n",
              "\n",
              "validate your data. Your neural network response is the same for every input (given your example output is real). Maybe you are passing the same single sample (though seems unlikely as it's sklearn created data)\n",
              "start simple; no LR scheduler, no weight decay, simple neural network and optimizer only (Adam can stay). Use weight decay if your model is overfitting, it clearly isn't right now.\n",
              "fix your learning rate; it is one of the most important hyperparameters. 1 is probably too high, start with something like 3e-4 or 1e-3.\n",
              "try to overfit (loss ~0.0) on small amount of samples (say 32 samples). If you can't, your neural network probably doesn't have enough capacity or there is an error in your code (didn't spot it from quick glance, besides what I've mentioned above). You should verify input and output shapes are correct and returned values manually (it seems for each sample network returns the same logits?).\n",
              "if you are sure there is no error increase network capacity. Add new hidden layer or two (there is only one) and overfit on single batch. If it's capable go with more data\n",
              "\n",
              "\n",
              "I've used multilabel_soft_margin_loss as the pytorch docs suggest,\n",
              "\n",
              "It is the same thing as using torch.nn.BCEWithLogitsLoss which I think is more common, but that's an addendum.\n",
              " <P> Why would you need a library to do that?\n",
              "\n",
              "Simply pass the same examples through all your neural networks and get the predictions (either logits or probabilities or labels).\n",
              "\n",
              "\n",
              "Hard voting choose the label predicted most often by classifiers. \n",
              "Soft voting, average probabilities predicted by classifiers and choose the label having the highest.\n",
              "Weighted voting - either of the above can be weighted. Just assign weights to each classifier and multiply their predictions by them. Weights are usually normalized to (0, 1] range.\n",
              "\n",
              "\n",
              "In principle you could also sum logits and choose the label with highest.\n",
              "\n",
              "Oh, and weight averaging is different technique and requires you to have the same model and usually is done for the same initialization but at different training timesteps. You can read about it in this blog post.\n",
              " <P> First of all, you should check your extraction technique and whether it works correctly. Rest of the answer assumes this step is done.\n",
              "\n",
              "Distribution of EMNIST and distribution of your extracted data is probably quite different, hence it might be hard to obtain good results.\n",
              "\n",
              "There are some steps you may do to improve the score though.\n",
              "\n",
              "Additional data\n",
              "\n",
              "If you have some way to extract more images of letters and ciphers and label them appropriately you should use it during neural net training.\n",
              "\n",
              "The more of those you get, the better your results probably be (provided data is of quite high quality, e.g. not many false positives).\n",
              "\n",
              "Data augmentation\n",
              "\n",
              "You could do this without too much work I think. \n",
              "\n",
              "You have to remember though that data augmentation has to preserve labels. So no things like flipping (it would be fine for number 8 but u flipped could become n).\n",
              "\n",
              "Augmentations which should be fine:\n",
              "\n",
              "\n",
              "Small rotations (up to 20 degrees or so)\n",
              "Small Gaussian noise or similar\n",
              "CutOut with small patches (black rectangulars of size 3x3 pixels or similar being zeroed out on the image)\n",
              "Gentle spatial transformations (rescale, shift, linear transformations)\n",
              "MixUp (you mix linearly two images with different labels (e.g. image of A multiplied by 0.6 and cipher 2 multiplied by 0.4 and try to classify it as 0.6 A and 0.4 2). Remember labels dont have to be solely binary. This should help your network not to be overconfident with it's predictions\n",
              "\n",
              "\n",
              "You can find all of those in albumentations third party library.\n",
              "\n",
              "Model augmentation\n",
              "\n",
              "For your model you could employ thing like dropout (be careful with it's integration with batch norm though), shake shake, Stochastic Depth etc.\n",
              "\n",
              "Final\n",
              "\n",
              "You can use all of those, remember to test how it performs. I tried to list them with the most promising approach on top.\n",
              "\n",
              "One possibility would be to make the model more robust to variance via augmentation.\n",
              " <P> If I get this right, there is one mask for each sample, right? Use pandas to have the data and mask paired and then split them randomly with a help function:\n",
              "\n",
              "import glob\n",
              "import pandas as pd\n",
              "import numpy as np\n",
              "\n",
              "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
              "    np.random.seed(seed)\n",
              "    perm = np.random.permutation(df.index)\n",
              "    m = len(df.index)\n",
              "    train_end = int(train_percent * m)\n",
              "    validate_end = int(validate_percent * m) + train_end\n",
              "    train = df.ix[perm[:train_end]]\n",
              "    validate = df.ix[perm[train_end:validate_end]]\n",
              "    test = df.ix[perm[validate_end:]]\n",
              "    return train, validate, test\n",
              "\n",
              "folder_data = glob.glob(\"D:\\\\Neda\\\\Pytorch\\\\U-net\\\\my_data\\\\imagesResized\\\\*.png\")\n",
              "folder_mask = glob.glob(\"D:\\\\Neda\\\\Pytorch\\\\U-net\\\\my_data\\\\labelsResized\\\\*.png\")\n",
              "\n",
              "data_mask = pd.DataFrame({\"data\": folder_data, \"mask\": folder_mask})\n",
              "\n",
              "train, validate, test = train_validate_test_split(data_mask)\n",
              "\n",
              "\n",
              "Credits of helper function from @piRSquared answer in this question\n",
              " <P> Indeed having a \"random\" baseline is common practice, but usually you do not need to explicitly generate one, let alone \"train\" it. In most cases you can have quite accurate expectation values for the \"random\" baseline. For instance, in ImageNet classification you have 1000 categories of equal size, than predicting a category at random should give you an expected accuracy of 1/1000. You do not need to instantiate a random classifier to produce that number.\n",
              "If you insist on explicitly instantiate a random classifier - what is the meaning of \"training\" it? There are the errors you get, pytorch simply cannot understand what you are doing. You can have a random classifier and you can evaluate its performance, but there is no meaning to training it.\n",
              " <P> You can access the data and labels of the dataset, for either split, using the data and targets attributes respectively. So, for example, here you can access the training data and labels using mnist_train.data and mnist_train.labels respectively.\n",
              "Since the targets attribute is a torch.Tensor for this dataset, you can count the number of instances of each target by using torch.bincount. Since there are 10 classes in total, the output will be a tensor of length 10, where the ith index specifies the number of data points of class i.\n",
              "Example:\n",
              " mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transforms.ToTensor(), download=True)\n",
              " mnist_train.targets\n",
              "tensor([5, 0, 4,  ..., 5, 6, 8])\n",
              " torch.bincount(mnist_train.targets, minlength=10)\n",
              "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n",
              "\n",
              "You can see that class 5 has 5,421 data points in the training split.\n",
              " <P> What you need to do is: Average the loss over all the batches and then append it to a variable after every epoch and then plot it. Implementation would be something like this:\n",
              "import matplotlib.pyplot as plt\n",
              "\n",
              "def my_plot(epochs, loss):\n",
              "    plt.plot(epochs, loss)\n",
              "    \n",
              "def train(num_epochs,optimizer,criterion,model):\n",
              "    loss_vals=  []\n",
              "    for epoch in range(num_epochs):\n",
              "        epoch_loss= []\n",
              "        for i, (images, labels) in enumerate(trainloader):\n",
              "            # rest of the code\n",
              "            loss.backward()\n",
              "            epoch_loss.append(loss.item())\n",
              "            # rest of the code\n",
              "        # rest of the code\n",
              "        loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
              "        # rest of the code\n",
              "    \n",
              "    # plotting\n",
              "    my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)\n",
              "\n",
              "my_plot([1, 2, 3, 4, 5], [100, 90, 60, 30, 10])\n",
              "\n",
              "You can do a similar calculation for accuracy.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col0\" class=\"data row57 col0\" >source code for conv2d,Can someone point to the cpp code for conv2d. I would like to know how pytorch implements convolution internally.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col1\" class=\"data row57 col1\" >It depends on the backend (GPU, CPU, distributed etc) but in the most interesting case of GPU it's pulled from cuDNN which is released in binary format and thus you can't inspect its source code. It's a similar story for CPU MKLDNN. I am not aware of any place where PyTorch would \"handroll\" it's own convolution kernels, but I may be wrong. EDIT: indeed, I was wrong as pointed out in an answer below.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col2\" class=\"data row57 col2\" >Hi, We try to use high-performance libraries like cudnn and mkldnn as much as possible. In practise, their code is most likely to be the one that is executed when conv is called. There you can find the general implementation we have that works in all cases: for gpu and for cpu.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row57_col3\" class=\"data row57 col3\" ><P> \n",
              "It depends on the backend (GPU, CPU, distributed etc) but in the most interesting case of GPU it's pulled from cuDNN which is released in binary format and thus you can't inspect its source code. It's a similar story for CPU MKLDNN. I am not aware of any place where PyTorch would \"handroll\" it's own convolution kernels, but I may be wrong. EDIT: indeed, I was wrong as pointed out in an answer below.\n",
              "It's difficult without knowing how PyTorch is structured. A lot of code is actually being autogenerated based on various markup files, as explained here. Figuring this out requires a lot of jumping around. For instance, the conv.cpp file you're linking uses torch::conv1d, which is defined here and uses at::convolution which in turn uses at::_convolution, which dispatches to multiple variants, for instance at::cudnn_convolution. at::cudnn_convolution is, I believe, created here via a markup file and just plugs in directly to cuDNN implementation (though I cannot pinpoint the exact point in code when that happens).\n",
              "\n",
              " <P> The functional code is all implemented in C++. The entry point into the C++ pytorch code for conv2d is here.\n",
              " <P> A Conv2D is mostly a generalized version of Conv1D. You can of course use a degenerate version of Conv2D to reproduce a 1D convolution - \n",
              "You'll need to add in another dimension to the data: \n",
              "\n",
              "data_pnt = data_pnt [..., numpy.newaxis]\n",
              "\n",
              "\n",
              "You'll also need to specify the kernel size - You'll be choosing a 1D kernel - for example:\n",
              "\n",
              "Conv2d(in,out, kernel_size=(3,1), Other Parameters)\n",
              "\n",
              " <P> The conv2d operator is listed under the list of GPU operators supporting channels_last. This is not true for the CPU version of conv2d:\n",
              "If you switch to cuda device, it will return True:\n",
              "tsr = torch.randn(N, C, H, W).to('cuda', memory_format=memory_format)\n",
              "kernel = torch.randn(out_channels, C, *kernel_size).to('cuda', memory_format=memory_format)\n",
              "conv_out = F.conv2d(tsr, kernel)\n",
              "\n",
              " conv_out.is_contiguous(memory_format=memory_format)\n",
              "True\n",
              "\n",
              " <P> since einsum is a fairly dynamic op,  amp.autocast might keep it in the “safe” region and do the computation in fp32.\n",
              "It should be quick to try and find out. <P> In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):\n",
              " \n",
              " https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\n",
              " \n",
              " https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\n",
              " \n",
              " Math type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because\n",
              " \n",
              " 1) computations with half math are not supported anywhere except Pascal and TX1\n",
              " \n",
              " 2) Even on Pascal, 6.1 cards have very low performance fp16 math\n",
              " \n",
              " 3) fp16 math comes with a set of accuracy problems\n",
              " \n",
              " @colesbury <P> It just signals the Pytorch to use the fastest implementation available for operations such as Convolution etc. when enabled, they usually consume more memory (that is cudnn.benchmark and cudnn.fastest)\n",
              "It shouldn’t have any effect, as torch.backends.cudnn.fastest is not implemented.\n",
              "In your IDE or here with link \"https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/backends/cudnn/__init__.py#L86-L93\" you can see, that benchmark, deterministic, and enabled are known flags.\n",
              "benchmark = True will use cudnnFind to profile all available Kernels and select the fastest one.\n",
              "The repository seems to only set this new argument without using it in the code.\n",
              " so much .\n",
              "So it means that torch.backends.cudnn.fastest can be deleted in my coding set. right?\n",
              "You should be able to delete it, as I cannot find any usage of it.\n",
              "Since the PyTorch backends doesn’t use this flag, I searched for any usage in the repository, but couldn’t find it either.\n",
              "I’m not familiar with the repository, but I couldn’t find any “custom PyTorch” installation, which would use this flag.\n",
              " so much \n",
              "Anyway, let me know, if you see any difference after removing the flag.\n",
              "Also,  seems to have seen it before, so it would be interesting where it was used before.\n",
              "Actually I guess I mistook torch with Pytorch and though this is available here as well (as cudnn.benchmark is!) . This was the case for torch back in the day if I’m not mistaken.\n",
              "Since Pytorch released I have only used cudnn.benchmark myself and never used the cudnn.fastest as I remember benchmark handled everything by itself . I have never seen cudnn.fastes in any official examples and chances are those repos that do use cudnn.fastes, do it as a habit comming from torch! <P> I have bought same VGA a few days ago and using it.\n",
              "tested it, it works fine in Python (I used numby library) and used CUDA 10.1\n",
              " <P> Hi, This OOM exception comes from the python api implement of conv2d_weight actually. In backprop weight calculation, the output gradients need to be expanded with output channel times. When default cudnn implement this with data prefetch block and block (not allocate more memory), python api uses a repeat that will allocate a huge size of memory on output gradients tensor with unnecessary duplication of data. you can easily fix this by convert the repeat into a loop function at conv2d_weight. <P> GPU operations have to additionally get memory to/from the GPU\n",
              "\n",
              "The problem is that your GPU operation always has to put the input on the GPU memory, and \n",
              "then retrieve the results from there, which is a quite costly operation.\n",
              "\n",
              "NumPy, on the other hand, directly processes the data from the CPU/main memory, so there is almost no delay here. Additionally, your matrices are extremely small, so even in the best case scenario there should only be a minute difference.\n",
              "\n",
              "This is also partially the reason why you use mini-batches when training on a GPU in neural networks: Instead of having several extremely small operations, you now have \"one big bulk\" of numbers that you can process in parallel. \n",
              "Also note that GPU clock speeds are generally way lower than CPU clocks, so the GPU only really shines because it has way more cores. If your matrix does not utilize all of them fully, you are also likely to see a faster result on your CPU.\n",
              "\n",
              "TL;DR: If your matrix is big enough, you will eventually see a speed-up, even with the additional cost of the GPU transfer.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col0\" class=\"data row58 col0\" >Flattening Efficientnet model</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col1\" class=\"data row58 col1\" >The problem is quite simple. When flag=True (as in getSequentialVersion()), there's a missing Flatten operation. Therefore, to fix the problem, you need to add this operation like this:\n",
              "if flag:\n",
              "    # for CIFAR10\n",
              " layers += [nn.Flatten(), nn.Linear(512, 10)]  #  add Flatten before Linear\n",
              "In the forward call, you can see the flatten in its view form:\n",
              "def forward(self, x):\n",
              " > x = self.view(input.size(0), -1)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col2\" class=\"data row58 col2\" >There is a _swish Module after the _fc simply means that for former was registered after the latter. \n",
              "class EfficientNet(nn.Module):\n",
              "    def __init__(self, blocks_args=None, global_params=None):\n",
              "\n",
              "        # [...]\n",
              "\n",
              "        # Final linear layer\n",
              "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
              "        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
              "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
              "        self._swish = MemoryEfficientSwish()\n",
              "\n",
              "The order in which they are defined is the order that they will be printed. \n",
              "def forward(self, inputs):\n",
              "    # Convolution layers\n",
              "    x = self.extract_features(inputs)\n",
              "\n",
              "    # Pooling and final linear layer\n",
              "    x = self._avg_pooling(x)\n",
              "    x = x.flatten(start_dim=1)\n",
              "    x = self._dropout(x)\n",
              "    x = self._fc(x)\n",
              "\n",
              "    return x\n",
              "\n",
              "and, as you can see, there is nothing after self._fc(x), which means no Swish will be applied.\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row58_col3\" class=\"data row58 col3\" ><P> Did you forget to flatten the output feature map of the final conv layer?\n",
              "I did not flatten the output feature map. Why is it required ? And how do you do that ? Sorry am a newbie with Deep learning and PyTorch!\n",
              "so you just want to change the last layer of vgg19 ?\n",
              "you can do like this.\n",
              "old_classifier = list(model_conv.classifier.children()) # get the classifier part alone\n",
              "old_classifier.pop() # remove the last layer\n",
              "old_classifier.append(nn.Linear(4096,2)) # add a new layer\n",
              "model_conv.classifier = nn.Sequential(*old_classifier) # attach it to the original vgg model\n",
              "\n",
              "\n",
              "\n",
              "It works now. Thanks a lot  ! <P> Just make a new Flatten layer.\n",
              "\n",
              "from collections import OrderedDict\n",
              "\n",
              "class Flatten(nn.Module):\n",
              "    def forward(self, input):\n",
              "        return input.view(input.size(0), -1)\n",
              "\n",
              "layers = OrderedDict()\n",
              "layers['conv1'] = nn.Conv2d(1, 5, 3)\n",
              "layers['relu1'] = nn.ReLU()\n",
              "layers['conv2'] = nn.Conv2d(5, 1, 3)\n",
              "layers['relu2'] = nn.ReLU()\n",
              "layers['flatten'] = Flatten()\n",
              "layers['linear1'] = nn.Linear(3600, 1)\n",
              "model = nn.Sequential(\n",
              "layers\n",
              ").cuda()\n",
              "\n",
              " <P> Amazing question (and welcome to StackOverflow)! Research paper for quick reference.\n",
              "TLDR\n",
              "\n",
              "Try wider networks (64 channels)\n",
              "Add Batch Normalization after activation (or even before, shouldn't make much difference)\n",
              "Add residual connections (shouldn't improve much over batch norm, last resort)\n",
              "\n",
              "Please check this out in this order and give a comment what (and if) any of that worked in your case (as I'm also curious).\n",
              "Things you do differently\n",
              "\n",
              "Your neural network is very deep, yet very narrow (81 parameters per layer only!)\n",
              "\n",
              "Due to above, one cannot reliably create those weights from normal distribution as the sample is just too small.\n",
              "Try wider networks, 64 channels or more\n",
              "\n",
              "You are trying much deeper network than they did\n",
              "\n",
              "Section: Comparison Experiments\n",
              "\n",
              "We conducted comparisons on a deep but efficient model with  14  weight\n",
              "layers (actually 22 was also tested in comparison with Xavier)\n",
              "\n",
              "That was due to date of release of this paper (2015) and hardware limitations \"back in the days\" (let's say)\n",
              "Is this normal?\n",
              "Approach itself is quite strange with layers of this depth, at least currently;\n",
              "\n",
              "each conv block is usually followed by activation like ReLU and Batch Normalization (which normalizes signal and helps with exploding/vanishing signals)\n",
              "usually networks of this depth (even of depth half of what you've got) use also residual connections (though this is not directly linked to vanishing/small signal, more connected to degradation problem of even deep networks, like 1000 layers)\n",
              "\n",
              " <P> The problem is quite simple. When flag=True (as in getSequentialVersion()), there's a missing Flatten operation. Therefore, to fix the problem, you need to add this operation like this:\n",
              "if flag:\n",
              "    # for Cifar10\n",
              "    layers += [nn.Flatten(), nn.Linear(512, 10)]  #  add Flatten before Linear\n",
              "\n",
              "In the forward call, you can see the flatten in its view form:\n",
              "def forward(self, x):\n",
              "    x = self.features(x)\n",
              "    x = x.view(x.size(0), -1)  # here, equivalent to torch.flatten(x, 1)\n",
              "    x = self.classifier(x)\n",
              "    return x\n",
              "\n",
              "and this is what was missing when you were transforming the layers to Sequential.\n",
              " <P> Just answering to update this post, there is now a nn.Flatten() layer in Pytorch as of 1.3:\n",
              "\n",
              "https://pytorch.org/docs/stable/_modules/torch/nn/modules/flatten.html\n",
              "\n",
              "Also, worth mentioning that if you can't use =1.3 and you \"need\" that CNN output size (for example if you have multiple heads, most people do programmatically get the output from a dummy input, with something like:\n",
              "def get_flat_fts(self, input_shape, conv_net):\n",
              "    f = conv_net(Variable(torch.ones(1,*input_shape)))\n",
              "    return int(np.prod(f.size()[1:]))\n",
              "\n",
              " <P> detach will not really \"freeze\" your layer.\n",
              "If you don't want to train a layer, you should use requires_grad=False instead.\n",
              "\n",
              "For example:\n",
              "\n",
              "hidden2.weight.requires_grad = False\n",
              "hidden2.bias.requires_grad = False\n",
              "\n",
              "\n",
              "Then to unfreeze, you do the same with requires_grad=True.\n",
              " <P> What the original paper tries to explain is to reduce overfitting use Batch Normalization.\n",
              "\n",
              "\n",
              "  Where should you splice the normalization when designing a network?\n",
              "\n",
              "\n",
              "Set the normalization early on inputs. Unbalanced input extreme values can cause instability. \n",
              "\n",
              "While if you normalize on outputs this will not prevent the inputs to cause the instability all over again.\n",
              "\n",
              "Here is the little code that explains what the BN do:\n",
              "\n",
              "import torch\n",
              "import torch.nn as nn\n",
              "\n",
              "m = nn.BatchNorm1d(100, affine=False)\n",
              "input = 1000*torch.randn(3, 100)\n",
              "print(input)\n",
              "output = m(input)\n",
              "print(output)\n",
              "print(output.mean()) # should be ~ 0\n",
              "print(output.std()) # should be ~ 1\n",
              "\n",
              "\n",
              "\n",
              "  Does it make sense to normalize any time after you have a dense layer\n",
              "\n",
              "\n",
              "Yes, you may do so as matrix multiplication may lead to producing the extremes. Also, after convolution layers, because these are also matrix multiplication, similar but less intense comparing to dense (nn.Linear) layer. If you for instance print the resent model, you will see that batch norms are set every time after the conv layer like this:\n",
              "\n",
              "(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "\n",
              "\n",
              "To print the full resnet you may use this:\n",
              "\n",
              "import torchvision.models as models\n",
              "r = models.resnet18()\n",
              "print(r)\n",
              "\n",
              " <P> Similiar questions have already been asked, but here it goes:    \n",
              "\n",
              "import torch\n",
              "\n",
              "actor = torch.nn.Sequential(\n",
              "    torch.nn.Linear(9, 20), # output shape has to be specified\n",
              "    torch.nn.ReLU(),\n",
              "    torch.nn.Linear(20, 20), # same goes over here\n",
              "    torch.nn.ReLU(),\n",
              "    torch.nn.Linear(20, 27), # and here\n",
              "    torch.nn.Softmax(),\n",
              ")\n",
              "\n",
              "print(actor)\n",
              "\n",
              "\n",
              "Initialization: By default, from version 1.0 onward, linear layers will be initialized with Kaiming Uniform (see this post). If you want to initialize your weights differently, see most upvoted answer to this question.\n",
              "\n",
              "You may also use Python's OrderedDict to match certain layers easier, see Pytorch's documentation, you should be able to proceed from there.\n",
              " <P> For ones who need Maxout, I changed the above code to make it work. \n",
              "python\n",
              "class Maxout(nn.Module):\n",
              " def __init__(self, d_in, d_out, pool_size):\n",
              " super().__init__()\n",
              " self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\n",
              " self.lin = nn.Linear(d_in, d_out * pool_size)\n",
              " def forward(self, inputs):\n",
              " shape = list(inputs.size())\n",
              " shape[-1] = self.d_out\n",
              " shape.append(self.pool_size)\n",
              " max_dim = len(shape) - 1\n",
              " out = self.lin(inputs)\n",
              " m, i = out.view(*shape).max(max_dim)\n",
              " return m\n",
              " <P> The way I do it is to put in some arbitrary value and let the model throw an error. You will be able to see the number of input features in the error description.\n",
              "There are other ways to do it as well. You can compute the size by hand and write a comment next to each nn.Conv2d layer depicting the layer output. Before you use the nn.Flatten(), you will have the output, simply multiply all the dimensions except the bacthsize. The resulting value is the number of input features for nn.Linear() layer.\n",
              "If you don't want to do any of this, you can try torchlayers. A handy package that lets you define pytorch models like Keras.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col0\" class=\"data row59 col0\" >Rowwise numpy.isin for 2D arrays</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col1\" class=\"data row59 col1\" >There is no bitwise and/or operation for tensors in Torch. There are element-wise operations implemented in Torch, but not bite-wise ones.\n",
              "\n",
              "However, if you could convert each bit as a separate Tensor dimension, you can use Element-wise operation. \n",
              "\n",
              "For an example,\n",
              "\n",
              "a = torch.Tensor{0, 1,1,0}\n",
              "b = torch.(tensor{1,2,3,4,5,6,7,8,9]])\n",
              "rows, cols = np.indices(arr.shape)\n",
              "Set the</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col2\" class=\"data row59 col2\" >Here is a way to do this :\n",
              "(A == B[..., None]).any(axis=1).astype(bool)\n",
              "# > array([[False,  True],\n",
              "#          [ True, False],\n",
              "#          [False, False]])\n",
              "\n",
              "You could also do it inside a list comprehension:\n",
              "\n",
              "[np.isin(a, b) for a,b in zip(A, B)]\n",
              "# > [array([False,  True]), array([ True, False]), array([False, False])]\n",
              "np.array([np.isin(a, b) for a,b in zip(A, B)])\n",
              "# > array([[False,  True],\n",
              "#          [ True, False],\n",
              "#          [False, False]])</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row59_col3\" class=\"data row59 col3\" ><P> arr[[0,1,2,3], [0,2,1,2]]\n",
              "\n",
              "or if you prefer np.arange(4) for the 1st indexing array.\n",
              " <P> You can also use np.indices. If the row and column indexes % 2 are equal, you are on an \"even\" diagonal. If they are not equal you are on an \"odd\" diagonal.\n",
              "arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
              "rows, cols = np.indices(arr.shape)\n",
              "\n",
              "Set the even indexes to 0\n",
              " arr[(rows%2)==(cols%2)] = 0\n",
              " arr\n",
              "... array([[0, 2, 0],\n",
              "           [4, 0, 6],\n",
              "           [0, 8, 0]])\n",
              "\n",
              "Set the odd indexes to 0\n",
              " arr[(rows%2)!=(cols%2)] = 0\n",
              " arr\n",
              "... array([[1, 0, 3],\n",
              "           [0, 5, 0],\n",
              "           [7, 0, 9]])\n",
              "\n",
              " <P> One way would be to use numpy.einsum.\n",
              "C = np.einsum('ijk,ikl-il', A, B)\n",
              "\n",
              "Or you could use broadcasted matrix multiply.\n",
              "C = (A @ B).squeeze(axis=1)\n",
              "# equivalent: C = np.matmul(A, B).squeeze(axis=1)\n",
              "\n",
              " <P> Check this. There is no bitwise and/or operation for tensors in Torch. There are element-wise operations implemented in Torch, but not bite-wise ones.\n",
              "\n",
              "However, if you could convert each bit as a separate Tensor dimension, you can use element-wise operation. \n",
              "\n",
              "For an example,\n",
              "\n",
              "a = torch.Tensor{0,1,1,0}\n",
              "b = torch.Tensor{0,1,0,1}\n",
              "\n",
              "torch.cmul(a,b):eq(1)\n",
              "0\n",
              "1\n",
              "0\n",
              "0\n",
              "[torch.ByteTensor of size 4]\n",
              "\n",
              "torch.add(a,b):ge(1)\n",
              "0\n",
              "1\n",
              "1\n",
              "1\n",
              "[torch.ByteTensor of size 4]\n",
              "\n",
              "\n",
              "Hope this will help you.\n",
              " <P> A possible solution is to increment axis value:\n",
              "\n",
              "a = np.array([[[[1.0, 1.1]]],[[[2.1,2.0]]]])\n",
              "np.argmax(a,axis=3)\n",
              "\n",
              "array([[[1]],\n",
              "       [[0]]])\n",
              "\n",
              "\n",
              "But I still have inner brackets.\n",
              " <P> As a supplementary of @kmario23's solution, you can still achieve the same results like\n",
              "\n",
              "b[torch.nonzero(a==1,as_tuple=True)]\n",
              "\n",
              " <P> I would do it following way using numpy\n",
              "import numpy as np\n",
              "arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
              "masktile = np.array([[True,False],[False,True]])\n",
              "mask = np.tile(masktile, (2,2))  # this must be at least as big as arr\n",
              "arr0 = np.where(mask[:arr.shape[0],:arr.shape[1]], arr, 0)\n",
              "arr1 = np.where(mask[:arr.shape[0],:arr.shape[1]], 0, arr)\n",
              "print(arr0)\n",
              "print(arr1)\n",
              "\n",
              "output:\n",
              "[[1 0 3]\n",
              " [0 5 0]\n",
              " [7 0 9]]\n",
              "[[0 2 0]\n",
              " [4 0 6]\n",
              " [0 8 0]]\n",
              "\n",
              "Explanation: I am creating mask which is array of Trues and Falses to use to decide if given element is to remain or should be replaced by 0. I create single tile which then I feed into np.tile to get \"chessboard\" of sufficient size, then I use part of apprioate size of it together with np.where to replace selected elements with 0.\n",
              " <P> Use .squeeze() and a negative index.\n",
              "\n",
              "a = np.array([[[[1.0, 1.1]]], [[[2.1, 2.0]]]])\n",
              "np.argmax(a, axis = -1).squeeze()\n",
              "\n",
              "array([1, 0], dtype=int32)\n",
              "\n",
              " <P> Indexing in PyTorch is almost similar to numpy.\n",
              "\n",
              "a = torch.randn(2, 2, 3)\n",
              "b = torch.eye(2, 2, dtype=torch.long)\n",
              "c = torch.eye(2, 2, dtype=torch.long)\n",
              "\n",
              "print(a)\n",
              "print(a[b, c, :])\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "tensor([[[ 1.2471,  1.6571, -2.0504],\n",
              "         [-1.7502,  0.5747, -0.3451]],\n",
              "\n",
              "        [[-0.4389,  0.4482,  0.7294],\n",
              "         [-1.3051,  0.6606, -0.6960]]])\n",
              "tensor([[[-1.3051,  0.6606, -0.6960],\n",
              "         [ 1.2471,  1.6571, -2.0504]],\n",
              "\n",
              "        [[ 1.2471,  1.6571, -2.0504],\n",
              "         [-1.3051,  0.6606, -0.6960]]])\n",
              "\n",
              " <P> Edit: This should be an in-place version:\n",
              "\n",
              "import torch\n",
              "import numpy as np\n",
              "\n",
              "t = torch.rand(10)\n",
              "print('Original Tensor:', t)\n",
              "\n",
              "order = np.array(range(10))\n",
              "np.random.shuffle(order)\n",
              "print('Order:', order)\n",
              "\n",
              "# in-place changing of values\n",
              "t[np.array(range(10))] = t[order]\n",
              "print('New Tensor:', t)\n",
              "\n",
              "\n",
              "Output:\n",
              "\n",
              "Original Tensor: tensor([ 0.3380,  0.3450,  0.2253,  0.0279,  0.3945,  0.6055,  0.1489,\n",
              "         0.7676,  0.4213,  0.2683])\n",
              "Order: [7 1 3 6 2 9 0 5 4 8]\n",
              "New Tensor: tensor([ 0.7676,  0.3450,  0.0279,  0.1489,  0.2253,  0.2683,  0.3380,\n",
              "         0.6055,  0.3945,  0.4213])\n",
              "\n",
              "\n",
              "I hope this is roughly what you were looking for!\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col0\" class=\"data row60 col0\" >Unable to import torch.distributed.rpc</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col1\" class=\"data row60 col1\" >You can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards.\n",
              "\n",
              "Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\n",
              "`\n",
              "rm -rf aten/src/ATen/Config.h\n",
              "`</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col2\" class=\"data row60 col2\" >PyTorch Distributed package does not support Windows yet. </td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row60_col3\" class=\"data row60 col3\" ><P> @seemethere @malfet should we set up a GCC 9.X CI? <P> cc @ailzhang <P> Turns out the GPU build in was cached by setuptools (build/, dist/, etc.) and also installed!\n",
              " <P> You have `WITH_DISTRIBUTED_MW` enabled, but this is not a flag we support at the moment. Disable it and it should build just fine. <P> cc @janeyx99 <P> I think you're missing `\\\"-D_GLIBCXX_USE_CXX11_ABI=0\\\"` in your build flags. We provide this in our TorchConfig.cmake, which is why we recommend cmake as the easiest way to build LibTorch. That said, you can add that to your QtCreator config. See https://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in for the relevant file\n",
              "Please let me know if that fixes the issue. <P> @adamjstewart here's a simple workaround: `export CMAKE_PREFIX_PATH=\"/opt/intel/compilers_and_libraries_2019.4.233/mac/mkl:$PATH\"` <P> you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards. <P> Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\n",
              "`\n",
              "rm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\n",
              "` <P> Try `export TORCH_CUDA_ARCH_LIST=\"8.6\"`.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col0\" class=\"data row61 col0\" >Numpy random.choice probablities don&#39;t sum up to 1</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col1\" class=\"data row61 col1\" >p should either be a scalar or tensor containing probablities to be used for drawing the binary random number. If it is a tensor, the ith element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i]). In this case p must have floating point dtype. See also BERNOUlli() and torch.bernoulli()</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col2\" class=\"data row61 col2\" >As error suggests prob[j] doesn't sum to 1.\n",
              "\n",
              "Your epsilon 1e-6 is way too big to be considered insignificant, there is no need for this operation at all. If you insist, you have to redistribute zero-ed out values across what's left to 1 (and it seems you did just that actually).\n",
              "\n",
              "All in all you didn't normalize the array to 1:\n",
              "\n",
              "prob /= prob.sum(axis=1) # make it prob dist\n",
              "BTW. Broadcasting will extend your single number to the whole row, no need for np.full:\n",
              "\n",
              "prob[all_zero] = 1 / prob.shape[1]</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row61_col3\" class=\"data row61 col3\" ><P> Perhaps you'd want to use torch.argmax(nn_result, dim=1) ? Since dim defaults to 0, it returns just a single number constructed as a tensor. Let me illustrate with the below example:\n",
              "\n",
              " x = np.array(1)\n",
              " x.shape\n",
              "()\n",
              " len(x)\n",
              "Traceback (most recent call last):\n",
              "  File \"stdin\", line 1, in module\n",
              "TypeError: len() of unsized object\n",
              " x = np.array([1])\n",
              " x.shape\n",
              "(1,)\n",
              " len(x)\n",
              "1\n",
              "\n",
              "\n",
              "Essentially np.array will take up any object type that you construct with. In the first case, object is not an array because of which you do not see a valid shape. Since it is not an array calling len throws an error.\n",
              "\n",
              "torch.argmax with dim=0 returns a tensor as seen in the first case of the above example and hence the error.\n",
              " <P> You could just remove these 0s using conditional indexing (also assumed you meant len(l) - 1):\n",
              "a= torch.randperm(len(l)-1) #where l is total no of testing image in dataset, code output-tensor([10, 0, 1, 2, 4, 5])\n",
              "a=a[a!=0]\n",
              "b=torch.tensor([0]) # code output- tensor([0])\n",
              "c=torch.cat((b,a))# gives output as - tensor([0, 10, 0, 1, 2, 4, 5]) and 0 is used twice so repeated test image\n",
              "\n",
              "Or if you want to make sure it's never put in:\n",
              "a=torch.arange(1,len(l)) \n",
              "a=a[torch.randperm(a.shape[0])]\n",
              "b=torch.tensor([0]) \n",
              "c=torch.cat((b,a))\n",
              "\n",
              "The second approach is a bit more versatile as you can have whatever values you'd like in your initial a declaration as well as replacement.\n",
              " <P> Based on the documentation you didn't specify the num_samples of multinomial function to draw your multinomial distribution.\n",
              "\n",
              "\n",
              "  torch.multinomial(input, num_samples, replacement=False, out=None)\n",
              "  \n",
              "  \n",
              "    Returns a tensor where each row contains num_samples\n",
              "    indices sampled from the multinomial probability distribution located\n",
              "    in the corresponding row of tensor input.\n",
              "  \n",
              "\n",
              "\n",
              "Change the code as below:\n",
              "\n",
              "def select_action(self, state):\n",
              "    probs = F.softmax(self.model(Variable(state, volatile = True))*7)\n",
              "    action = probs.multinomial(1) # 1 is the number of samples to draw\n",
              "    return action.data[0,0]\n",
              "\n",
              " <P> p should either be a scalar or tensor containing probabilities to be used for drawing the binary random number. If it is a tensor, the ith element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i]). In this case p must have floating point dtype. See also bernoulli() and torch.bernoulli() <P> %%timeit -r 10 -n 10\n",
              "A[A[:,-1].argsort()]\n",
              "\n",
              "38.6 µs ± 23 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
              "\n",
              "%%timeit -r 10 -n 10\n",
              "sorted(A, key = lambda x: x[-1])\n",
              "\n",
              "69.6 µs ± 34.8 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
              "\n",
              "\n",
              "Both output \n",
              "\n",
              "tensor([[0.5951, 0.9315, 0.6548, 1.0000],\n",
              "        [0.7704, 0.0720, 0.0330, 2.0000],\n",
              "        [0.9133, 0.5071, 0.6222, 3.0000]])\n",
              "\n",
              "\n",
              "Then there is \n",
              "\n",
              "%%timeit -r 10 -n 10\n",
              "a, b = torch.sort(A, dim=-2)\n",
              "\n",
              "The slowest run took 8.45 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
              "14.3 µs ± 18.1 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
              "\n",
              "\n",
              "with a as the sorted tensor and b as the indices\n",
              " <P> You could try:\n",
              " num_occurrences_list = [0,2,0]\n",
              " factor = 2\n",
              " my_array * np.power(factor, np.bincount(num_occurrences_list))\n",
              "array([ 8,  4, 12])\n",
              "\n",
              "But I am not sure if it provides any advantage over using for loops explicitly.\n",
              "Here, np.bincount provides an array where each index contains the number of occurrences of it in your list. np.power with base 2 specifies how many times you want to multiply by 2. Finally, you perform the actual multiplication with the computed weights for each element of the array.\n",
              " <P> Found the issue - \n",
              "The rois after multiplication with spatial scale were being rounded down and had to call round function before calling long like so \n",
              "\n",
              "rois = rois.data.float()\n",
              "num_rois = rois.size(0)\n",
              "\n",
              "rois[:,1:].mul_(self.spatial_scale)\n",
              "rois = rois.round().long() ## Check this here !!\n",
              "\n",
              "\n",
              "Hope this helps someone!\n",
              " <P> As error suggests prob[j] doesn't sum to 1. \n",
              "\n",
              "Your epsilon 1e-6 is way too big to be considered insignificant, there is no need for this operation at all. If you insist, you have to redistribute zero-ed out values across what's left to 1 (and it seems you did just that actually).\n",
              "\n",
              "All in all you didn't normalize the array to 1:\n",
              "\n",
              "prob /= prob.sum(axis=1) # make it prob dist\n",
              "\n",
              "\n",
              "BTW. Broadcasting will extend your single number to the whole row, no need for np.full:\n",
              "\n",
              "prob[all_zero] = 1 / prob.shape[1]\n",
              "\n",
              " <P> Use return_inverse of numpy.unique:\n",
              "y = [ 1, 6, 5, 8, 10, 5, 4, 5, 10, 10, 9, 8, 10, 4, 10, 9]\n",
              "uniq, inv, cnt = np.unique(y, return_inverse=True, return_counts=True)\n",
              "cnt[inv]\n",
              "\n",
              "Output:\n",
              "array([1, 1, 3, 2, 5, 3, 2, 3, 5, 5, 2, 2, 5, 2, 5, 2])\n",
              "\n",
              " <P> solved it\n",
              "box_a = np.random.randn(1,4)\n",
              "box_b = np.random.randn(1,4)\n",
              "max_xy = np.broadcast_to(np.expand_dims(box_a[:, 2:],axis=1),(1,1,2))\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col0\" class=\"data row62 col0\" >With consideration of what is the element-wise arctangent of inputi/otheritextinputi / text</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col1\" class=\"data row62 col1\" >the element-wise arctangent of inputi/otheritextinputi / text context: <P> Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the bucket are set by boundaries.   Do cartesian product of the given sequence of tensors. Computes batched the p-norm distance between each pair of the two collections of row vectors.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col2\" class=\"data row62 col2\" >quadrant</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row62_col3\" class=\"data row62 col3\" ><P> Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other. <P> Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other. <P> Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor <P> Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim. <P> Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row63\" class=\"row_heading level0 row63\" >63</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col0\" class=\"data row63 col0\" >Distributed Using Gloo on Multiple Nodes Does not Work</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col1\" class=\"data row63 col1\" >This is a ref-cycle in C++ that goes as `x -> CopySlices -> AccumulateGrad -> x`.\n",
              "Note that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created.\n",
              "This can be done by making [this check](</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col2\" class=\"data row63 col2\" >Your ranks should be 0, 1, 2, 3 for world size 4.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row63_col3\" class=\"data row63 col3\" ><P> this and other autograd tasks (especially high pri ones) would be great for Alban to tackle once he's back into the world next week :) <P> Why is selection handled through multiplication by 0, though? And doesnâ€™t the multiplication incur an overhead too? Shouldnâ€™t it be handled by a conditional like torch.where? <P> We can solve this once we get scalars <P> @dawnwch The equivalent to `with torch.no_grad()` in C++ is `torch::NoGradGuard no_grad`. For example:\n",
              " \n",
              " Python:\n",
              " \n",
              " python\n",
              " \n",
              " with torch.no_grad():\n",
              " \n",
              "  module.weight += 1\n",
              " \n",
              " \n",
              " \n",
              " C++:\n",
              " \n",
              " cpp\n",
              " \n",
              " {\n",
              " \n",
              "  torch::NoGradGuard no_grad;\n",
              " \n",
              "  module->weight += 1;\n",
              " \n",
              " } // Note that anything out of this scope will still record gradients\n",
              " \n",
              "  \n",
              " \n",
              " Please try it out and let me know if it resolves the memory usage issue. <P> @soumith Thanks a lot! `cudaDeviceReset()` can release the resource associated with the current process. <P> This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\n",
              "This only happens because `x` here is a leaf Tensor.\n",
              "Note that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created.\n",
              "This can be done by making [this check](https://github.com/pytorch/pytorch/blob/6249d7302b7277864ed0ade93f58d88ee0cd3aa8/torch/csrc/autograd/VariableTypeUtils.h#L44) detect views of leafs in addition to only leafs. <P> The text seems to describe this scenario - two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`. <P> In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.\n",
              " \n",
              " \n",
              " \n",
              " Heres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)\n",
              " \n",
              " \n",
              " \n",
              " Hope it helps <P> Has anyone ever taken a look at this? Thanks! <P> It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\n",
              "\n",
              "In fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\n",
              "\n",
              "In the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\n",
              "* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\n",
              "* `apply()` in `torch/csrc/autograd/custom_function.h`\n",
              "* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\n",
              "\n",
              "Of course, the inevitable check for grad mode enabled still happens at some point.\n",
              "\n",
              "With this in mind, I propose that we:\n",
              "1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\n",
              "2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\n",
              "\n",
              "While I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\n",
              "\n",
              "If this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\n",
              "\n",
              "Thoughts on this? @albanD @gchanan </td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row64\" class=\"row_heading level0 row64\" >64</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col0\" class=\"data row64 col0\" >distributed pagerank with pytorch,I am trying to implement Pagerank with libtorch.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col1\" class=\"data row64 col1\" >This error has been fixed in  (The fix is in functional.py), so you should be able to get it if you install pytorch's nightly build.\n",
              "\n",
              "However, in this same PR, converting Upsample in bilinear mode has been disabled; the reason is that PyTorch's Bivariate mode does not align with ONNX's, and Nearest mode is the only mode currently supported.\n",
              "The output labels need to have -100 for the masked version. The transoformers library does not do it for you.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col2\" class=\"data row64 col2\" >It can be possible using APIs like send/recv APIs, collective communication APIs and RPC APIs</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row64_col3\" class=\"data row64 col3\" ><P> Try with batch_first=False. It is not supported on True by ONNX. you might need to transpose your datas because you'll have : (timesteps, batch, features) instead of (batch, timesteps, features).\n",
              " <P> My fail, forgot to scale input features, now works fine. \n",
              " <P> you duplicate this step\n",
              "\n",
              "remove it then try again\n",
              " <P> ok I found out where I was making the mistakes. This is all thanks to this thread in the HuggingFace forum.\n",
              "\n",
              "The output labels need to have -100 for the masked version. The transoformers library does not do it for you.\n",
              "One silly mistake I made was with the mask. It should have been output_mask = mask[:, 1:] instead of :-1.\n",
              "\n",
              "1. Using Model\n",
              "We need to set the masks of output to -100. It is important to use clone as shown below:\n",
              "labels = output_tokens[\"input_ids\"].clone()\n",
              "labels[output_tokens[\"attention_mask\"]==0] = -100\n",
              "\n",
              "outputs = model(\n",
              "    input_ids=input_tokens[\"input_ids\"], \n",
              "    attention_mask=input_tokens[\"attention_mask\"],\n",
              "    decoder_input_ids=output_tokens[\"input_ids\"], \n",
              "    decoder_attention_mask=output_tokens[\"attention_mask\"],\n",
              "    labels=labels, \n",
              "    return_dict=True)\n",
              "\n",
              "2. Calculating Loss\n",
              "So the final way to replicate it is as follows:\n",
              "idx = output_tokens[\"input_ids\"]\n",
              "logits = F.log_softmax(outputs[\"logits\"], dim=-1)\n",
              "mask = output_tokens[\"attention_mask\"]\n",
              "\n",
              "# shift things\n",
              "output_logits = logits[:,:-1,:]\n",
              "label_tokens = idx[:, 1:].unsqueeze(-1)\n",
              "output_mask = mask[:,1:]\n",
              "\n",
              "# gather the logits and mask\n",
              "select_logits = torch.gather(output_logits, -1, label_tokens).squeeze()\n",
              "-select_logits[output_mask==1].mean(), outputs[\"loss\"]\n",
              "\n",
              "The above however ignores the fact that this comes from two different lines. So an alternate way of calculating loss could be:\n",
              "seq_loss = (select_logits * output_mask).sum(dim=-1, keepdims=True) / output_mask.sum(dim=-1, keepdims=True)\n",
              "seq_loss.mean()\n",
              "\n",
              " <P> Just downgrade the PyTorch to anything under 1.5.0 (which is latest, as of writing this).\n",
              "\n",
              "pip uninstall torch\n",
              "pip install torch==1.4.0\n",
              "\n",
              " <P> I solved my problem by overriding the create_batches methods inside the data.Iterator. <P> This error has been fixed in https://github.com/pytorch/pytorch/pull/21434 (the fix is in functional.py), so you should be able to get it if you install pytorch's nightly build.\n",
              "\n",
              "However, in this same PR, converting Upsample in bilinear mode has been disabled; the reason is that Pytorch's bilinear mode does not align with ONNX's, and Nearest mode is the only mode currently supported.\n",
              "\n",
              "Upsample (Now called Resize) in ONNX is being updated in opset 11 to support a bilinear mode that aligns with Pytorch in https://github.com/onnx/onnx/pull/2057, but this is not yet pushed.\n",
              " <P> The avail_actions[:, 1:] == 0 create a new tensor, and possibly the whole line itself create another tensor before delete the old one after finish the operation.\n",
              "If speed is not a problem then you can just use for loop. Like\n",
              "for i in range(target_mac_out.size(0)):\n",
              "    for j in range(target_mac_out.size(1)-1):\n",
              "        if target_mac_out[i, j+1] == 0:\n",
              "            target_mac_out[i, j+1] = -9999999\n",
              "\n",
              " <P> Modnet forward method requires a parameter called inference which is a boolean, indeed when the model is trained they pass it in this way:\n",
              "# forward the main model\n",
              "pred_semantic, pred_detail, pred_matte = modnet(image, False)\n",
              "\n",
              "So here what you have to do is modify your example_input like this:\n",
              "example_input = (example_input, True)\n",
              "\n",
              " <P> I would use workaround like this - make my own crop class inherited from RandomCrop, redefining call with\n",
              "    …\n",
              "        if self.call_is_even :\n",
              "            self.ijhw = self.get_params(img, self.size)\n",
              "        i, j, h, w = self.ijhw\n",
              "        self.call_is_even = not self.call_is_even\n",
              "\n",
              "instead of\n",
              "    i, j, h, w = self.get_params(img, self.size)\n",
              "\n",
              "The idea is to suppress randomizer on odd calls\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row65\" class=\"row_heading level0 row65\" >65</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col0\" class=\"data row65 col0\" >Computes the what of the gamma function on input?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col1\" class=\"data row65 col1\" >Computes the Gaussian cross-entropy of A tensor with respect to which A can be either a 1-dimensional tensor or a 2-D tensor depending on the TF version of PyTorch.\n",
              "\n",
              "A tensor filled with zeros of A.\n",
              "B.\n",
              "C.\n",
              "D.\n",
              "E.\n",
              "H.\n",
              "A = torch.randn(1, 3, 3)\n",
              "A.shape</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col2\" class=\"data row65 col2\" >logarithmic derivative</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row65_col3\" class=\"data row65 col3\" ><P> Alias for torch.linalg.matrix_power() <P> Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m × n)(m \\times n)(m × n) and a matrix BBB of size (m × k)(m \\times k)(m × k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch. <P> Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi​/otheri​ with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis. <P> Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi​/otheri​ with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp(). <P> Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor. <P> Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs. <P> Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   torch.all tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension. <P> Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by ⊗\\otimes⊗, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row66\" class=\"row_heading level0 row66\" >66</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col0\" class=\"data row66 col0\" >How to perform bicubic upsampling of image using pytorch?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col1\" class=\"data row66 col1\" >You are asking two different questions, I will try to answer both.\n",
              "\n",
              "Indeed, you should first reshape to (C, H, W) where C is the channel dimension In most cases, you will need that extra dimension because most 'image' layers are built to receive 3D dimensional tensors - not counting the batch dimension - such as nn.Conv2d, BatchNorm2D, etc... I don't believe there's anyways around it, and doing so would restrict yourself to one-layer image datasets.\n",
              "You can broadcast to the desired shape with torch.reshape or Tensor</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col2\" class=\"data row66 col2\" >You can do this\n",
              "import torch \n",
              "import torchvision.transforms as transforms\n",
              "from PIL import Image\n",
              "t = transforms.ToTensor()\n",
              "img = Image.open(\"Table.png\")\n",
              "b = torch.nn.functional.upsample(t(img).unsqueeze(0),(500,400),mode = \"bicubic\")\n",
              "you can also apply Bicubic using Image\n",
              "img = Image.open(\"Table.png\")\n",
              "re = img.resize((400, 400),Image.BICUBIC)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row66_col3\" class=\"data row66 col3\" ><P> You are asking two different questions, I will try to answer both.\n",
              "\n",
              "Indeed, you should first reshape to (c, h, w) where c is the channel dimension In most cases, you will need that extra dimension because most 'image' layers are built to receive 3d dimensional tensors - not counting the batch dimension - such as nn.Conv2d, BatchNorm2d, etc... I don't believe there's anyways around it, and doing so would restrict yourself to one-layer image datasets.\n",
              "You can broadcast to the desired shape with torch.reshape or Tensor.view:\n",
              "X = X.reshape(1, *X.shape)\n",
              "\n",
              "Or by adding an additional dimension using torch.unsqueeeze:\n",
              "X.unsqueeze(0)\n",
              "\n",
              "\n",
              "About normalization. Batch-normalization and dataset-normalization are two different approaches.\n",
              "The former is a technique that can achieve improved performance in convolution networks. This kind of operation can be implemented using a nn.BatchNorm2d layer and is done using learnable parameters: a scale factor (~ std) and a bias (~ mean). This type of normalization is applied when the model is called and is applied per-batch.\n",
              "The latter is a pre-processing technique which allows making different features have the same scale. This normalization can be applied inside the dataset per-element. It requires you measure the mean and standard deviation of your training set.\n",
              "\n",
              "\n",
              " <P> I got the solution using this notation\n",
              "\n",
              "img[:, :, y:y+height, x:x+width]\n",
              "\n",
              "\n",
              "So the output would be a resized torch image. Thanks\n",
              " <P> You can normalise without the need of transposing the image or splitting it based on its channels \n",
              "\n",
              "torchvision.transforms.Normalize(mean=[l_channel_mean, a_channel_mean , b_channel_mean], std= [l_channel_mean, a_channel_mean , b_channel_mean])\n",
              "\n",
              "The only required transform is the one that converts the images to tensors : \n",
              "\n",
              "torchvision.transforms.ToTensor()\n",
              "\n",
              " <P> Use torch.reshape and only a single dimension can be passed to flatten it. If you do not want the dimension to be hardcoded, just -1 could be specified and the correct dimension would be inferred.\n",
              "\n",
              " x = torch.tensor([[1,2], [3,4]])\n",
              " x.reshape(-1)\n",
              "tensor([1, 2, 3, 4])\n",
              "\n",
              "\n",
              "EDIT:\n",
              "\n",
              "For your example:\n",
              "\n",
              " <P> Just generalising the above solution for any upsampling factor 'r' like in pixel shuffle.\n",
              "\n",
              "B = A.reshape(-1,r,3,s,s).permute(2,3,0,4,1).reshape(1,3,rs,rs)\n",
              "\n",
              "\n",
              "Here 's' is the spatial resolution of each channel in 'A' and 'r' is the upsampling factor. For the particular case, r=2 and s=2. This solution should work for arbitary values of 'r' with appropriate size of 'A'. \n",
              "\n",
              "So for the problem in hand s=2, r=2 and so the solution goes as\n",
              "\n",
              "B = A.reshape(-1,2,3,2,2).permute(2,3,0,4,1).reshape(1,3,4,4)\n",
              "\n",
              "\n",
              "as posted by @ddoGas\n",
              "\n",
              "Similarly if 'A' had been of size (1, 192, 356, 532) and wanting to upsample by r=8 do\n",
              "\n",
              "B = A.reshape(-1,8,3,356,532).permute(2,3,0,4,1).reshape(1,3,2848,4256)\n",
              "\n",
              " <P> You should use the transforms to do some image augmentation for your problem. \n",
              "\n",
              "As I read your comment, you can restrict translate = (a, b) to do some tiny random shifts in both dimensions.\n",
              "\n",
              "import torchvision.transforms as transforms\n",
              "\n",
              "transform = transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
              "\n",
              "img = PIL.Image.open('path/img')\n",
              "\n",
              "new_img = transform(img)\n",
              "\n",
              "\n",
              "If you want to perform more transforms like Crop as well, group all the transform into one big transform using transforms.Compose. Here is your reference\n",
              " <P> In summary, according to the comments you and I posted:\n",
              "\n",
              "The error is due to torch.nn only supports mini-batches. The input should be in the form (batch_size, channels, height, width). You seem to be missing the batch dimension. You can add .unsqueeze(0) to add a fake batch dimension in the first position.\n",
              "\n",
              "In addition to the above, you'll also have to rearrange the dimensions of your image from [HxWxC] to [CxHxW]. This is done by .ToTensor() transformation in PyTorch.\n",
              "\n",
              "For the size mismatch problem of your input image, you could use transformation like this:\n",
              "\n",
              "transform = transforms.Compose(\n",
              "                   [transforms.Resize((32,32)),\n",
              "                    transforms.ToTensor(),\n",
              "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
              "\n",
              " <P> You can resize the tensor initially to a shape of [128, 1, 1, 28, 28]\n",
              "\n",
              "# tensor.resize_((`new_shape`))    \n",
              "imgs.resize_((128, 1, 1, 28, 28))\n",
              "\n",
              "\n",
              "No when you loop through each image, will be of the desired shape [1, 1, 28, 28].\n",
              "\n",
              "Secondly, if you don't want to change the original data, reshape each image individually\n",
              "\n",
              "# tensor.resize_((`new_shape`))    \n",
              "img.resize_((1, 1, 28, 28))\n",
              "\n",
              "\n",
              "Have a look at the PyTorch documentation\n",
              " <P> It is a very common problem in segmentation networks where skip-connections are often involved in the decoding process. Networks usually (depending on the actual architecture) require input size that has side lengths as integer multiples of the largest stride (8, 16, 32, etc.).\n",
              "There are two main ways:\n",
              "\n",
              "Resize input to the nearest feasible size.\n",
              "Pad the input to the next larger feasible size.\n",
              "\n",
              "I prefer (2) because (1) can cause small changes in the pixel level for all the pixels, leading to unnecessary blurriness. Note that we usually need to recover the original shape afterward in both methods.\n",
              "My favorite code snippet for this task (symmetric padding for height/width):\n",
              "import torch\n",
              "import torch.nn.functional as F\n",
              "\n",
              "def pad_to(x, stride):\n",
              "    h, w = x.shape[-2:]\n",
              "\n",
              "    if h % stride  0:\n",
              "        new_h = h + stride - h % stride\n",
              "    else:\n",
              "        new_h = h\n",
              "    if w % stride  0:\n",
              "        new_w = w + stride - w % stride\n",
              "    else:\n",
              "        new_w = w\n",
              "    lh, uh = int((new_h-h) / 2), int(new_h-h) - int((new_h-h) / 2)\n",
              "    lw, uw = int((new_w-w) / 2), int(new_w-w) - int((new_w-w) / 2)\n",
              "    pads = (lw, uw, lh, uh)\n",
              "\n",
              "    # zero-padding by default.\n",
              "    # See others at https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad\n",
              "    out = F.pad(x, pads, \"constant\", 0)\n",
              "\n",
              "    return out, pads\n",
              "\n",
              "def unpad(x, pad):\n",
              "    if pad[2]+pad[3]  0:\n",
              "        x = x[:,:,pad[2]:-pad[3],:]\n",
              "    if pad[0]+pad[1]  0:\n",
              "        x = x[:,:,:,pad[0]:-pad[1]]\n",
              "    return x\n",
              "\n",
              "A test snippet:\n",
              "x = torch.zeros(4, 3, 1080, 1920) # Raw data\n",
              "x_pad, pads = pad_to(x, 16) # Padded data, feed this to your network \n",
              "x_unpad = unpad(x_pad, pads) # Un-pad the network output to recover the original shape\n",
              "\n",
              "print('Original: ', x.shape)\n",
              "print('Padded: ', x_pad.shape)\n",
              "print('Recovered: ', x_unpad.shape)\n",
              "\n",
              "Output:\n",
              "Original:  torch.Size([4, 3, 1080, 1920])\n",
              "Padded:  torch.Size([4, 3, 1088, 1920])\n",
              "Recovered:  torch.Size([4, 3, 1080, 1920])\n",
              "\n",
              "Reference: https://github.com/seoungwugoh/STM/blob/905f11492a6692dd0d0fa395881a8ec09b211a36/helpers.py#L33\n",
              " <P> ToTensor transforms the image to a tensor with range [0,1].\n",
              "Thus it already implies some kind of normalization. If you want to use the normalization transform afterwards you should keep in mind that a range of [0,1] usually implies mean and std to be around 0.5 (the real values depend on your data). These are the values you should pass to the normalization transform as mean and std.\n",
              "\n",
              "\n",
              "\n",
              " pytorch docs:\n",
              "\n",
              "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
              "\n",
              "\n",
              "Which means you need to have your image in range of [0,255] before. Maybe your loaded image already lies in a range of [0,1]?\n",
              "Thank makes sense but my colorbar has a range of ~1e36 so it doesn’t seem to be normalising it between 0-1 strangely\n",
              "What is your image range directly after loading your image (inside your dataset)?\n",
              "277.83605046608886 -223.96290534932243\n",
              "If I divide my images by 255 when they are loaded in, then maybe that will work.\n",
              "Update: That seems to work! I guess the transformation cannot handle data outside the 255 range\n",
              "Or you could do\n",
              "img = LOAD_YOUR_IMAGE\n",
              "img += img.min()\n",
              "img *= 255/img.max()\n",
              "img = np.astype(np.uint8)\n",
              "\n",
              "If your image is a numpy array (which I assume from your plotting code). This would work for all image ranges and would always use the full range of [0,255]. The disadvantage however would be that if you have some effects with high intensity only in part of the images, they would be scaled differently. Another approach would be to calculate mean and std during your datasets init (would take some time for huge dataset)\n",
              "take care of this issue as well.\n",
              "\n",
              "\n",
              "github.com/pytorch/vision with link \"https://github.com/pytorch/vision/issues/546\"\n",
              "\n",
              "\n",
              " with link \"https://github.com/InnovArul\"\n",
              "Issue: transforms.ToTensor() for numpy float array in the range of [0.0, 255.0] with link \"https://github.com/pytorch/vision/issues/546\"\n",
              "\n",
              "\n",
              "\topened by InnovArul with link \"https://github.com/InnovArul\"\n",
              "\ton 2018-07-13 with link \"https://github.com/pytorch/vision/issues/546\"\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "I had come across a debugging scenario where the ToTensor() didn't convert the numpy float array in the range of [0.0,...\n",
              "\n",
              "enhancement\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Would that not be: img -= img.min() instead of +?\n",
              "If I try that I start off with an image:\n",
              "\n",
              "and then get the following image after the transform:\n",
              "\n",
              "strangely with a mean of nan:\n",
              "tensor[0,:,:].mean()\n",
              "\n",
              "Out: nan\n",
              "\n",
              "which is odd as I use tensor[tensor != tensor] = 0 to remove all nan values upon loading the file.\n",
              "\n",
              "\n",
              "\n",
              " spacemeerkat:\n",
              "\n",
              "Would that not be: img -= img.min() instead of +?\n",
              "\n",
              "\n",
              "You’re right.\n",
              "Do you try this before removing NaNs or after?\n",
              "Can you post your whole transformation code?\n",
              "To avoid this, I added the conversion to unsigned ints, since ints are expected to be in range [0,255]\n",
              "Ah I missed the uint8 line as Jupyter Notebooks doesn’t support np.astype but I’ve replaced it with img.astype(np.uint8) and it works perfectly now.\n",
              "So for anyone who comes across this thread, here is the complete code:\n",
              "img[img != img] = 0\n",
              "img -= img.min()\n",
              "img *= 255/img.max()\n",
              "img = img.astype(np.uint8)\n",
              "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0], [1])])\n",
              "tensor = transform(img.reshape(*img.shape,1)).float()\n",
              "\n",
              "And the final image after transform:\n",
              "\n",
              "Many thanks as always </td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row67\" class=\"row_heading level0 row67\" >67</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col0\" class=\"data row67 col0\" >Pytorch, Pre-trained model: How to use feature and classifier at the same time</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col1\" class=\"data row67 col1\" >If you want to change the number of classes in a pre-trained model, then you can replace the last fully connected layer with a new one and train only this specific layer on new samples. Here's a sample code for this case from PyTorch's autograd mechanics notes:\n",
              "\n",
              "model = torchvision.models.resnet18(pretrained=True)\n",
              "for param in model.parameters():\n",
              "    param.requires_grad = False\n",
              "# Parameters of newly constructed modules have requires_grad=True by default\n",
              "model.fc = nn.Linear(512, 100)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col2\" class=\"data row67 col2\" >def get_model():\n",
              "    model = models.vgg16(pretrained=True)\n",
              "    model.features = model.features[:]\n",
              "    model.classifier = model.classifier[:4]\n",
              "\n",
              "    model = model.eval()\n",
              "    # model.cuda()  # send the model to GPU, DO NOT include this line if you haven't a GPU\n",
              "    return model\n",
              "\n",
              "result:\n",
              "\"</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row67_col3\" class=\"data row67 col3\" ><P> Im not doing skeleton detection research, but your problem seems to be general. \n",
              "\n",
              "(1) I dont think other people should teaching you from begining on how to load data and run their code from begining.  \n",
              "\n",
              "(2) For running other peoples code,  just modify their test script which is provided e.g\n",
              "\n",
              "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/tools/test.py\n",
              "\n",
              "They already helps you loaded the model\n",
              "\n",
              " model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
              "        cfg, is_train=False\n",
              "    )\n",
              "\n",
              "    if cfg.TEST.MODEL_FILE:\n",
              "        logger.info('= loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
              "        model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n",
              "    else:\n",
              "        model_state_file = os.path.join(\n",
              "            final_output_dir, 'final_state.pth'\n",
              "        )\n",
              "        logger.info('= loading model from {}'.format(model_state_file))\n",
              "        model.load_state_dict(torch.load(model_state_file))\n",
              "\n",
              "    model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()\n",
              "\n",
              "\n",
              "Just call \n",
              "\n",
              "# evaluate on Variable x with testing data\n",
              "y = model(x)\n",
              "# access Variable's tensor, copy back to CPU, convert to numpy\n",
              "arr = y.data.cpu().numpy()\n",
              "# write CSV\n",
              "np.savetxt('output.csv', arr)\n",
              "\n",
              "\n",
              "You should be able to open it in excel\n",
              "\n",
              "(3) \"convert them to the .json annotations that I need\". \n",
              "\n",
              "That's the problem nobody can help. We don't know what format you want. For their format, it can be obtained either by their paper. Or looking at their training data by\n",
              "\n",
              "X, y = torch.load('some_training_set_with_labels.pt')\n",
              "\n",
              "\n",
              "By correlating the x and y. Then you should have a pretty good idea.\n",
              " <P> If you don't want to change the output of the classifier (i.e. the number of classes), then you can simply continue training the model with new example images, assuming that they are reshaped to the same shape that the pretrained model accepts. \n",
              "\n",
              "On the other hand, if you want to change the number of classes in a pre-trained model, then you can replace the last fully connected layer with a new one and train only this specific layer on new samples. Here's a sample code for this case from PyTorch's autograd mechanics notes:\n",
              "\n",
              "model = torchvision.models.resnet18(pretrained=True)\n",
              "for param in model.parameters():\n",
              "    param.requires_grad = False\n",
              "# Replace the last fully-connected layer\n",
              "# Parameters of newly constructed modules have requires_grad=True by default\n",
              "model.fc = nn.Linear(512, 100)\n",
              "\n",
              "# Optimize only the classifier\n",
              "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n",
              "\n",
              " <P> Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started <P> From maskrcnn_resnet50_fpn document:\n",
              "\n",
              "pretrained (bool) – If True, returns a model pre-trained on COCO train2017\n",
              "pretrained_backbone (bool) – If True, returns a model with backbone pre-trained on Imagenet\n",
              "trainable_backbone_layers (int) – number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.\n",
              "\n",
              "So for training from scratch using:\n",
              "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False, trainable_backbone_layers=5, num_classes=your_num_classes)\n",
              "\n",
              "or:\n",
              "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False, num_classes=your_num_classes)\n",
              "\n",
              "because in source code of maskrcnn_resnet50_fpn:\n",
              "if not (pretrained or pretrained_backbone):\n",
              "    trainable_backbone_layers = 5\n",
              "\n",
              " <P> TorchVision provides only ImageNet data pretrained model for the SqueezeNet architecture. However, you can train your own model using MNIST dataset by taking only the model (but not the pre-trained one) from torchvision.models.\n",
              "\n",
              "In [10]: import torchvision as tv\n",
              "\n",
              "# get the model architecture only; ignore `pretrained` flag\n",
              "In [11]: squeezenet11 = tv.models.squeezenet1_1()\n",
              "\n",
              "In [12]: squeezenet11.training   \n",
              "Out[12]: True\n",
              "\n",
              "\n",
              "Now, you can use this architecture to train a model on MNIST data, which should not take too long.\n",
              "\n",
              "\n",
              "\n",
              "One modification to keep in mind is to update the number of classes which is 10 for MNIST. Specifically, the 1000 should be changed to 10, and the kernel and stride accordingly.\n",
              "\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5)\n",
              "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (2): ReLU(inplace)\n",
              "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
              "  )\n",
              "\n",
              "\n",
              "Here's the relevant explanation: finetuning_torchvision_models-squeezenet\n",
              " <P> If you just save the weights of pretrained networks somewhere, you can load them just like you can load any other network weights.\n",
              "\n",
              "Saving:\n",
              "\n",
              "import torchvision\n",
              "\n",
              "#  I am assuming we have internet access here\n",
              "model = torchvision.models.vgg16(pretrained=True)\n",
              "torch.save(model.state_dict(), \"Somewhere\")\n",
              "\n",
              "\n",
              "Loading:\n",
              "\n",
              "import torchvision\n",
              "\n",
              "def create_vgg16(dict_path=None):\n",
              "    model = torchvision.models.vgg16(pretrained=False)\n",
              "    if (dict_path != None):\n",
              "        model.load_state_dict(torch.load(dict_path))\n",
              "    return model\n",
              "\n",
              "model = create_vgg16(\"Somewhere\")\n",
              "\n",
              " <P> training (enum, default TrainingMode.EVAL) – TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) – names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) – names to assign to the output nodes of the graph, in order aten (bool, default False) – [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset.py are exported as ATen ops. export_raw_ir (bool, default False) – [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) –  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph: is exported as: In the above example, prim::ListConstruct is not supported, hence exporter falls through. <P> training (enum, default TrainingMode.EVAL) – TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) – names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) – names to assign to the output nodes of the graph, in order aten (bool, default False) – [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset.py are exported as ATen ops. export_raw_ir (bool, default False) – [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) – <P> There is no automatic way of doing so - because you need to explicitly decide what to do when things do not match.\n",
              "\n",
              "Personally, when I need to \"force\" a pre-trained weights on a slightly changed model. I find that working with the state_dict itself is the most convenient way.\n",
              "\n",
              "new_model = model( ... )  # construct the new model\n",
              "new_sd = new_model.state_dict()  # take the \"default\" state_dict\n",
              "pre_trained_sd = torch.load(file)  # load the old version pre-trained weights\n",
              "# merge information from pre_trained_sd into new_sd\n",
              "# ...\n",
              "# after merging the state dict you can load it:\n",
              "new_model.load_state_dict(new_sd)\n",
              "\n",
              " <P> Torchvision models are pretrained on the ImageNet dataset. Due to its comprehensiveness and size, ImageNet is the most commonly used dataset for pretraining  transfer learning. As you noted, it has 1000 classes. The complete class list can be searched, or you can refer to this listing on GitHub: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row68\" class=\"row_heading level0 row68\" >68</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col0\" class=\"data row68 col0\" >\n",
              "I’m using Convolutional Autoencoder Network. While training my CPU RAM (30GB) is getting fully used in just 20 epochs but my GPU memory (8GB) is used only 5%.\n",
              "How should I approach to use my GPU better and reduce CPU memory usage.\n",
              "Thanks</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col1\" class=\"data row68 col1\" >CUDA operations are executed asynchronously, so you would have to synchronize the code before starting and stopping the timer via torch.cuda.synchronize(). Otherwise you’ll only profile the Python overhead as well as the kernel launch in the first iterations until your script encounters a blocking operation.\n",
              "Ah! Thanks, things make sense after this.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col2\" class=\"data row68 col2\" >You are storing the computation graph using this line of code:\n",
              "running_loss += loss\n",
              "\n",
              "Change it to running_loss += loss.item() and try it again.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row68_col3\" class=\"data row68 col3\" ><P> CUDA operations are executed asynchronously, so you would have to synchronize the code before starting and stopping the timer via torch.cuda.synchronize(). Otherwise you’ll only profile the Python overhead as well as the kernel launch in the first iterations until your script encounters a blocking operation.\n",
              "Ah! Thanks, things make sense after this. <P> Usually this happens because of memory on your GPU. If you have more powerful GPUs, your problem could be solved (as you mentioned in your answer).\n",
              "But if you do not have, you can scale down your images into about 256*x sizes.\n",
              "It is also good practice for performance's sake.\n",
              " <P> Try to reduce the batch size and check which batch size would fit and how much memory is used.\n",
              "Your GPUs might have not enough memory for the code you are using or are you using exactly the setup the authors were also using?\n",
              "You’re right Mr. , the problem is that GPU 0 shared with some other users. <P> It’s possible your network is mostly bandwidth bound rather than compute bound, in which case the 1/2X memory traffic for fp16 ops would allow a speedup.  It’s also probable that many Pytorch kernels compute internally in fp32 even if the input/output is fp16, so the lack of raw fp16 throughput isn’t a problem.  It’s also possible the same is true for cuda library calls (gemms/convolutions):  they may use fp32 compute internally for fp16 input/output, so the compute throughput is not worse than fp32 and the required bandwidth is reduced. <P> \n",
              "The memory consumption is doubled most likely because both outputs are saved.\n",
              "Indeed, to compute the backward of the convolution, we need to know what was the output. So if you do two of them, we need to know both outputs. <P> Hi, it would be very helpful if you could post a script of your test. One possibility that comes to mind is that your measurement for the forward propagation is not synchronizing the CUDA device so when you move the data back to the CPU you have to wait for the computation for finish first. <P> CUDA operations are called asynchronously, so you should synchronize the code before starting and stopping the timer using torch.cuda.synchronize(). In your current code snippet logits = logits.detach().cpu().numpy() will create a synchronization point, so that your code will wait at this line of code for all preceding operations to finish (which also might include the forward pass of your models) while the timer was already started. <P> Hi Two things We have a custom allocator, so even when the memory is released, you won’t see it available on nvidia-smi  but you will be able to use it in pytorch.The memory is realeased only when you don’t reference it anymore. You might want to wrap the content of your inner loop in a function so that all the intermediary results go out of scope (and are thus released) between loop iterations. <P> If you are running the code on a GPU and call p1 and p2 after each other, these calls will be queued onto the device and executed asynchronously. Depending on the workload of p1, p2 might start while p1 is still executing.If you have multiple GPUs in your system (and don‚Äôt use data parallel), you could execute p1 and p2 on each device and concatenate the result back on a single device. <P> Because ur batch-size is 200 and densenet is demanding.  Why don’t you use smaller BS and increase up the the gpu memory?\n",
              "I mean, according to the code your batch size is 200.\n",
              "Densenet is heavy (densily conected)\n",
              "\n",
              "\n",
              "\n",
              " JuanFMontesinos:\n",
              "\n",
              "increase up the the gpu memory\n",
              "\n",
              "\n",
              "Thanks for your reply. What do you mean by increasing the gpu memory? Buying a new one?\n",
              "Increasing the memory demand. I mean, start from a small batch size and increase it checking gpu usage with nvidia-smi. You need to compute the backward as well since it requires memory. When you see you can compute forward and backward reaching the memory limit thats the maximum batch siD you can use\n",
              "Thanks for the tip! I guess this is a good systematic approach </td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row69\" class=\"row_heading level0 row69\" >69</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col0\" class=\"data row69 col0\" >Error trying to convert simple convolutional model to CoreML</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col1\" class=\"data row69 col1\" >The LSTM requires two hidden states, not one. So instead of\n",
              "\n",
              "h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
              "\n",
              "Use\n",
              "\n",
              "self.0 = (torch.zerOS(self.(self.NUM_Layers, X.size) for self in enumerate(block_config):\n",
              "\n",
              "block = DenseBlock(NUM_ Layers, in_size=num_features, growth_rate=growth_rate, block=block_placeholder, droprate</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col2\" class=\"data row69 col2\" >Core ML does not have 1-dimensional batch norm. The tensor must have at least rank 3.\n",
              "\n",
              "If you want to convert this model, you should fold the batch norm weights into those of the preceding layer and remove the batch norm layer. (I don't think PyTorch has a way to automatically do this for you.)\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row69_col3\" class=\"data row69 col3\" ><P> The LSTM requires two hidden states, not one. So instead of\n",
              "h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
              "\n",
              "use\n",
              "h0 = (torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device), torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device))\n",
              "\n",
              "So you need two hidden states in a tuple.\n",
              " <P> adaptive_avg_pool2d  is not supported in my case and this  nn.AdaptiveAvgPool2d((None,1)) also have have issue.\n",
              " <P> It was coremltools version related issue. Tried with latest beta coremltools 3.0b2.\n",
              "\n",
              "Following works without any error with latest beta.\n",
              "\n",
              "import torch\n",
              "\n",
              "class cat_model(torch.nn.Module):\n",
              "    def __init__(self):\n",
              "        super(cat_model, self).__init__()\n",
              "\n",
              "    def forward(self, a, b):\n",
              "        c = torch.cat((a, b), 1)\n",
              "        # print(c.shape)\n",
              "        return c\n",
              "\n",
              "a = torch.randn((1, 128, 128, 256))\n",
              "b = torch.randn((1, 1, 128, 256))\n",
              "\n",
              "model = cat_model()\n",
              "torch.onnx.export(model, (a, b), 'cat_model.onnx')\n",
              "\n",
              "import onnx\n",
              "model = onnx.load('cat_model.onnx')\n",
              "onnx.checker.check_model(model)\n",
              "print(onnx.helper.printable_graph(model.graph))\n",
              "\n",
              "from onnx_coreml import convert\n",
              "mlmodel = convert(model)\n",
              "\n",
              " <P> \n",
              "class MGenDenseNet(nn.Module):\n",
              "  def __init__(self, ngpu, growth_rate=32, block_config=(16,24,12,6), in_size=1024, drop_rate=0.0):\n",
              "    super(MGenDenseNet, self).__init__()\n",
              "    import pdb; pdb.set_trace()\n",
              "    self.ngpu = ngpu\n",
              "    self.features = nn.Sequential()\n",
              "    self.features.add_module('btch0', nn.BatchNorm2d(in_size))\n",
              "    block_placeholder = DenseLayer \n",
              "    num_features = in_size\n",
              "    for i, num_layers in enumerate(block_config):\n",
              "      block = DenseBlock(num_layers=num_layers, in_size=num_features, growth_rate=growth_rate, block=block_placeholder, droprate=drop_rate)  look at change\n",
              "      self.features.add_module('denseblock{}'.format(i+1), block)\n",
              "      num_features -= num_layers*growth_rate\n",
              "    self.features.add_module('convfinal', nn.ConvTranspose2d(num_features, 3, kernel_size=7, stride=2, padding=3, bias=False))\n",
              "    self.features.add_module('Tanh', nn.Tanh())\n",
              "  def forward(self, input):\n",
              "    return self.features(input)\n",
              "\n",
              "It is because you define block as DenseLayer, then reassign block it to an initalized DenseBlock() and then pass that as block=block. So after one iteration through the for loop it is passing a DenseBlock() object instead of DenseLayer so it's wrongly using the forward pass.\n",
              "Just change block = DenseLayer to block_placeholder and use that variable instead.\n",
              "I spotted this by placing a debugger in your code and noticing that the DenseBlock line only fails on second call.\n",
              " <P> I found the answer myself, the hidden state of the GRU was still attached to the last batch run, so it had to be detached using\n",
              "h.detach_()\n",
              "\n",
              " <P> The output that you have generated vgg16(input), thats still in cuda. This is so because this output is used for calculating the loss afterwards. So to avoid having your output being stored in CUDA and eat up your GPU memory, move it to CPU using .cpu().numpy(). If that throws an error, you might have to use .detach() as well to detach the variable.\n",
              " <P> Okay, found a solution here\n",
              "https://discuss.pytorch.org/t/how-to-create-mlp-model-with-arbitrary-number-of-hidden-layers/13124\n",
              "\n",
              "Basically, I have to register the layers with the module.\n",
              " <P> nn.Module don't have a predict function, just call the object for inference:\n",
              "prediction = model(img_reshape)\n",
              "\n",
              "This will call the object's __call__ function which, in turns, callsthe model forward function.\n",
              " <P> The first argument to super() should be class itself, not a different class.\n",
              "\n",
              "class LSTMCell(nn.Module):\n",
              "\n",
              "    def __init__(self, input_size, hidden_size, bias=True):\n",
              "        super(LSTM, self).__init__()\n",
              "#             ^^^^ self is not an instance of LSTM but LSTMCell\n",
              "\n",
              "\n",
              "It should be:\n",
              "\n",
              "super(LSTMCell, self).__init__()\n",
              "\n",
              "\n",
              "Since Python 3 you can omit the arguments to super to get the same result (as you have done in the LSTM class):\n",
              "\n",
              "super().__init__()\n",
              "\n",
              " <P> Short answer, you are in a tough spot.\n",
              "Long answer, it's difficult yet possible. What makes your problem difficult is your graph is already trained. It is inefficient, yet easier to convert NCHW-NHWC while you create the training graph. See similar answer here and here.\n",
              "Now to your answer, you'll have to overload conv2D operator with custom convolution operator. Here is a pseudo code to get started.\n",
              "   tensor Conv2D(X, W, B) {\n",
              "     int perm[] = {0, 3, 1, 2};\n",
              "     X = transposeTensor(X, perm);\n",
              "     W = transposeTensor(W, perm);\n",
              "     Y = Conv2D_orig(X, W, B, ...) ;\n",
              "     perm = {0, 2, 3, 1};\n",
              "     return transposeTensor(Y, perm);\n",
              "   }\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row70\" class=\"row_heading level0 row70\" >70</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col0\" class=\"data row70 col0\" >Enabling AVX512 vectorization for `qadaptive_avg_pool2d_nhwc_kernel` & `qavg_pool2d_nhwc_kernel`</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col1\" class=\"data row70 col1\" >This is a duplicate of #2793 which is implemented in #52695. I'm sure, this is introduced by \n",
              " \n",
              " In #49978 I try to change the magma includes to magma_v2.h. I haven't seen it with OMP_NUM_THREADS=1.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col2\" class=\"data row70 col2\" >This issue is fixed</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row70_col3\" class=\"data row70 col3\" ><P> See https://github.com/pytorch/pytorch/pull/5225 <P> This is a duplicate of #2793 which is implemented in #52695. <P> Yea, this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688 <P> I can reproduce this with OMP_NUM_THREADS=2. I haven't seen it with OMP_NUM_THREADS=1. <P> > Some changes in code would need to be made so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h.\n",
              " \n",
              " \n",
              " \n",
              " In #49978 I try to change the magma includes to magma_v2.h. <P> i'm working on this today. <P> I see, that makes sense. Thanks for the response @mruberry.\n",
              "\n",
              "> If, however, there's some functionality that HyperLSTM needs that's tricky to implement in PyTorch, then it makes sense to file a more focused issue requesting that functionality.\n",
              "\n",
              "There is one thing, however I don't know if I should open another issue or if it's not really worth discussing.\n",
              "\n",
              "Inside the hyperLSTM, two regular LSTM's exist. However, one of them needs LayerNormalization applied to the variable `c` (the long term memory) before it is multiplied by the output gate. The original paper on LayerNormalization also describes the same technique. \n",
              "\n",
              "Is it worthy to open another issue to request this feature be implemented (a boolean `layerNorm` passed to the LSTM)? Or is this is a niche use-case where it would be better off if I just implement my own variant of an LSTM that includes LayerNormalization. <P> Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow <P> The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added. <P> it's not widely used enough yet, to be pushed into core (feel free to reopen once it becomes more of a standard).\n",
              "Additionally, we are working on a user handwritten RNNs being fast, rather than adding more fundamental multi-layer RNNs into core.\n",
              "So, I'm closing the feature request.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row71\" class=\"row_heading level0 row71\" >71</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col0\" class=\"data row71 col0\" >Error implementing torch.optim.lr_scheduler.LambdaLR in Pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col1\" class=\"data row71 col1\" >The issue is caused by this line here\n",
              "\n",
              "scheduler = torch.optim.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
              "As the error suggests you are trying to reference value before it has been assigned,i.e. the lambda function is called with itself as the argument which is currently not assigned to anything.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col2\" class=\"data row71 col2\" >The issue is caused by this line here\n",
              "\n",
              "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
              "\n",
              "\n",
              "As the error suggests you are trying to reference value before it has been assigned,i.e. the lambda function is called with itself as the argument which is currently not assigned to anything. As a result of this, an error is raised in lr_scheduler.py\n",
              "\n",
              "Maybe you want to pass something else to the lambda function.\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row71_col3\" class=\"data row71 col3\" ><P> The issue is caused by this line here\n",
              "\n",
              "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
              "\n",
              "\n",
              "As the error suggests you are trying to reference value before it has been assigned,i.e. the lambda function is called with itself as the argument which is currently not assigned to anything. As a result of this, an error is raised in lr_scheduler.py\n",
              "\n",
              "Maybe you want to pass something else to the lambda function.\n",
              " <P> This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\n",
              "\n",
              "I realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\n",
              "\n",
              "I've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\n",
              "`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\n",
              "(Just realized the error has a typo in \"greatly\" but oh well ;))\n",
              "\n",
              "You could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup..'`) on model save.\n",
              "\n",
              "@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast. <P> You could multiply the weights, if you are suspecting rounding errors or any other numerical issues.\n",
              "The weights are used relatively, so you should be able to add a constant offset to the tensor. <P> If the code you posted is the exact code you use, the problem is that you don't actually call backward on the loss (missing parentheses ()).\n",
              " <P> Another common reason for this error could be if the number of classes in the labels does not match with the number of units in the final softmaxed Linear Layer for a classification problem. I recently encountered this accidentally. <P> You could multiply the weights, if you are suspecting rounding errors or any other numerical issues.The weights are used relatively, so you should be able to add a constant offset to the tensor <P> Could it be that you are operating with a very constrained memory? From the error messages, it looks like you are erroring out in cudnnSetDropoutDescriptor. The kernel initializing dropout states requires a lot of stack memory (grrrrrrrrr, XORWOW!), and it might return the error you are seeing if it is not able to allocate it. <P> Setting set_detect_anomaly doesnt give any other output Make sure to use latest pytorch as we recently fix warnings not showing up in colab.Or run your code in command line to have the corresponding forward code.The code does quite a lot of inplace and viewing ops.Plus, it is working without any error, when Pytorch is downgraded to 1.5.0.This kind of check is here to make sure we don‚Äôt compute silently wrong gradients. So it is most likely that the old behavior was silently computing wrong gradients and has been fixed in more recent versions. <P> I faced the same problem. The problem was solved by changing the number of classes.\n",
              "\n",
              "num_classes = 10 (changed to the actual class number, instead of 1)\n",
              " <P> I think no_epochs=0 with this initialization. Possibly (len(train_loader) / batch_size) > n_iterations. Then int(no_eps) = 0. Try to change no_epochs to 100 manually, for example.\n",
              "\n",
              "no_eps = n_iterations / (len(train_loader) / batch_size)\n",
              "no_epochs = int(no_eps)\n",
              "for epoch in range(no_epochs):\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row72\" class=\"row_heading level0 row72\" >72</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col0\" class=\"data row72 col0\" >How to find c++ source code of torch.bmm of pytorch</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col1\" class=\"data row72 col1\" >If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in pytorch/onnx/symbolic_opset.py and follow the instructions listed as below: Define the symbolic Function in print( torch/ONNX/SymbolicOperator.py, for example torch/ONNX/SYMPOLATE_OPSET9.py). Make sure the function has the same</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col2\" class=\"data row72 col2\" >- func: bmm(Tensor self, Tensor mat2) - Tensor\n",
              "  use_c10_dispatcher: full\n",
              "  variants: function, method\n",
              "  dispatch:\n",
              "    CPU: bmm_cpu\n",
              "    CUDA: bmm_cuda\n",
              "    SparseCPU: bmm_sparse_cpu\n",
              "    SparseCUDA: bmm_sparse_cuda\n",
              "  supports_named_tensor: True\n",
              "\n",
              "\n",
              "The implementations (like bmm_cpu) are to be found in aten\\src\\ATen\\native\\LinearAlgebra.cpp.\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row72_col3\" class=\"data row72 col3\" ><P> ReduceOp is a C++ enum, and is exposed to the python interface using pybind (https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/init.cpp#L145). That enum is defined here: https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/Types.hpp#L8 <P> I was able to get it working by doing the following:\n",
              "\n",
              "\n",
              "  PyStorm 2019.3\n",
              "\n",
              "\n",
              "Open the settings for external documentation:\n",
              "\n",
              "\n",
              "  File / Settings / Tools / External Documentation\n",
              "\n",
              "\n",
              "Add the following URL patterns:\n",
              "\n",
              "Module Name: torch.nn.functional\n",
              "        URL: https://pytorch.org/docs/stable/nn.functional.html#{element.qname}\n",
              "\n",
              "\n",
              "Module Name: torch\n",
              "        URL: https://pytorch.org/docs/stable/{module.basename}.html#{element.qname}\n",
              "\n",
              "\n",
              "Seems to work for most APIs but you have to trigger the quick documentation tool window. This won't show docs if you CTRL+CLICK something.\n",
              " <P> To achieve this, developers need to touch the source code of PyTorch. Please follow the instructions for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the ONNX operator list. If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset.py, for example torch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen operator/function defined in VariableType.h. <P> tl;dr Upgrade to PyTorch 0.4.1\n",
              "\n",
              "\n",
              "\n",
              "Notice that DGL requires PyTorch 0.4.1 and you are using PyTorch 0.4.0. If you take a closer look, you'll see that as_tensor was proposed in 30 Apr 2018 and merged in 1 May 2018. You'll also see that PyTorch 0.4.0 was released before that on 24 Apr 2018, whereas PyTorch 0.4.1 was release after on 26 Jul 2018. In fact, if you take a look at the changelog of the 0.4.1 version, you'll notice a new operator being announced: torch.as_tensor :)\n",
              " <P> If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset.py, for example torch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen operator/function defined in VariableType.h. The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in VariableType.h, because dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this: <P> If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset.py, for example torch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen operator/function defined in VariableType.h. The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in VariableType.h, because dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. <P> You can also check source code directly from PyTorch documentation.\n",
              "See here for torch.nn.Module.parameters function (just click orange \"Source\", which gets you here).\n",
              "Source is linked if it isn't written in C/C++/low level, in this case you have to go through GitHub and get around the project.\n",
              " <P> see Subtensor add and division in libtorch with link https://discuss.pytorch.org/t/subtensor-add-and-division-in-libtorch/63224  <P> the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source <P> Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch. Please follow the instructions for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the ONNX operator list. If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset.py, for example torch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen operator/function defined in VariableType.h. The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in VariableType.h, because dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row73\" class=\"row_heading level0 row73\" >73</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col0\" class=\"data row73 col0\" >why cannot cuda model be initialized under the __init__ method in a class that inherits multiprocessing.process?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col1\" class=\"data row73 col1\" >The above quote can be understood that nn.DataParallel is just a wrapper class to inform model.cuda() should make a multiple copies to GPUs.\n",
              "\n",
              "In my case, I don't have any GPU on my laptop. I still call N.DataPallel() without any problem.\n",
              "from torch import nn\n",
              "import torch\n",
              "\n",
              "Set device to CUDA if CUDA is available\n",
              "\n",
              "module = model.module.to(device)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col2\" class=\"data row73 col2\" >Error occurs because in python multiprocessing requires Process class objects to be pickelable so that data can be transferred to the process being created i.e. Serialisation and deserialization of the object. Suggestion to overcome the issue, lazy instantiate the Helmet_Detector object (hint: try property in python).\n",
              " you should use pytorch's multiprocessing library instead of standard multiprocessing library\n",
              "\n",
              "Example:\n",
              "\n",
              "import torch.multiprocessing as mp\n",
              "\n",
              "class Processor(mp.Process):\n",
              ".\n",
              ".\n",
              ".\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row73_col3\" class=\"data row73 col3\" ><P> QUESTION: Suppose each process has a different random generator state, when DistributedDataParallel is initialized does each process need to have the same parameter values?No. Rank 0 will broadcast model states to all other ranks when you construct DDP. Code for that is here.In order to evaluate, on one GPU, can we use ddp_model.module? Yes, this should work.Can we use something like EMA to copy new parameters to ddp_model.module and then restore them after evaluation?Yes, if you make sure you restored those model param values correctly. Otherwise, if this introduces inconsistency across param values across different processes, DDP will not fix that for you, as DDP only syncs grad instead of params. This might be helpful to explain. In order to save the model, can we use ddp_model.module Yes. And when you restore from the checkpoint, it‚Äôs better to reconstruct the DDP instance using the restored module to make sure that DDP starts from a clean state. Do we need to use torch.distributed.barrier so that the other processes don‚Äôt continue training while the master evaluates? It‚Äôs recommended this way. But if you are not consuming the checkpoint right away and not worried about timeout due to rank0 is doing more work, this is not necessary. Because the next DDP backward will launch allreduce comm ops, which will sync anyway. Some of this is also explained here. <P> Pytorch provides DataParallel module to run a model on mutiple GPUs. Detailed documentation of DataParallel and toy example can be found here and here.\n",
              " <P> You should get the neural network out of DataParallel first.\n",
              "Assuming your DataParallel is named model you could do:\n",
              "device = torch.device(\"cuda:1\")\n",
              "module = model.module.to(device)\n",
              "\n",
              " <P> They add few lines in the tutorial to explain nn.DataParallel. \n",
              "\n",
              "\n",
              "  DataParallel splits your data automatically, and send job orders to multiple models on different GPUs using the data. After each model finishes their job, DataParallel collects and merges the results for you.\n",
              "\n",
              "\n",
              "The above quote can be understood that nn.DataParallel is just a wrapper class to inform model.cuda() should make a multiple copies to GPUs.\n",
              "\n",
              "In my case, I don't have any GPU on my laptop. I still call nn.DataParallel() without any problem.\n",
              "\n",
              "import torch\n",
              "import torchvision\n",
              "\n",
              "model = torchvision.models.alexnet()\n",
              "model = torch.nn.DataParallel(model)\n",
              "# No error appears if I don't move the model to `cuda`\n",
              "\n",
              " <P> When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note <P> DataParallel should work on a single GPU as well, but you should check if args.gpus only contains the id of the device that is to be used (should be 0) or None.\n",
              "Choosing None will make the module use all available devices.\n",
              "\n",
              "Also you could remove DataParallel as you do not need it and move the model to GPU only by calling model.cuda() or, as I prefer, model.to(device) where device is the device's name.\n",
              "\n",
              "Example:\n",
              "\n",
              "This example shows how to use a model on a single GPU, setting the device using .to() instead of .cuda().\n",
              "\n",
              "from torch import nn\n",
              "import torch\n",
              "\n",
              "# Set device to cuda if cuda is available\n",
              "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
              "\n",
              "# Create model\n",
              "model = nn.Sequential(\n",
              "  nn.Conv2d(1,20,5),\n",
              "  nn.ReLU(),\n",
              "  nn.Conv2d(20,64,5),\n",
              "  nn.ReLU()\n",
              ")\n",
              "\n",
              "# moving model to GPU\n",
              "model.to(device)\n",
              "\n",
              "\n",
              "If you want to use DataParallel you could do it like this\n",
              "\n",
              "# Optional DataParallel, not needed for single GPU usage\n",
              "model1 = torch.nn.DataParallel(model, device_ids=[0]).to(device)\n",
              "# Or, using default 'device_ids=None'\n",
              "model1 = torch.nn.DataParallel(model).to(device)\n",
              "\n",
              " <P> Please use forum (discuss.pytorch.org) for questions. DataParallel uses multiple GPUs only for forward method, if you want to use multiple GPUs for other methods, you have to manually do it. <P> Call .cuda() on the model during initialization.\n",
              "As per your above comments, you have GPUs, as well as CUDA installed, so there's no point of checking the device availability with torch.cuda.is_available().\n",
              "Additionally, you should wrap your model in nn.DataParallel to allow PyTorch use every GPU you expose it to. You also could do DistributedDataParallel, but DataParallel is easier to grasp initially.\n",
              "Example initialization:\n",
              "model = UNet().cuda()\n",
              "model = torch.nn.DataParallel(model)\n",
              "\n",
              "Also, you can be sure you're exposing the code to all GPUs by executing the python script with the following flag:\n",
              "CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train_unet.py\n",
              "\n",
              "Last thing to note - nn.DataParallel encapsulates the model itself, so for saving the state_dict, you'll need to reach module inside DataParallel:\n",
              "torch.save(model.module.state_dict(), 'unet.pth')\n",
              "\n",
              " <P> You trained your model using DataParallel and saved it. So, the model weights were stored with a module. prefix. Now, when you load without DataParallel, you basically are not loading any model weights (the model has random weights). As a result, the model predictions are wrong.\n",
              "\n",
              "I am giving an example.\n",
              "\n",
              "model = nn.Linear(2, 4)\n",
              "model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
              "model.state_dict().keys() # = odict_keys(['module.weight', 'module.bias'])\n",
              "\n",
              "\n",
              "On the other hand,\n",
              "\n",
              "another_model = nn.Linear(2, 4)\n",
              "another_model.state_dict().keys() # = odict_keys(['weight', 'bias'])\n",
              "\n",
              "\n",
              "See the difference in the OrderedDict keys.\n",
              "\n",
              "So, in your code, the following three-line works but no model weights are loaded.\n",
              "\n",
              "pretrained_dict = model_old.state_dict()\n",
              "model_dict = model.state_dict()\n",
              "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
              "\n",
              "\n",
              "Here, model_dict has keys without the module. prefix but pretrained_dict has when you do not use DataParalle. So, essentially pretrained_dict is empty when DataParallel is not used.\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  Solution: If you want to avoid using DataParallel, or you can load the weights file, create a new OrderedDict without the module prefix, and load it back.\n",
              "\n",
              "\n",
              "Something like the following would work for your case without using DataParallel.\n",
              "\n",
              "# original saved file with DataParallel\n",
              "model_old = torch.load(path, map_location=dev)\n",
              "\n",
              "# create new OrderedDict that does not contain `module.`\n",
              "from collections import OrderedDict\n",
              "\n",
              "new_state_dict = OrderedDict()\n",
              "for k, v in model_old.items():\n",
              "    name = k[7:] # remove `module.`\n",
              "    new_state_dict[name] = v\n",
              "\n",
              "# load params\n",
              "model.load_state_dict(new_state_dict)\n",
              "\n",
              " <P> You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU, as described in @soumith's link (Important notices, item 4).</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row74\" class=\"row_heading level0 row74\" >74</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col0\" class=\"data row74 col0\" >PyTorch Dataloader - List is not callable error when enumerating</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col1\" class=\"data row74 col1\" >Your __len__ is just 1 greater than your actual data, since you loaded the df with header=None.\n",
              "Simply change the last line of __init__ to self.train_size=self.data_len = len(self.image_arr). This should fix your problem with minimal change.\n",
              "(Alternatively, set header=True while loading your df, in which case, you'll have to change iloc[1:,...] to ILOC[:,...], as you no longer need to skip the first row.)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col2\" class=\"data row74 col2\" >Did you remember to call transforms.Compose on your list of transforms?\n",
              "\n",
              "In this line\n",
              "\n",
              "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
              "the transform parameter is expecting a callable object, not a list.\n",
              "\n",
              "So, for example, this is wrong:\n",
              "\n",
              "train_transforms = [\n",
              "    transforms.RandomResizedCrop(224),\n",
              "    transforms.RandomHorizontalFlip(),\n",
              "    transforms.ToTensor(),\n",
              "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
              "It should look like this\n",
              "\n",
              "train_transforms = transforms.Compose([\n",
              "    transforms.RandomResizedCrop(224),\n",
              "    transforms.RandomHorizontalFlip(),\n",
              "    transforms.ToTensor(),\n",
              "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row74_col3\" class=\"data row74 col3\" ><P> you have a typo here:\n",
              "\n",
              "data_alice = x_data[2:0]\n",
              "target_alice = y_data[2:0]\n",
              "\n",
              "\n",
              "Should be [2:]\n",
              "\n",
              "Because data_alice is failing, you had this error.\n",
              " <P> I just put\n",
              "def collate_fn(batch):\n",
              "    data_list, label_list = [], []\n",
              "    for _data, _label in batch:\n",
              "        data_list.append(_data)\n",
              "        label_list.append(_label)\n",
              "    return torch.Tensor(data_list), torch.LongTensor(label_list)\n",
              "\n",
              "in my code and it works.\n",
              " <P> Your __len__ is just 1 greater than your actual data, since you loaded the df with header=None.\n",
              "Simply change the last line of __init__ to self.data_len = len(self.image_arr). This should fix your problem with minimal change.\n",
              "(Alternatively, set header=True while loading your df, in which case, you'll have to change iloc[1:, ...] to iloc[:, ...], as you no longer need to skip the first row.)\n",
              " <P> Your transform variable is unused, it should be passed to the Dataset constructor:\n",
              "`dataset = torchvision.datasets.ImageFolder('datasets', transform=transform)`\n",
              "\n",
              "Because of that, the ToTensor is never applied to your data, and thus they remain PIL images, not tensors.\n",
              " <P> I was able to solve this problem! It turns out I didn't have to inherit datasets.DatasetFolder. Since the labels were the same, I just created one class which inherits datasets.ImageFolder, and fed a modified path to the function npy_loader.\n",
              " <P> To use Dataset as an iterable you must implement either __iter__ method or __getitem__ with Sequence semantics. The iteration stops when method __getitem__ raises IndexError for some index idx\n",
              "The problem with your dataset is that:\n",
              "self.embeddings = h5py.File(self.val_embed_path, 'r')[\"embeds\"]\n",
              "\n",
              "is actually of type h5py._hl.dataset.Dataset which on out-of-index requests raises ValueError\n",
              "You have to either load entire embeddings at the class constructor so that accessing numpy array  on out-of-index will raise IndexError  or re-throw IndexError on ValueError in __getitem__\n",
              " <P> You haven't built the vocab for the LABEL field.\n",
              "After TEXT.build_vocab(train, ...), run LABEL.build_vocab(train), and the rest will run.\n",
              " <P> I had the same problem.\n",
              "The reason was that some rows in my input csv dataset were empty.\n",
              " <P> The rename happens in the collator. In the trainer init, when data_collator is None, a default one is used:\n",
              "\n",
              "class Trainer:\n",
              "    # ...\n",
              "    def __init__(...):\n",
              "        # ...\n",
              "        self.data_collator = data_collator if data_collator is not None else default_data_collator\n",
              "        # ...\n",
              "\n",
              "\n",
              "FYI, the self.data_collator is later used when you get the dataloader:\n",
              "\n",
              "data_loader = DataLoader(\n",
              "    self.train_dataset,\n",
              "    batch_size=self.args.train_batch_size,\n",
              "    sampler=train_sampler,\n",
              "    collate_fn=self.data_collator,              # -- here\n",
              "    drop_last=self.args.dataloader_drop_last,\n",
              ")\n",
              "\n",
              "\n",
              "The default collator has a special handling for labels, which does this renaming, if needed:\n",
              "\n",
              "# Special handling for labels.\n",
              "# Ensure that tensor is created with the correct type\n",
              "# (it should be automatically the case, but let's make sure of it.)\n",
              "if hasattr(first, \"label\") and first.label is not None:\n",
              "    if type(first.label) is int:\n",
              "        labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
              "    else:\n",
              "        labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
              "    batch = {\"labels\": labels}  # -- here is where it happens\n",
              "elif hasattr(first, \"label_ids\") and first.label_ids is not None:\n",
              "    if type(first.label_ids[0]) is int:\n",
              "        labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
              "    else:\n",
              "        labels = torch.tensor([f.label_ids for f in features], dtype=torch.float)\n",
              "    batch = {\"labels\": labels}\n",
              "else:\n",
              "    batch = {}\n",
              "\n",
              " <P> I can't completely understand your problem as your code is not formatted properly in your question but the error is just an expected datatype error.\n",
              "You need to convert your dataframe to a np array. Just add .values at the end of your dataframe. So if your input was a sample dataframe like so:\n",
              "df = pd.DataFrame({\"Col1\":[1,2,3,4], \"Col2\":[2,2,3,4]})\n",
              "\n",
              "Convert the whole thing to a numpy array like so:\n",
              "sample_array = df.values\n",
              "\n",
              "or convert one column to a np array like so:\n",
              "sample_array_2 = df[\"Col1\"].values\n",
              "\n",
              "Update:\n",
              "As mentioned in the comments, pandas recommends .to_numpy() instead, so use something like:\n",
              "sample_array = df.to_numpy()\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row75\" class=\"row_heading level0 row75\" >75</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col0\" class=\"data row75 col0\" >Feature Request: Length Masking for RNNs</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col1\" class=\"data row75 col1\" >I can't believe I made this silly mistake in verson1 queries are outputted from w_v, instead of w_Q. I'll put up a PR.\n",
              "\n",
              "Should it be\n",
              "`v_{t+1} = p_{t}*v_{T} + g_{t +1}`? (Added a subscript for `p`)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col2\" class=\"data row75 col2\" >But sometimes we have more than one input. We can't sort both seq1 and seq2 at the same time. Like machine comprehension model, our inputs is: question-passage.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row75_col3\" class=\"data row75 col3\" ><P> @colesbury batching along 2nd dimension is common, since the standard input format for all recurrent layers in `nn` is `seq_length x batch_size x dim`. <P> I can't  believe I made this silly mistake in verson1 queries are outputted from w_v, instead of w_q. <P> IMO, this is a no brainer. I'll put up a PR.\n",
              "\n",
              "Should it be\n",
              "`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`) <P> @carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\n",
              "\n",
              "From [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\n",
              "> ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken <P> 4 levels are\n",
              "\n",
              "Batch\n",
              "Channel\n",
              "Row\n",
              "Column\n",
              "\n",
              " <P> Thanks for the proposal. Yes, it's all the way to `nn.MultiheadAttention`. We have a plan to refactor nn.MultiheadAttention by splitting the long function into several pieces. And we can make an option to accept a 3-D mask there. <P> I have not yet fully analyzed the problem and I am not sure if the observed effect is a bug or by design.\n",
              " \n",
              " \n",
              " \n",
              " It seems that in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect. \n",
              " \n",
              " \n",
              " \n",
              " I modified the repro script a bit in order to get the effect without `add_zero_attn=True` (initial in-projection bias should be 0) and to split the contribution of query and key & value: \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " import torch\n",
              " \n",
              " \n",
              " \n",
              " embedding_dim = 1\n",
              " \n",
              " batch_size = 1\n",
              " \n",
              " num_heads = 1\n",
              " \n",
              " seq_len = 4\n",
              " \n",
              " \n",
              " \n",
              " net = torch.nn.MultiheadAttention(embedding_dim, num_heads, add_zero_attn=False)\n",
              " \n",
              " mask = torch.cat([torch.ones(seq_len, seq_len).triu(), torch.zeros(seq_len, 1)], dim=1)\n",
              " \n",
              " mask[mask==1]=float('-inf')\n",
              " \n",
              " print(mask)\n",
              " \n",
              " \n",
              " \n",
              " for i in range(seq_len):\n",
              " \n",
              "  x = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n",
              " \n",
              "  y = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n",
              " \n",
              "  z = torch.cat([y, torch.zeros(1, 1, embedding_dim)]) # add zero sequence element\n",
              " \n",
              "  o, w = net(x, z, z, attn_mask=mask)\n",
              " \n",
              "  #print(w)\n",
              " \n",
              "  # o.shape is (seq_len, batch_size, embedding_dim)\n",
              " \n",
              "  o.mean([1, 2])[i].backward()\n",
              " \n",
              "  print(i, 'x:', x.grad.abs().sum([1, 2]).view(-1))\n",
              " \n",
              "  print(i, 'y:', y.grad.abs().sum([1, 2]).view(-1))\n",
              " \n",
              "  \n",
              " \n",
              " \n",
              " \n",
              " Output is:\n",
              " \n",
              " \n",
              " \n",
              " tensor([[-inf, -inf, -inf, -inf, 0.],\n",
              " \n",
              "  [0., -inf, -inf, -inf, 0.],\n",
              " \n",
              "  [0., 0., -inf, -inf, 0.],\n",
              " \n",
              "  [0., 0., 0., -inf, 0.]])\n",
              " \n",
              " 0 x: tensor([0., 0., 0., 0.])\n",
              " \n",
              " 0 y: tensor([0., 0., 0., 0.])\n",
              " \n",
              " 1 x: tensor([0.0000, 0.0148, 0.0000, 0.0000])\n",
              " \n",
              " 1 y: tensor([0.2801, 0.0000, 0.0000, 0.0000])\n",
              " \n",
              " 2 x: tensor([0.0000, 0.0000, 0.0127, 0.0000])\n",
              " \n",
              " 2 y: tensor([0.1798, 0.1798, 0.0000, 0.0000])\n",
              " \n",
              " 3 x: tensor([0.0000, 0.0000, 0.0000, 0.0105])\n",
              " \n",
              " 3 y: tensor([0.1323, 0.1323, 0.1323, 0.0000])\n",
              " \n",
              "  <P> A way to make the code cleaner would just to skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level <P> There may be some formal or informal expectations between different implentations of a given ONNX operator (I'm not sure). For that you could ask in the ONNX GitHub or Slack. But between ONNX and PyTorch I don't think there's any standard other than basic correctness. <P> @glample CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row76\" class=\"row_heading level0 row76\" >76</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col0\" class=\"data row76 col0\" >GPU torch.multinomial produces an out-of-bounds index</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col1\" class=\"data row76 col1\" >This is expected, as it's a float32 limitation.\n",
              "From the [Wikipedia article]( (\n",
              "> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between −167772 16 and 0)\n",
              "Integers between `2**24=1677F7216` and `2->25=33554432` round to a multiple of 2 (even number)\n",
              "\n",
              "Implemented using torch.lu_unpack(). Warning</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col2\" class=\"data row76 col2\" >In case this is helpful to anyone, a possible temporary workaround is to use\n",
              "\n",
              "_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n",
              "\n",
              "where `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row76_col3\" class=\"data row76 col3\" ><P> @carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return <P> I don't think the GPU behavior is completely wrong, for a mathematical reason: it should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.\n",
              " \n",
              " \n",
              " \n",
              " The way Numpy gets out of this situation, is they ask you for an initial element to handle the case explicitly, and error if you don't provide it. So in the end I agree with your suggested course of action. <P> Thanks for catching this! We would accept a PR to fix this. <P> fyi, @nairbv <P> This is expected, as it's a float32 limitation.\n",
              "From the [Wikipedia article](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\n",
              "> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between −16777216 and 0)\n",
              "Integers between `2**24=16777216` and `2**25=33554432` round to a multiple of 2 (even number)\n",
              "Integers between `2**25` and `2**26` round to a multiple of 4\n",
              "... <P> We would accept a PR to fix this. <P> @albanD Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes. <P> We would definitely accept a PR fixing these! <P> This function does not check if the factorization was successful or not if get_infos is True since the status of the factorization is present in the third element of the return tuple. Note In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning <P> Made default to 0.\n",
              "fixed via 4eb12a2</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row77\" class=\"row_heading level0 row77\" >77</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col0\" class=\"data row77 col0\" >How do you add a symbolic function in the corresponding Function class?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col1\" class=\"data row77 col1\" >Forward passes of anything in pytorch, layer or network, is done in functional way var(x). This is made in python with overwriting __call__() built-in. Try it yourself. Make a class, overwrite __call __() and use it like function.\n",
              "\n",
              "Dynamic_axes (dict> or dict, default empty dict) – A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes for provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col2\" class=\"data row77 col2\" >Create a symbolic function named symbolic</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row77_col3\" class=\"data row77 col3\" ><P> Forward passes of anything in pytorch, layer or network, is done in functional way var(x). This is made in python with overwriting __call__() built-in. Try it yourself. Make a class, overwrite __call__() and use it like function.\n",
              " <P> dynamic_axes (dict> or dict, default empty dict) –  a dictionary to specify dynamic axes of input/output, such that: - KEY:  input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) –  If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version < 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored. <P> This function can also equivalently be used as a decorator: A wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through. fn_or_name (Union[str, Callable]) – The function or name of the global function to insert into the graph when it’s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph. Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code. Construct a GraphModule. <P> grad_fn is a function \"handle\", giving access to the applicable gradient function.  The gradient at the given point is a coefficient for adjusting weights during back-propagation.\n",
              "\"Handle\" is a general term for an object descriptor, designed to give appropriate access to the object.  For instance, when you open a file, open returns a file handle.  When you instantiate a class, the __init__ function returns a handle to the created instance.  The handle contains references (usually memory addresses) to the data and functions for the item in question.\n",
              "It appears as the generic object class because it's from the underlying implementation in another language, such that it does not map exactly to the Python function type.  PyTorch handles the inter-language call and return.  This hand-off is part of the pre-complied (shared-object) run-time system.\n",
              "Is that enough to clarify what you see?\n",
              " <P> map works on a single the same way it works on list/tuple of lists, it fetches an element of the given input regardless what is it.\n",
              "\n",
              "The reason why torch.tensor works, is that it accepts a list as input.\n",
              "\n",
              "If you unfold the following line you provided:\n",
              "\n",
              "x_train, y_train, x_valid, y_valid = map(\n",
              "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
              ")\n",
              "\n",
              "\n",
              "it's the same as doing:\n",
              "\n",
              "x_train, y_train, x_valid, y_valid = [torch.tensor(x_train), torch.tensor(y_train), torch.tensor(x_valid), torch.tensor(y_valid)]\n",
              "\n",
              "\n",
              "On other hand, your sqr function does not accept lists. It expects a scalar type to square, which is not the case for your a an b, they are lists. \n",
              "\n",
              "However, if you change sqr to:\n",
              "\n",
              "def sqr(a):\n",
              "    return [s * s for s in a]\n",
              "\n",
              "\n",
              "a = [1, 2, 3, 4]\n",
              "b = [1, 3, 5, 7]\n",
              "\n",
              "a, b = map(sqr, (a, b))\n",
              "\n",
              "\n",
              "or as suggested by @Jean, a, b = map(sqr, x) for x in (a, b)\n",
              "\n",
              "It will work. \n",
              " <P> You can use nn.Sequential() to simulate identity <P> If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph. <P> If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator. We try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator. We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override; in VariableType.h. This means elu is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in symbolic_opset9.py, symbolic_opset10.py. The interface for specifying operator definitions is experimental; adventurous users should note that the APIs will probably change in a future interface. <P> Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator. We try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator. We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override; in VariableType.h. This means elu is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in symbolic_opset9.py, symbolic_opset10.py. <P> The Python documentation table Mapping Operators to Functions provides canonical mappings from:\n",
              "\n",
              "operator -> __function__()\n",
              "\n",
              "Eg:\n",
              "\n",
              "\n",
              "Matrix Multiplication        a @ b        matmul(a, b)\n",
              "\n",
              "\n",
              "\n",
              "Elsewhere on the page, you will see the __matmul__ name as an alternate to matmul.\n",
              "\n",
              "The definitions of the PyTorch __functions__ are found either in:\n",
              "\n",
              "\n",
              "The torch.Tensor module documentation\n",
              "python_variable_methods.cpp\n",
              "\n",
              "\n",
              "You can look up the documentation for the named functions at:\n",
              "\n",
              "https://pytorch.org/docs/stable/torch.html?#torch.FUNCTION-NAME\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row78\" class=\"row_heading level0 row78\" >78</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col0\" class=\"data row78 col0\" >Calculate the standard deviation of a moving windows using 2d convolution</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col1\" class=\"data row78 col1\" >So they basically have 1 original image, which they treat as the left side view for the depth perception algorithm, but since you need stereo vision to calculate depth in a still image they use a neural structure to synthesise a right side view.\n",
              "1 Dimensional Correlation takes 2 sequences and calculates the correlation at each point giving you another 1D sequence of the same length as the 2 inputs. So if you apply this correlation along a certain axis of a tensor the resultant tensor does not change shape.\n",
              "Intuitively they thought it made sense to correlate the images along the horizontal axes a bit like reading the images like</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col2\" class=\"data row78 col2\" >I think it can be somehow like this:\n",
              "\n",
              "\n",
              "mean (img) using convolution.\n",
              "Subtract from the original image of p. 1\n",
              "Calculate the square of each element.\n",
              "By convolution we find the average, p. 3\n",
              "Calculates the square root of the elements of p. 4\n",
              "\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row78_col3\" class=\"data row78 col3\" ><P> So they basically have 1 original image, which they treat as the left side view for the depth perception algorithm, but since you need stereo vision to calculate depth in a still image they use a neural structure to synthesise a right side view.\n",
              "\n",
              "1 Dimensional Correlation takes 2 sequences and calculates the correlation at each point giving you another 1D sequence of the same length as the 2 inputs. So if you apply this correlation along a certain axis of a tensor the resultant tensor does not change shape.\n",
              "\n",
              "Intuitively they thought it made sense to correlate the images along the horizontal axes a bit like reading the images like reading a book, but in this instance it should have an effect akin to identifying that things that are further away also appear to be points that are closer together in the left and right side views. The correlation is probably higher for left and right side data-points that are further away and this makes the depth classification for the neural network much easier.\n",
              " <P> I think it can be somehow like this:\n",
              "\n",
              "\n",
              "mean (img) using convolution.\n",
              "Subtract from the original image of p. 1\n",
              "Calculate the square of each element.\n",
              "By convolution we find the average, p. 3\n",
              "Calculates the square root of the elements of p. 4\n",
              "\n",
              " <P> Here is a function for computing the (unbiased) sample covariance matrix on a 3 channel image, named rgb_cov. Cholesky decomposition is straightforward with torch.cholesky:\n",
              "import torch\n",
              "def rgb_cov(im):\n",
              "    '''\n",
              "    Assuming im a torch.Tensor of shape (H,W,3):\n",
              "    '''\n",
              "    im_re = im.reshape(-1, 3)\n",
              "    im_re -= im_re.mean(0, keepdim=True)\n",
              "    return 1/(im_re.shape[0]-1) * im_re.T @ im_re\n",
              "\n",
              "#Test:\n",
              "im = torch.randn(50,50,3)\n",
              "cov = rgb_cov(im)\n",
              "L_cholesky = torch.cholesky(cov)\n",
              "\n",
              " <P> One way for the covariance matrix:\n",
              "h,w = 3,5\n",
              "\n",
              "def cov(X):\n",
              "    X = X/np.sqrt(X.size(0) - 1)\n",
              "    return X.T @ X\n",
              "\n",
              "x = torch.randn(h,w)\n",
              "print(x)\n",
              "\n",
              "c = cov(x)\n",
              "print(c)\n",
              "\n",
              "Out:\n",
              "tensor([[-1.5029e-01, -2.0626e-01, -7.7845e-01, -1.6811e+00,  5.0312e-01],\n",
              "        [ 4.4658e-01, -1.8570e+00, -6.2250e-01, -1.0989e+00,  1.6159e+00],\n",
              "        [ 6.8612e-01, -4.2650e-02, -9.5685e-01, -1.7947e-03,  2.1187e-01]])\n",
              "tensor([[ 0.3464, -0.4138, -0.4088, -0.1197,  0.3957],\n",
              "        [-0.4138,  1.7464,  0.6787,  1.1938, -1.5568],\n",
              "        [-0.4088,  0.6787,  0.9545,  0.9972, -0.8001],\n",
              "        [-0.1197,  1.1938,  0.9972,  2.0169, -1.3110],\n",
              "        [ 0.3957, -1.5568, -0.8001, -1.3110,  1.4546]])\n",
              "\n",
              "The mean() should be trivial just refer the documentation.\n",
              " <P> In my experience, punctual things in neural networks will have a bad performance because it cuts the influence of distant pixels. \n",
              "\n",
              "Thus, instead of using a gaussian kernel, it would be better to have an actual gaussian function applied to all pixels.\n",
              "\n",
              "So, taking a 2D gaussian distribution function:\n",
              "\n",
              "\n",
              "\n",
              "We can use it like this:\n",
              "\n",
              "\n",
              "\n",
              "This means some steps in a custom function:\n",
              "\n",
              "import keras.backend as K\n",
              "\n",
              "def coords_to_gaussian(x): #where x is shape (batch, 10, 2), and 2 = x, y\n",
              "\n",
              "    #pixel coordinates - must match the values of x and y\n",
              "    #here I suppose from 0 to image size, but you may want it normalized, maybe\n",
              "    x_pixels = K.reshape(K.arange(image_size), (1,1,image_size,1))\n",
              "    x_pixels = K.concatenate([x_pixels]*image_size, axis=-1) #shape(1,1,size,size)\n",
              "\n",
              "    y_pixels = K.permute_dimensions(x_pixels, (0,1,3,2))\n",
              "\n",
              "    pixels = K.stack([x_pixels, y_pixels], axis=-1) #shape(1,1,size,size,2)\n",
              "\n",
              "\n",
              "    #adjusting the AE locations to a compatible shape:\n",
              "    locations = K.reshape(x, (-1, 10, 1, 1, 2))\n",
              "\n",
              "\n",
              "    #calculating the upper part of the equation\n",
              "    result = K.square(pixels - locations) #shape (batch, 10, size, size, 2)\n",
              "    result = - K.sum(result, axis=-1) / (2*square_sigma) #shape (batch, 10, size, size)\n",
              "\n",
              "    #calculating the E:\n",
              "    result = K.exp(result) / (2 * pi * square_sigma)\n",
              "\n",
              "    #sum the 10 channels (principle of superposition)\n",
              "    result = K.sum(result, axis=1) #shape (batch, size, size)\n",
              "\n",
              "    #add a channel for future convolutions\n",
              "    result = K.expand_dims(result, axis=-1) #shape (batch, size, size, 1)\n",
              "\n",
              "    return result\n",
              "\n",
              "\n",
              "Use this in a Lambda layer:\n",
              "\n",
              "from keras.layers import Lambda\n",
              "Lambda(coords_to_gaussian)(coordinates_tensor_from_encoder)\n",
              "\n",
              "\n",
              "I'm not considering the covariances here, but you might find a way to put them in the formulas and adjust the code. \n",
              " <P> If you are only after the mean of the 9 elements centered at each pixel, then your best option would be to use a 2D convolution with a constant 3x3 filter:\n",
              "\n",
              "import torch.nn.functional as nnf\n",
              "\n",
              "def mean_filter(x_bchw):\n",
              "  \"\"\"\n",
              "  Calculating the mean of each 3x3 neighborhood.\n",
              "  input:\n",
              "    - x_bchw: input tensor of dimensions batch-channel-height-width\n",
              "  output:\n",
              "    - y_bchw: each element in y is the average of the 9 corresponding elements in x_bchw\n",
              "  \"\"\"\n",
              "  # define the filter\n",
              "  box = torch.ones((3, 3), dtype=x_bchw.dtype, device=x_bchw.device, requires_grad=False)  \n",
              "  box = box / box.sum()\n",
              "  box = box[None, None, ...].repeat(x_bchw.size(1), 1, 1, 1)\n",
              "  # use grouped convolution - so each channel is averaged separately.  \n",
              "  y_bchw = nnf.conv2d(x_bchw, box, padding=1, groups=x_bchw.size(1))\n",
              "  return y_bchw\n",
              "\n",
              "\n",
              "however, if you want to apply a more elaborate function over each neighborhood, you may want to use nn.Unfold. This operation converts each 3x3 (or whatever rectangular neighborhood you define) to a vector. Once you have all the vectors you may apply your function to them.\n",
              "See this answer for more details on unfold and fold.\n",
              " <P> You could permute the first and second axis to keep the channel dimension on dim=0, then flatten all other dimensions, and lastly, take the mean on that new axis:\n",
              "x.permute(1, 0, 2, 3).flatten(start_dim=1).mean(dim=1)\n",
              "\n",
              "Here are the shapes, step by step:\n",
              " x.permute(1, 0, 2, 3).shape\n",
              "(512, 1, 14, 14)\n",
              "\n",
              " x.permute(1, 0, 2, 3).flatten(start_dim=1).shape\n",
              "(512, 1, 196)\n",
              "\n",
              " x.permute(1, 0, 2, 3).flatten(start_dim=1).mean(dim=1).shape\n",
              "(512,)\n",
              "\n",
              " <P> Based on SciPy's implementation of the mahalanobis distance, you would do this in PyTorch. Assuming u and v are 1D and cov is the 2D covariance matrix.\n",
              "def mahalanobis(u, v, cov):\n",
              "    delta = u - v\n",
              "    m = torch.dot(delta, torch.matmul(torch.inverse(cov), delta))\n",
              "    return torch.sqrt(m)\n",
              "\n",
              "Note: scipy.spatial.distance.mahalanobis takes in the inverse of the covariance matrix.\n",
              " <P> If you are using softmax on top of the two output network you get an output that is mathematically equivalent to using a single output with sigmoid on top.\n",
              "Do the math and you'll see.\n",
              "\n",
              "In practice, from my experience, if you look at the raw \"logits\" of the two outputs net (before softmax) you'll see that one is exactly the negative of the other. This is a result of the gradients pulling exactly in the opposite direction each neuron.\n",
              "\n",
              "Therefore, since both approaches are equivalent, the single output configuration has less parameters and requires less computations, thus it is more advantageous to use a single output with a sigmoid ob top.\n",
              " <P> You theoreticaly can compute the 3d-gaussian convolution using three 2d-convolutions, but that would mean you have to reduce the size of the 2d-kernel, as you're effectively convolving in each direction twice.\n",
              "But computationally more efficient (and what you usually want) is a separation into 1d-kernels. I changed the second part of your function to implement this. (And I must say I really liked your permutation-based appraoch!) Since you're using a 3d volume you can't really use the conv2d or conv1d functions well, so the best thing is really just using conv3d even if you're just computing 1d-convolutions.\n",
              "Note that allclose uses a threshold of 1e-8 which we do not reach with this method, probably due to cancellation errors.\n",
              "def test_3d_gaussian_blur(blur_sigma=2):\n",
              "    # Make a test volume\n",
              "    vol = torch.randn([VOL_SIZE] * 3) # using something other than zeros\n",
              "    vol[VOL_SIZE // 2, VOL_SIZE // 2, VOL_SIZE // 2] = 1\n",
              "\n",
              "    # 3D convolution\n",
              "    vol_in = vol.reshape(1, 1, *vol.shape)\n",
              "    k = make_gaussian_kernel(blur_sigma)\n",
              "    k3d = torch.einsum('i,j,k-ijk', k, k, k)\n",
              "    k3d = k3d / k3d.sum()\n",
              "    vol_3d = F.conv3d(vol_in, k3d.reshape(1, 1, *k3d.shape), stride=1, padding=len(k) // 2)\n",
              "\n",
              "    # Separable 1D convolution\n",
              "    vol_in = vol[None, None, ...]\n",
              "    # k2d = torch.einsum('i,j-ij', k, k)\n",
              "    # k2d = k2d / k2d.sum() # not necessary if kernel already sums to zero, check:\n",
              "    # print(f'{k2d.sum()=}')\n",
              "    k1d = k[None, None, :, None, None]\n",
              "    for i in range(3):\n",
              "        vol_in = vol_in.permute(0, 1, 4, 2, 3)\n",
              "        vol_in = F.conv3d(vol_in, k1d, stride=1, padding=(len(k) // 2, 0, 0))\n",
              "    vol_3d_sep = vol_in\n",
              "    print((vol_3d- vol_3d_sep).abs().max()) # something ~1e-7\n",
              "    print(torch.allclose(vol_3d, vol_3d_sep)) # allclose checks if it is around 1e-8\n",
              "\n",
              "\n",
              "Addendum: If you really want to abuse conv2d to process the volumes you can try\n",
              "# separate 3d kernel into 1d + 2d\n",
              "vol_in = vol[None, None, ...]\n",
              "k2d = torch.einsum('i,j-ij', k, k)\n",
              "k2d = k2d.expand(VOL_SIZE, 1, len(k), len(k))\n",
              "# k2d = k2d / k2d.sum() # not necessary if kernel already sums to zero, check:\n",
              "# print(f'{k2d.sum()=}')\n",
              "k1d = k[None, None, :, None, None]\n",
              "vol_in = F.conv3d(vol_in, k1d, stride=1, padding=(len(k) // 2, 0, 0))\n",
              "vol_in = vol_in[0, ...]\n",
              "# abuse conv2d-groups argument for volume dimension, works only for 1 channel volumes\n",
              "vol_in = F.conv2d(vol_in, k2d, stride=1, padding=(len(k) // 2, len(k) // 2), groups=VOL_SIZE)\n",
              "vol_3d_sep = vol_in\n",
              "\n",
              "Or using exclusively conv2d you could do:\n",
              "# separate 3d kernel into 1d + 2d\n",
              "vol_in = vol[None,  ...]\n",
              "# 1d kernel\n",
              "k1d = k[None, None, :,  None]\n",
              "k1d = k1d.expand(VOL_SIZE, 1, len(k), 1)\n",
              "# 2d kernel\n",
              "k2d = torch.einsum('i,j-ij', k, k)\n",
              "k2d = k2d.expand(VOL_SIZE, 1, len(k), len(k))\n",
              "vol_in = vol_in.permute(0, 2, 1, 3)\n",
              "vol_in = F.conv2d(vol_in, k1d, stride=1, padding=(len(k) // 2, 0), groups=VOL_SIZE)\n",
              "vol_in = vol_in.permute(0, 2, 1, 3)\n",
              "vol_in = F.conv2d(vol_in, k2d, stride=1, padding=(len(k) // 2, len(k) // 2), groups=VOL_SIZE)\n",
              "vol_3d_sep = vol_in\n",
              "\n",
              "These should still be faster than three consecutive 2d convolutions.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row79\" class=\"row_heading level0 row79\" >79</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col0\" class=\"data row79 col0\" >comparing torch.nn.CrossEntropyLoss with label as int and prob tensor produces an error</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col1\" class=\"data row79 col1\" >You are using nn.CrossEntropyLoss as the criterion for your training. You correctly passed the labels as indices of the ground truth class: 0s and 1s. However, as the error message suggests, it needs to be a 1D tensor!\n",
              "Simply remove the reshape in ECGNet's __getitem__:\n",
              "def __getItem__(self, idx):\n",
              "    ecgVec = self.ecg[idx]\n",
              "  [0, 0, 1, 0] - [2]\n",
              "[1, 0. 0, 0], -</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col2\" class=\"data row79 col2\" >nn.CrossEntropyLoss() expects targets of type torch.LongTensor. Just do this:\n",
              "label = tensor([0.]).long()\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row79_col3\" class=\"data row79 col3\" ><P> When using NLLLoss the target tensor must contain the index representation of the labels and not one-hot. So for example:\n",
              "I guess this is what your target looks like:\n",
              "target = [0, 0, 1, 0]\n",
              "\n",
              "Just convert it to just the number which is the index of the 1:\n",
              "[0, 0, 1, 0] - [2]\n",
              "[1, 0, 0, 0] - [0]\n",
              "[0, 0, 0, 1] - [3]\n",
              "\n",
              "And then convert it to long tensor, ie:\n",
              "target = [2]\n",
              "target = torch.Tensor(target).type(torch.LongTensor)\n",
              "\n",
              "It might be confusing, that your output is a tensor with the length of classes and your target is an number but that how it is.\n",
              "You can check it out yourself here.\n",
              " <P> You are using nn.CrossEntropyLoss as the criterion for your training. You correctly passed the labels as indices of the ground truth class: 0s and 1s. However, as the error message suggests, it needs to be a 1D tensor!\n",
              "Simply remove the reshape in ECGNet's __getitem__:\n",
              "def __getitem__(self, idx):\n",
              "    ecgVec = self.ecg[idx]\n",
              "    labelID = self.target[idx]\n",
              "    return ecgVec,labelID\n",
              "\n",
              "\n",
              "Edit\n",
              "\n",
              "I want to increase the batch_size to 8. But now I get the error [...]\n",
              "\n",
              "You are doing a lot of broadcasting (flattening) which surely will affect the batch size. As a general rule of thumb never fiddle with axis=0. For instance, if you have an input shape of (8, 500), straight off you have a problem when doing x.view(1, 50, -1). Since the resulting tensor will be (1, 50, 80) (the desired shape would have been (8, 50, 10)). Instead, you could broadcast with x.view(x.size(0), 50, -1).\n",
              "Same with x.view(1, -1) later down forward. You are looking to flatten the tensor, but you should not flatten it along with the batches, they need to stay separated! It's safer to use torch.flatten, yet I prefer nn.Flatten which flattens from axis=1 to axis=-1 by default.\n",
              "\n",
              "My personal advice is to start with a simple setup (without train loops etc...) to verify the architecture and intermediate output shapes. Then, add the necessary logic to handle the training.\n",
              "class ECGNet(data.Dataset):\n",
              "    \"\"\"ImageNet Limited dataset.\"\"\"\n",
              "    \n",
              "    def __init__(self, ecgs, labls, transform=None):\n",
              "        self.ecg = ecgs\n",
              "        self.target = labls\n",
              "        self.transform = transform\n",
              "\n",
              "    def __getitem__(self, idx):\n",
              "        ecgVec = self.ecg[idx]\n",
              "        labelID = self.target[idx]\n",
              "        return ecgVec, labelID\n",
              "\n",
              "    def __len__(self):\n",
              "        return len(self.ecg)\n",
              "\n",
              "\n",
              "class Simple1DCNN(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Simple1DCNN, self).__init__()\n",
              "        self.layer1 = nn.Conv1d(in_channels=50, \n",
              "                                out_channels=20, \n",
              "                                kernel_size=5, \n",
              "                                stride=2)\n",
              "        self.act1 = nn.ReLU()\n",
              "        self.layer2 = nn.Conv1d(in_channels=20, \n",
              "                                out_channels=10, \n",
              "                                kernel_size=1)\n",
              "        \n",
              "        self.fc1 = nn.Linear(10*3, 2)\n",
              "        self.flatten = nn.Flatten()\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = x.view(x.size(0), 50, -1)\n",
              "        x = self.layer1(x)\n",
              "        x = self.act1(x)\n",
              "        x = self.layer2(x)\n",
              "        x = self.flatten(x)\n",
              "        x = self.fc1(x)\n",
              "        \n",
              "        return x\n",
              "\n",
              "batch_size = 8\n",
              "train_data = ECGNet(ecg_train, labels_train)\n",
              "train_dl = DataLoader(dataset=train_data,\n",
              "                      batch_size=batch_size, \n",
              "                      shuffle=True,\n",
              "                      num_workers=0)\n",
              "\n",
              "model = Simple1DCNN()\n",
              "criterion = nn.CrossEntropyLoss()\n",
              "\n",
              "Then\n",
              " x, y = next(iter(train_dl))\n",
              " y_hat = model(x)\n",
              "\n",
              " y_hat.shape\n",
              "torch.Size([8, 2])\n",
              "\n",
              "Also, make sure your loss works:\n",
              " criterion(y_hat, y)\n",
              "tensor(..., grad_fn=NllLossBackward)\n",
              "\n",
              " <P> As you mentioned, you are getting the error in the following line.\n",
              "\n",
              "D_loss1 = ((D(X_mb) + 1e-8).log()).mean() + ((1 - D(G_sample) + 1e-8).log()).mean()\n",
              "\n",
              "\n",
              "\n",
              "  I doubt the problematic part is: D(G_sample). Why?\n",
              "\n",
              "\n",
              "Because G_sample = G(X_mb) is of shape [batch_size, 1] which cannot be given as an input to the Discriminator, D because it takes tensor of shape [batch_size, dim] as input.\n",
              "\n",
              "That's why you are getting the error:\n",
              "\n",
              "RuntimeError: size mismatch, m1: [128 x 1], m2: [1392 x 2784]\n",
              "\n",
              "\n",
              "As you can see, you have an input of shape, [128 x 1] where batch_size = 128. But the Discriminator D is expecting an input of shape [batch_size x 1392]. Here, m2 is the shape of the weight matrix of fc1 layer in the Discriminator.\n",
              " <P> Your input shape to the loss function is (N, d, C) = (256, 4, 1181) and your target shape is (N, d) = (256, 4), however, according to the docs on NLLLoss the input should be (N, C, d) for a target of (N, d).\n",
              "\n",
              "Supposing x is your network output and y is the target then you can compute loss by transposing the incorrect dimensions of x as follows:\n",
              "\n",
              "loss = loss_function(x.transpose(1, 2), y)\n",
              "\n",
              "\n",
              "Alternatively since NLLLoss is just averaging all the responses anyway, you avoid creating copies of data by just reshaping x and y into (N*d, C) and (N*d) tensors and get the same result:\n",
              "\n",
              "loss = loss_function(x.reshape(N*d, C), y.reshape(N*d))\n",
              "\n",
              " <P> \n",
              "\n",
              "\n",
              " Julio_Marco_A_Silva:\n",
              "\n",
              "As you know, BCEWithLogitsLoss accepts a vector of integers (one for each element in the batch) and I have a one-hot vector of two elements as the output of my network.\n",
              "\n",
              "\n",
              "That’s not correct, as nn.BCEWithLogitsLoss expects raw logits as the model output (FloatTensors) and a target tensor with the same shape and type as the output containing values in [0, 1].\n",
              "I’m a bit confused about your use case at the moment.\n",
              "If you are dealing with a multi-label classification (each sample can belong to more than a single class), remove the softmax in your model and just pass the logits to the criterion without any max operations.\n",
              "On the other hand, if you are dealing with a multi-class classification (each sample belongs to one class only), still remove the softmax, use nn.CrossEntropyLoss as the criterion, and pass the targets as class indices (e.g. by using torch.argmax(target).\n",
              " very much! That solved it!\n",
              "The code I posted was a mess, due to several failed attempts at correction, but you gave me the answer in the “torch.argmax” function.\n",
              "Cheers! <P> nn.CrossEntropyLoss() expects target tensors of type Long, but what you're passing is of type Double.\n",
              "Try to change this line\n",
              "from:\n",
              "single_loss = loss_function(y_pred, train_op)\n",
              "to:\n",
              "single_loss = loss_function(y_pred, train_op.long())\n",
              " <P> Your discriminator loss is wrong. The labels for the real images should be 1 instead of 0.\n",
              "Updated code:\n",
              "def get_disc_loss(gen,disc,criterion,real,num_images,z_dim,device):\n",
              "    noise=get_noise(num_images,z_dim,device=device)\n",
              "    gen_out=gen(noise)\n",
              "    disc_fake_out=disc(gen_out.detach())\n",
              "    fake_loss=criterion(disc_fake_out,torch.zeros_like(disc_fake_out))\n",
              "    disc_real_out=disc(real)\n",
              "    real_loss=criterion(disc_real_out,torch.ones_like(disc_real_out))\n",
              "    disc_loss=(fake_loss+real_loss)/2\n",
              "    return(disc_loss)\n",
              "\n",
              "The output image looks pretty good to me:\n",
              "\n",
              " <P> \n",
              "one_hot_label = torch.cat((1 - current_label, current_label), dim=1)\n",
              "\n",
              "explanation:\n",
              "\n",
              "(1 - current_label, current_label) would be (1,0) when current label is 0; and it would be (0,1) when current label is 1.\n",
              "dim=1 means to operate on the 2nd dimension (which is channels)\n",
              " <P> I think you are using CrossEntropyLoss incorrectly. See the documentation here.\n",
              "\n",
              "In particular, if the input is of shape [NxCxd] then target should be of shape [Nxd], and value in target are integer between 0 and C-1 i.e you can just provide the class labels and it is not required to one-hot encode the target variable. Error message also states that same. \n",
              " <P> The error you got refers to the second (#2) argument of the loss: the target.\n",
              "NLLLoss expects (for each element) to have a float vector of probabilities, and a single long (i.e., integer) target per element.\n",
              "In your case, your \"target\" values are [4., 2., 2., 8., ...] which are of type float. you need to convert your target to long:\n",
              "\n",
              "target = target.to(dtype=torch.long)\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row80\" class=\"row_heading level0 row80\" >80</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col0\" class=\"data row80 col0\" >When will the extension need to be recompiled?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col1\" class=\"data row80 col1\" >when will the extension need to be recompiled? This is expected behavior and will not be part of the next patch. On the other hand, if the extension is not active, it may throw an error. This is fixed in 1.7.1, 1.8.2, and master in master.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col2\" class=\"data row80 col2\" >a new card is installed</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row80_col3\" class=\"data row80 col3\" ><P> On it <P> cc malfet <P> friday <P> Note <P> bumping priority based on user activity <P> certainly! <P> #ERROR! <P> Windows CI is now enabled <P> indeed :) <P> It's actually:\n",
              "\n",
              "\n",
              "  requires_grad\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row81\" class=\"row_heading level0 row81\" >81</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col0\" class=\"data row81 col0\" >How do they know mean and std, the input value of transforms.Normalize</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col1\" class=\"data row81 col1\" >The input is a tensor of shape (C, H, W) and mean and std can be sequences, that are internally converted to tensors. The normalization is done through broadcasting in this way:\n",
              "\n",
              "tensor.sub_(mean[:, None, None]).div_(std[:, 0, None])</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col2\" class=\"data row81 col2\" >For normalization input[channel] = (input[channel] - mean[channel]) / std[channel], the mean and standard deviation values are to be taken from the training dataset.\n",
              "\n",
              "Here, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] are the mean and std of Imagenet dataset.\n",
              "\n",
              "On Imagenet, we’ve done a pass on the dataset and calculated per-channel mean/std. check here(\n",
              "\n",
              "The pre-trained models available in torchvision for transfer learning were pretrained on Imagenet,\n",
              "so using its mean and std deviation would be fine for fine-tuning your model.\n",
              "\n",
              "If you're trying to train your model from scratch, it would be better to use the mean and std deviation of your training dataset (face dataset in this case).\n",
              "Other than that, in most of the cases, the mean and std of Imagenet suffice( your problem.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row81_col3\" class=\"data row81 col3\" ><P> Using the mean and std of Imagenet is a common practice. They are calculated based on millions of images. If you want to train from scratch on your own dataset, you can calculate the new mean and std. Otherwise, using the Imagenet pretrianed model with its own mean and std is recommended. \n",
              " <P> Shifting and scaling refers to the color space. What you do is you subtract the mean (shifting to the mean of the pixel values of the whole dataset to 0) and divide by the standard deviation (scaling the pixel values to [0, 1]. \n",
              "\n",
              "It has nothing to do with modifying the size of the image or the like.\n",
              "\n",
              "In numy you would do something like:\n",
              "\n",
              "mean, std = np.mean(image), np.std(image)\n",
              "image = image - mean\n",
              "image = image / std\n",
              "\n",
              "\n",
              "Note: You wouldn't want to normalize the data bz just 0.5 but by its mean and standard deviation.\n",
              " <P> Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. <P> That can just be the distribution of the colours in the original dataset of the code authors (such as COCO or PascalVOC). It would only be through chance that all colours are equally represented. However, if you used the same mean in your case, I doubt it would make much of a difference due to the similarity of the means and stds.\n",
              "For example, in my custom dataset taken from a GoPro camera, the means and standard deviations are as such:\n",
              "mean: [0.2841186 , 0.32399923, 0.27048702],\n",
              "std: [0.21937862, 0.26193094, 0.23754872]\n",
              "\n",
              "where the means are hardly equal. This does not mean, however, that they are treated differently in the ML model. All this transform does is make sure that each feature is standardised (through z-score standardisation).\n",
              "Think about it this way: if one colour is represented with a generally-higher intensity in your dataset (eg. if you have lots of vibrant \"blue\" pictures of the beach, the sky, lagoons, etc), than you will have to subtract a larger number from that channel to ensure that your data is standardised.\n",
              " <P> The normalize function called in there is this one https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py#L191\n",
              "\n",
              "The input is a tensor of shape (C, H, W) and mean and std can be sequences, that are internally converted to tensors. The normalization is done through broadcasting in this way:\n",
              "\n",
              "tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
              " <P> Normalize in pytorch context subtracts from each instance (MNIST image in your case) the mean (the first number) and divides by the standard deviation (second number). This takes place for each channel seperately, meaning in mnist you only need 2 numbers because images are grayscale, but on let's say cifar10 which has colored images you would use something along the lines of your last sform (3 numbers for mean and 3 for std).\n",
              "So basically each input image in MNIST gets transformed from [0,255] to [0,1] because you transform an image to Tensor (source: https://pytorch.org/docs/stable/torchvision/transforms.html --\n",
              "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8)\n",
              "After that you want your input image to have values in a range like [0,1] or [-1,1] to help your model converge to the right direction (Many reasons why scaling takes place, e.g. NNs prefer inputs around that range to avoid gradient saturation). Now as you probably noticed passing 0.5 and 0.5 in Normalize would yield vales in range:\n",
              "Min of input image = 0 - 0-0.5 = -0.5 - gets divided by 0.5 std - -1\n",
              "Max of input image = 255 - toTensor - 1 - (1 - 0.5) / 0.5 - 1\n",
              "so it transforms your data in a range [-1, 1]\n",
              " <P> What normalization tries to do is mantain the overall information on your dataset, even when there exists differences in the values, in the case of images it tries to set apart some issues like brightness and contrast that in certain case does not contribute to the general information that the image has. There are several ways to do this, each one with pros and cons, depending on the image set you have and the processing effort you want to do on them, just to name a few:\n",
              "\n",
              "\n",
              "Linear Histogram stetching: where you do a linear map on the current\n",
              "range of values in your image and stetch it to match the 0 and 255\n",
              "values in RGB \n",
              "Nonlinear Histogram stetching: Where you use a\n",
              "nonlinear function to map the input pixels to a new image. Commonly\n",
              "used functions are logarithms and exponentials. My favorite function\n",
              "is the cumulative probability function of the original histogram, it\n",
              "works pretty well.\n",
              "Adaptive Histogram equalization: Where you do a linear\n",
              "histogram stretching in certain places of your image to avoid doing\n",
              "an identity mapping where you have the max range of values in your original\n",
              "image.\n",
              "\n",
              " <P> The problem is that you seem to misunderstand what transforms.Normalize does. To quote from the PyTorch documentation:\n",
              "\n",
              "\n",
              "  Normalize a tensor image with mean and standard deviation. Given mean:\n",
              "  (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will\n",
              "  normalize each channel of the input torch.*Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
              "\n",
              "\n",
              "The calculation for a value of, say 100, and the std and mean you provided would then be: 100 - 0.5 / 0.5 = 199.\n",
              "Of course, you could increase std and mean, but this does not guarantee you the exact result that you might expect.\n",
              "As suggested in the comments, the best way would probably be to invert the operations that you performed in order to get the tensor to [0 255] in the first place.\n",
              "\n",
              "Edit:\n",
              "As it turns out, according to this forum post, it seems that the transformations from PIL images to tensors automatically turn your value range to [0 1] (and to [0 255] if you transform to a PIL image, respectively), as is written in the fine-print of transforms.ToTensor. For the return transformation it is not explicitly stated, but can be enforced via the mode.\n",
              " <P> In that example, they are using the mean and stddev of ImageNet, but if you look at their MNIST examples, the mean and stddev are 1-dimensional (since the inputs are greyscale-- no RGB channels).\n",
              "Whether or not to use ImageNet's mean and stddev depends on your data. Assuming your data are ordinary photos of \"natural scenes\"† (people, buildings, animals, varied lighting/angles/backgrounds, etc.), and assuming your dataset is biased in the same way ImageNet is (in terms of class balance), then it's ok to normalize with ImageNet's scene statistics. If the photos are \"special\" somehow (color filtered, contrast adjusted, uncommon lighting, etc.) or an \"un-natural subject\" (medical images, satellite imagery, hand drawings, etc.) then I would recommend correctly normalizing your dataset before model training!*\n",
              "Here's some sample code to get you started:\n",
              "import os\n",
              "import torch\n",
              "from torchvision import datasets, transforms\n",
              "from torch.utils.data.dataset import Dataset\n",
              "from tqdm.notebook import tqdm\n",
              "from time import time\n",
              "\n",
              "N_CHANNELS = 1\n",
              "\n",
              "dataset = datasets.MNIST(\"data\", download=True,\n",
              "                 train=True, transform=transforms.ToTensor())\n",
              "full_loader = torch.utils.data.DataLoader(dataset, shuffle=False, num_workers=os.cpu_count())\n",
              "\n",
              "before = time()\n",
              "mean = torch.zeros(1)\n",
              "std = torch.zeros(1)\n",
              "print('== Computing mean and std..')\n",
              "for inputs, _labels in tqdm(full_loader):\n",
              "    for i in range(N_CHANNELS):\n",
              "        mean[i] += inputs[:,i,:,:].mean()\n",
              "        std[i] += inputs[:,i,:,:].std()\n",
              "mean.div_(len(dataset))\n",
              "std.div_(len(dataset))\n",
              "print(mean, std)\n",
              "\n",
              "print(\"time elapsed: \", time()-before)\n",
              "\n",
              "\n",
              "† In computer vision, \"Natural scene\" has a specific meaning which isn't related to nature vs man-made, see https://en.wikipedia.org/wiki/Natural_scene_perception\n",
              "* Otherwise you run into optimization problems due to elongations in the loss function-- see my answer here.\n",
              " <P> What you are doing is correct but mean and std are not calculated based on your data, rather you've taken those values from ImageNet dataset.\n",
              "There will be some images which are out of [-1, 1] range as they weren't part of the mean and std calculations in the first place and it's expected during test. Also there are images outside this range as it changes mean and standard deviation to 0 and 1 respectively, hence there are samples which are outside this range.\n",
              "If you wish to fine-tune your neural network you should calculate per-channel mean and std and input those values instead (though it might not make a large difference, depending on dataset and how many images you have).\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row82\" class=\"row_heading level0 row82\" >82</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col0\" class=\"data row82 col0\" >if i use scripting to convert my model how can i call other methods in c</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col1\" class=\"data row82 col1\" >There is this thread, that describes the options but is not receiving a lot of attention.\n",
              "\n",
              "In summary, as of May 2020, there are only two options:\n",
              "\n",
              "1) ONNX.js but its development is currently stale.\n",
              "2) Converting the model to Tensorflow.\n",
              "Technically there is a third one, that includes no server. And that is to run the model in a Mobile Application.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col2\" class=\"data row82 col2\" > use this at: :Tensor output = module->forward(inputs).toTensor(); with torch: :Tensor output = module -> run_method(weighted_kernel_sum, input).toTensor();</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row82_col3\" class=\"data row82 col3\" ><P> @Gilad, You can do this with Deep Java Library (djl.ai). Check out: https://github.com/awslabs/djl/tree/master/pytorch/pytorch-engine\n",
              " <P> For those who need to do something similar, a co-author of the library suggested me to use SMOTENC, which can handle also categorical variables (like strings).\n",
              " <P> You could add a virtual environment with the following instructions, then you should add ROS distpackages (roslib) on it with this instruction.\n",
              "\n",
              "\n",
              "File > Settings (or Ctrl+Alt+s as shortcut)> Project:  > Project interpreter.\n",
              "In the project interpreter dropdown list, you can specify ROS Python interpreter by selecting the appropriate from the list.\n",
              "\n",
              "\n",
              "ROS distpackages path that you need: /opt/ros/kinetic/lib/python2.7/distpackages\n",
              " <P> \n",
              "You can use TorchScript intermediate representation of a PyTorch model, through tracing and scripting, that can be run in C++ environment. For this, you'll probably have to modify the model itself in order for it to be traced or scripted.\n",
              "You can use ONNX (Open Neural Network Exchange), through which you can export your model and load it in another C++ framework such as Caffe. It comes with its own implications though.\n",
              "The easiest is to try Embedding Python, through which you can run your python (pytorch) model in C++ environment. Note that the model will still run in python, but only through C++, so there won't be any speed gains that you might be expecting in C++.\n",
              "\n",
              "\n",
              "Also, with the release of torchvision 0.5, all models in torchvision have native support for TorchScript and ONNX.\n",
              " <P> Hello. Just to clarify things, there is no plan to add Java support (not Android only) to pytorch in a near future correct? I am considering switching from TF to pytorch but the lack of Java support is currently a limitation in my case. <P> I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable.  <P> we dont plan to provide an interface other than the one via TorchScript.\n",
              " \n",
              " \n",
              " \n",
              " > I really puzzled why pytorch DO NOT want to provide unified interface for both .pth and .pt, which is not difficult to implement\n",
              " \n",
              " \n",
              " \n",
              " Without re-implementing Python, it's simply not possible. So, no you are wrong, it **is** difficult to implement. <P> one step towards this that I'm working on is removing the middle layer (build_pytorch_libs.sh) <P> There is this thread, that describes the options but is not receiving a lot of attention. \n",
              "\n",
              "In summary, as of May 2020, there are only two options: \n",
              "\n",
              "1) ONNX.js but its development is currently stale.\n",
              "\n",
              "2) Converting the model to Tensorflow.\n",
              "\n",
              "Technically there is a third one, that includes no server. And that is to run the model in a Mobile Application.\n",
              " <P> After suffering a lot!...\n",
              "I learned to better use the Pytorch Discuss forum for Pytorch and Libtorch information. Using the tag C++ for example.\n",
              "Unfortunantly, there is the oficial source of information (altough quite messy). This is the reason why I am sharing my answer here in SO.\n",
              "namespace th = torch;\n",
              "...\n",
              "// th.set_grad_enabled(False)\n",
              "th::NoGradGuard guard; // or same as with torch.no_grad(): block \n",
              "...\n",
              "auto dtype_option = th::TensorOptions().dtype(th::kFloat32);\n",
              "//X = th.zeros((nobs, 3+p), device=dev, dtype=th.float32)\n",
              "//y = th.tensor(indata, device=dev, dtype=th.float32)\n",
              "//diffilter = th.tensor([-1., 1.], device=dev, dtype=th.float32).view(1, 1, 2)\n",
              "//dy = th.conv1d(y.view(1, 1, -1), diffilter).view(-1)\n",
              "//z = dy[p:].clone()\n",
              "auto X = th::zeros({nobs, 3+p}, dtype_option);\n",
              "auto y = th::from_blob(signal, {n}, dtype_option);\n",
              "auto diffilter = th::tensor({-1, 1}, dtype_option).view({ 1, 1, 2 }); // first difference filter\n",
              "auto dy = th::conv1d(y.view({ 1, 1, -1 }), diffilter).view({ -1 });\n",
              "auto z = dy.slice(0, p).clone();\n",
              "...\n",
              "// X[:, 0] = 1 # drift\n",
              "// X[:, 1] = th.arange(p+1, n) \n",
              "// X[:, 2] = y[p:-1]\n",
              "// create acessors to fill in the matrix\n",
              "auto ay = y.accessorfloat, 1(); // 1 dimension\n",
              "auto aX = X.accessorfloat, 2(); // 2 dimension\n",
              "for (auto i = 0; i  nobs; i++) {\n",
              "    aX[i][0] = 1; \n",
              "    aX[i][1] = p + 1 + i; \n",
              "    aX[i][2] = ay[p+i];  \n",
              "}\n",
              "...\n",
              "// Xm = th.zeros((nobsadf, 3+p), device=th.device('cpu'), dtype=th.float32)\n",
              "auto Xm = th::zeros({ nobsadf, 3 + p }, dtype_option.device(th::Device(th::kCPU)));\n",
              "// Xbt = th.zeros(batch_size, adfs_count, nobsadf, (3+p), device=th.device('cpu'), dtype=th.float32)\n",
              "auto Xbt = th::zeros({ batch_size, adfs_count, nobsadf, (3 + p) }, dtype_option.device(th::Device(th::kCPU)));\n",
              "...\n",
              "// this acessor will be used in the inner for loop k\n",
              "auto anobt = nobt.accessorfloat, 2();\n",
              "auto tline = 0; // start line for master main X OLS matrix/ z vector\n",
              "for (int i = 0; i  nbatchs; i++){\n",
              "    for (int j = 0; j  batch_size; j++){ // assembly batch_size matrixes\n",
              "        // Xm[:] = X[t:t+nobsadf]\n",
              "        Xm.copy_(X.narrow(0, tline, nobsadf)); \n",
              "        ... \n",
              "        // Xbt[j, :, :, :] = Xm.repeat(adfs_count, 1).view(adfs_count, nobsadf, (3+p))   \n",
              "        auto Xbts = Xbt.select(0, j);\n",
              "        Xbts.copy_(Xm.repeat({ adfs_count, 1 }).view({ adfs_count, nobsadf, (3 + p) }));\n",
              "        for (int k = 0; k  adfs_count; k++) { \n",
              "            // Xbt[j, k, :k, :] = 0\n",
              "            // nobt[j][k] = float(nobsadf - k - (p + 3));\n",
              "            Xbts.select(0, k).narrow(0, 0, k).fill_(0);\n",
              "            anobt[j][k] = float(nobsadf - k - (p + 3));\n",
              "        }\n",
              "        tline++;\n",
              "    }\n",
              "}\n",
              "      \n",
              "\n",
              "Probably there is a better or faster way of coding but the code above fully works. Fell free to make suggestions to improve my code.\n",
              "C++ Signatures of commom functions above\n",
              "Tensor Tensor::slice(int64_t dim, int64_t start, int64_t end, int64_t step)\n",
              "\n",
              "Tensor Tensor::narrow(int64_t dim, int64_t start, int64_t length) \n",
              "\n",
              "Tensor Tensor::select(int64_t dim, int64_t index)\n",
              "\n",
              "Tensor  Tensor::copy_(const Tensor  src, bool non_blocking=false)\n",
              "\n",
              "Further notes:\n",
              "Almost all C++ function have Pytorch Python equivalent. So here is my golden tip:\n",
              "\n",
              "Translate your python script using C++ equivalent functions like copy_, narrow, slice testing it (to make sure it works) than just go to C++ replicating everything.\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row83\" class=\"row_heading level0 row83\" >83</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col0\" class=\"data row83 col0\" >Cublas run time error with RTX 2080Ti with Cuda 9.0</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col1\" class=\"data row83 col1\" >I ran your code for 7h on an RTX2080Ti using the PyTorch `1.7.1` conda binary with CUDA10.2 and unfortunately cannot reproduce this issue.\n",
              " \n",
              " Have you had a chance to run it on bare metal without mps? Remove all the `MAGMA_V2` ifdefs</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col2\" class=\"data row83 col2\" >RTX 2080Ti needs CUDA10 version of PyTorch to be installed, not CUDA9.\n",
              " \n",
              " That's likely the reason for the error.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row83_col3\" class=\"data row83 col3\" ><P> I just reported another issue with this API in #42695, please take a look. Apparently, it produces so many kinds of error message with abort, and even segfault. Thank you! <P> Installing netcdf4 via pip solved the problem.\n",
              " <P> Could you update to 1.9.0 and rerun the code, please?We‚Äôve seen some issues in the pip wheels in 1.8.0 and 1.8.1 in particular for sm_61 by leaking cublas symbols, which might be also visible on the K80. <P> gh-40499 indeed fixed this, so closing. thanks @diego-plan9, all <P> I ran your code for 7h on an RTX2080Ti using the PyTorch `1.7.1` conda binary with CUDA10.2 and unfortunately cannot reproduce this issue.\n",
              " \n",
              " Have you had a chance to run it on bare metal without mps? <P> Remove all the `MAGMA_V2` ifdefs <P> Made default to 0.\n",
              "fixed via 4eb12a2 <P> Can you please check if this fix to SobolEngineOps.cpp fixes it for you? \n",
              " \n",
              " https://github.com/pytorch/pytorch/pull/36711 <P> Thanks for the fix! '-g0' flag solved the problem. It reduced overall size by ~460 MB which is perfect.\n",
              "Had no clue about NDK generating debug symbols for release builds behind the scenes, now it makes sense why the resulting binary was huge.\n",
              "\n",
              "We do use both \"--whole-archive\" and \"--gc-sections\" at the same time. And we will consider limiting ops we use for our modules to trim down size even more.\n",
              "Thanks for the quick and thorough response! <P> Thanks @ezyang. I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0. I am going to close the issue.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row84\" class=\"row_heading level0 row84\" >84</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col0\" class=\"data row84 col0\" >If dim is squeezed, outputs tensors have what?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col1\" class=\"data row84 col1\" >input tensors have an even number of elements. NaN values are NaNs, while SGTM outputs tensors with non zero elements. In this example, the output tensor contains [0,2,6]. i.e. the values located at positions 0, 1 and 3.\n",
              "So it's just picking out the contents using the index tensor as a pointer to the locations of the contents to be extracted from the input tensor.\n",
              "\n",
              "The dim is the dimension along which you want to index. So for 2D that'd give you the option of indexing by rows or columns, and you can</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col2\" class=\"data row84 col2\" >1 fewer dimension than input</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row84_col3\" class=\"data row84 col3\" ><P> SGTM <P> nnz mean number non zero elements. In this example nnz = 3.\n",
              " <P> Yes, it goes through the given dim (dimension) of the tensor, and collects into a new tensor the values specified by the index provided. So if I had a 1D tensor (is that allowed?) as \n",
              "\n",
              "MyValues = torch.tensor([0,2,4,6,8])\n",
              "\n",
              "\n",
              "and did \n",
              "\n",
              "torch.gather(MyValues, 0, torch.tensor([0,1,3])) \n",
              "\n",
              "\n",
              "I'd expect to return a 1D tensor containing [0,2,6]. i.e. the values located at positions 0, 1 and 3.\n",
              "\n",
              "So it's just picking out the contents using the index tensor as a pointer to the locations of the contents to be extracted from the input tensor.  \n",
              "\n",
              "The dim is the dimension along which you want to index. So for 2D that'd give you the option of indexing by rows or columns, and you can extrapolate that out into as many dimensions as you like. \n",
              " <P> Alias for torch.linalg.matrix_power() <P> It could be said that Linear layers operate on 2D Tensor of N x Channels, and that ConvXd operates on consumes a produces a set of feature maps. <P> friday <P> logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              " <P> Double-backward <P> Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that “ignores” NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel’s correction will be used.   If unbiased is True, Bessel’s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel’s correction will be used. <P> Note</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row85\" class=\"row_heading level0 row85\" >85</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col0\" class=\"data row85 col0\" >!\n",
              "I’m an experienced programmer, familiar to neural nets (theoretically) but new to pytorch. I’m trying out pytorch in C++ and am currently writing the learning stage of my simple network (1 hidden layer).\n",
              "I have an optimizer and have calculated a loss tensor, all according to the tutorial found here with link \"https://pytorch.org/tutorials/advanced/cpp_frontend.html\". The tutorial then calls d_loss_fake.backward(). I did some digging and this apparently updates the gradients of the network. I am confused however about how it achieves this, as I could not find any relations between the loss tensor and the network. Concretely: how does .backward() know which weights to update if there is no connection between the loss tensor and the network?\n",
              "Sorry for the beginner question. I hope this isn’t a duplicate, I’ve been googling for a couple of hours now. Thanks in advance!</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col1\" class=\"data row85 col1\" >You need to reset the gradient to 0 after each backprop. By default, pytorch accumulates gradients when you call loss.backward().\n",
              "Replacing the last 2 instructions of your loop with the following lines should fix the issue :\n",
              "with torch.no_grad():\n",
              "    model.w.data = model.data - alpha * model.W.grad\n",
              " model.b.data -> model.B.data\n",
              "model.data.data -= model.zero_grad()\n",
              "if condition:\n",
              "\n",
              "loss.Backward() with loss.Tuple else:\n",
              "else:</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col2\" class=\"data row85 col2\" >Maybe a starting point is \n",
              "Behind the scenes PyTorch tracks all operations on tensors with requires_grad == true and builds a computation graph during the forward pass. It knowns how the loss value was calculated and can automatically back-propagate the gradient step by step from the loss (or any scalar model output) to the model parameters.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row85_col3\" class=\"data row85 col3\" ><P> Instead of having one loss function and one optimizer for both your discriminator and generator, you could have a loss and optimizer for each of them. This means your training loop would look something like this:\n",
              "for i in range(NUM_ITERATIONS):\n",
              "    loss_gen = ...\n",
              "    loss_gen.backward()\n",
              "    optim_gen.step()\n",
              "\n",
              "    if i % 100 == 0:\n",
              "        loss_disc = ...\n",
              "        loss_disc.backward()\n",
              "        optim_disc.step()\n",
              "\n",
              "\n",
              "Hope this helps!\n",
              "Oh…i forget the way of multiple optimizer and loss objects. Now it is solved. thanks! <P> Yes, your approach should work just fine.\n",
              "The advantage of using a class is that it can hold internal attributes you would have to pass otherwise to your functional approach.\n",
              "E.g. using nn.CrossEntropyLoss, you could define the weight or reduction argument while creating an instance of this class. If you are using the functional method (F.cross_entropy), you would need to pass these arguments in each call.\n",
              "While this might be cumbersome in some cases, it might make the code cleaner e.g. if you need to change the weight argument often.\n",
              "Both APIs have some advantages and disadvantages and you can chose whatever feels right and makes the code clean.  <P> You could simulate a larger batch size by accumulating the gradients from a few forward passes and call backward() on these.\n",
              "Also in the DCGAN with link \"https://github.com/pytorch/examples/blob/master/dcgan/main.py#L240\" example the gradients from the “real” and “fake” loss are accumulated and the optimizer is called just after both backward passes were called.\n",
              "It gives you more flexibility, if you would like to experiment with some crazy stuff!  <P> You need to reset the gradient to 0 after each backprop. By default, pytorch accumulates gradients when you call loss.backward().\n",
              "Replacing the last 2 instructions of your loop with the following lines should fix the issue :\n",
              "with torch.no_grad():\n",
              "    model.w.data = model.w.data - alpha * model.w.grad\n",
              "    model.b.data = model.b.data - alpha * model.b.grad\n",
              "    model.w.grad.zero_()\n",
              "    model.b.grad.zero_()\n",
              "\n",
              " <P> ,\n",
              "You could just call .backward() without optimizer.step() if the step is unexpected condition.\n",
              "optimizer.zero_grad()\n",
              "if condition:\n",
              "    loss.backward()\n",
              "else:\n",
              "    loss.backward()\n",
              "    optimizer.step()\n",
              "\n",
              ",Thanks very much for your reply! But  this way may accumulate the backward gradient. I have tried another similar way. When loss is none, I change to use a normal way to calculate the loss and everything is fine now. <P> You have to reset gradients with optimizer.zero_grad() before calling again loss.backward()\n",
              "It doesn’t make sense. I need to accumulate gradient.\n",
              "For example:\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "How to implement accumulated gradient in pytorch (i.e. iter_size in caffe prototxt) with link \"https://discuss.pytorch.org/t/how-to-implement-accumulated-gradient-in-pytorch-i-e-iter-size-in-caffe-prototxt/2522\"\n",
              "\n",
              "\n",
              "    how to can i accumulate gradient during gradient descent in pytorch (i.e. iter_size in caffe prototxt). \n",
              "Currently, my code is: \n",
              "     for iter, (images, labels, indices) in enumerate(train_loader, 0):\n",
              " \n",
              "            optimizer.zero_grad() \n",
              "            outputs = net(Variable(images.cuda()))\n",
              "            loss    = criterion(outputs, Variable(labels.cuda()))\n",
              "            loss.backward()\n",
              "            optimizer.step()\n",
              "\n",
              "Do i do this? \n",
              "     for iter in range(N):\n",
              " \n",
              "            optimizer.zero_grad() \n",
              "        …\n",
              "  \n",
              "\n",
              "\n",
              "I’m sorry, I said a stupid thing. Are you reinitializing the hidden states between samples yet? If you don’t want to reinizitialize them, have you tried to detach the hidden states between batches? Usually you don’t need the entire history of the hidden states between one example and one other.\n",
              "When a new tensor is passed to model.encode() function  (or to model.forward()) then self.hidden_state become reinitialized.\n",
              "\n",
              "reinitializing the hidden states between samples\n",
              "\n",
              "Earlier, when I passed one training sample to the model at a time, the hidden_state was reinitialized between samples each time.\n",
              "But now I pass whole batch to the model and there’s no need to reinitialize hidden_state between samples.\n",
              "The problem I have is I need to calculate loss for the whole batch. When I’m trying to pass my decoder outputs and my target indexes (torch.Size([3, 2, 35620]) and torch.Size([3, 2])) to NLLLoss, I get an error about inconsistent tensor sizes. It seems NLLLoss asks me to use one-hot encoding for computing loss. But one-hot encoding will greatly increase memory usage, that makes impossible to use big batches in Google Colab.\n",
              "NLLLoss doesn’t ask for one-hot encoding. Input to the NLLLoss is (N,C) where C is the number of classes and the target is (N). Are you inputting elements one-by-one and accumulating the loss for each element? Can you recode so that you can input larger batches instead of batch size of 1? Accumulating loss for each element will surely result in a huge memory requirement.\n",
              "\n",
              "Can you recode so that you can input larger batches instead of batch size of 1?\n",
              "\n",
              "This is my question: how to recode it to input the entire batch instead of calculating loss one by one.\n",
              "If I try to input the batch to the criterion, I get:\n",
              "\n",
              "ValueError: Expected target size (7, 35620), got torch.Size([7, 50])\n",
              "\n",
              "I’m inputting torch.Size([7, 50, 35620]) as decoder outputs and torch.Size([7, 50]) as target indexes.\n",
              "Where 50 is a batch_size, 35620 – vocabulary size (number of classes), and 7 is a number of predicted words.\n",
              "It works when I pass it one by one (torch.Size([7, 35620]) with torch.Size([7])), and I don’t know how to adapt it for the batch input.\n",
              "EDIT:\n",
              "OMG, I tried “batch_first” way earlier and this didn’t work. But I didn’t try to put batch_size to the last position like torch.Size([7, 35620, 50]) and torch.Size([7, 50]). And this works now. Is this correct way to compute loss?\n",
              "opt.zero_grad()\n",
              "for i in range(iterations):\n",
              "  ...\n",
              "  loss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size))\n",
              "  loss.backward()\n",
              "## after few iterations\n",
              "opt.step()\n",
              "\n",
              "\n",
              "\n",
              "\n",
              " Defake:\n",
              "\n",
              "loss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size))\n",
              "\n",
              "\n",
              " you need to do criterion_test(dec_outs.view(-1, vocab_size),targets.view(-1))\n",
              "In your case, ( C )-> vocab_size and (N)-> (batch_size*seq_length). I am assuming all the batches have the same sequence length. If not, you’ll have to use pack_padded_sequence and also mask the loss for the pad token.\n",
              "Yes, you are right! The training speed of your calculating method is comparable with that I tried, but loss decreasing is much faster.\n",
              "Speed now is just mind-blowing. 30 times faster that I had earlier. !! <P> Yes, it should (if total.backward()). Try to print and see if they are different?Since the backward is on total i.e loss1+loss2, the computation graph would include both 1,2 inputs.You could also refer the GAN tutorial where something similar is done <P> \n",
              "When you do weight = weight - weight.grad*lr, weight now points to a brand new Tensor and so the gradient informations from the original weight Tensor are gone. You can check that after this line, weight.grad is None.\n",
              "The other problem you’re going to encounter is that weight = weight - XXX, this will be tracked by the autgrad which you most likely don’t want.\n",
              "To fix these, you can\n",
              "\n",
              "Change weight inplace, to avoid the first problem above\n",
              "Disable autograd while you update your weights to avoid the second one.\n",
              "\n",
              "Here is the new code update:\n",
              "for i in range(epochs):\n",
              "    predict = torch.mm(feature, weight) + bias.item()\n",
              "    loss = torch.sum(predict - label, dim=0)\n",
              "    loss.backward()\n",
              "    # Disable the autograd\n",
              "    with torch.no_grad():\n",
              "        # Inplace changes\n",
              "        weight.sub_(weight.grad*lr)\n",
              "        bias.sub_(bias.grad*lr) # A .grad is missing in your code here  ;)\n",
              "        # Do the reset in no grad mode as well in case you do second order\n",
              "        # derivatives later (meaning that weight.grad will requires_grad)\n",
              "        weight.grad.zero_()\n",
              "        bias.grad.zero_()\n",
              "\n",
              "! But I still get the same error following your code:\n",
              "Traceback (most recent call last):\n",
              "  File \"/Users/audrey/PycharmProjects/PyTorchLearning/main.py\", line 31, in \n",
              "    bias.grad.data.zero_()\n",
              "AttributeError: 'NoneType' object has no attribute 'data'\n",
              "\n",
              "Then I changed bias.item() into bias\n",
              "predict = torch.mm(feature, weight) + bias\n",
              "\n",
              "No error appeared anymore! I wonder why bias.item() doesn’t work.\n",
              "Anyway, my code can run successfully, thanks!\n",
              " I find the reason why bias.item() doesn’t work. Using scaler makes tensor bias not be tracked.\n",
              "Exactely: .item() gives you a python number that means that you don’t have Tensors anymore (and so no autograd anymore).\n",
              "Also given you error message, you seem to be using .data. You should not use it and using with torch.no_grad() instead as in my example.\n",
              "I tried to use with torch.no_grad() to resolve this error\n",
              "def weights_init_normal(m):\n",
              "    classname = m.__class__.__name__\n",
              "    lr = 0.01\n",
              "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
              "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
              "        with torch.no_grad():\n",
              "            m.bias.sub_(m.bias.grad*lr)\n",
              "            m.bias.grad.zero_()\n",
              "\n",
              "got alot of error.\n",
              "AttributeError: 'NoneType' object has no attribute 'sub_'\n",
              "I found out my bias is NoneType.\n",
              "Tried several techniques , nothing worked .\n",
              "I wanted to initialize the weights of bias to zero.\n",
              "Please help!!!\n",
              "\n",
              "By default, the .grad attribute is None. Which means “full of zeros”.\n",
              "Oh ,That means, It is ok to not to touch None type bias. However, my model was not converging, may be my error. Thanks for knowledge  <P> Your code looks correct, but you might want to divide the accumulated loss by the number of accumulation steps. Also, here is a nice overview of different approaches in case you want to trade compute for memory etc. <P> That would depend what your optimizer is.\n",
              "It might be clearer to have a custom optimizer that does the scaling by A * f?\n",
              "out = g + f\n",
              "out.backward() # Compute \\nable [ g + f]\n",
              "optimizer.step(A * f)\n",
              "\n",
              "But both should do the same step in the end (if your optimizer above is like SGD and has a lr of 1).</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row86\" class=\"row_heading level0 row86\" >86</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col0\" class=\"data row86 col0\" >Importing pytorch in Spyder crashes kernel after installing matplotlib</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col1\" class=\"data row86 col1\" >It's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\n",
              "Try:\n",
              "\n",
              "pip uninstall torch\n",
              "\n",
              "pip install torch\n",
              "Pip install the torch\n",
              "\n",
              "For another version/type of the software (OS, package, Language) installed, the command must be generated from the below-mentioned link.\n",
              "\n",
              "Collecting pyTorch\n",
              "Using cached  Using cached \n",
              "Found the Python version in your IDE(If you are using PyCharm) from the terminal using the command: python. If it returns 32bit this could happen, instead install Python 64-</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col2\" class=\"data row86 col2\" >I have ipython 7.0.1 and matplotlib 2.0.2 and the same problem, it seems like ipython crashes after the following two commands: %matplotlib auto followed by import torch.\n",
              "\n",
              "This happens both in spyder as in jupyter notebook when the two commands are in seperate blocks.\n",
              "\n",
              "What worked for me was: First making sure that spyders backend graphics is set to inline: Tools -> Preferences -> IPython console -> Graphics backed to Inline. Then import torch followed by switching from inline to external plotting with %matplotlib auto.\n",
              "\n",
              "Note that this does not happen any more with ipython 7.2.0 and matplotlib 3.0.2</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row86_col3\" class=\"data row86 col3\" ><P> I reinstalled anaconda and then created a virtual environment for pytorch.Now everything works fine\n",
              " <P> The message is pretty straight forward, Pytorch is not installed. You can download it from Pytorch website here.\n",
              " <P> I am using Torch 1.4.0 on Windows and I had the same issue. Turns out I had installed the 2.x version of Tensorboard. I reverted back to 1.15.0 and it solved the issue.\n",
              " <P> Anyone who is looking for the solution refer below:\n",
              "It seems command to install torch not is working as expected, instead, you can try to install PyTorch using below command.\n",
              "It's working and solved my above-mentioned issue.\n",
              "Run below command(for below-specified OS, package-manager, Language):\n",
              "# for OS: Windows, package-manager: pip, Language: python3.6 (below command is valid for only mentioned python 3.6)\n",
              "\n",
              "pip3 install https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-win_amd64.whl\n",
              "pip3 install https://download.pytorch.org/whl/cu90/torchvision-0.3.0-cp36-cp36m-win_amd64.whl\n",
              "\n",
              "For another version/type of the software (OS, package, Language) installed, the command must be generated from the below-mentioned link.\n",
              "https://pytorch.org/get-started/locally/\n",
              "Also, look for the Python version in your IDE(If you are using PyCharm) from the terminal using the command: python. If it returns 32bit this could happen, instead install Python 64-bit.\n",
              " <P> From your error:\n",
              "\n",
              "Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
              "\n",
              "which tells you what you need to know, instead of\n",
              "pip install pytorch\n",
              "\n",
              "it should be\n",
              "pip install torch\n",
              "\n",
              "\n",
              "I downloaded the matching wheel from here, but am couldn't figure out what to do with it\n",
              "\n",
              "Installing .whl files is as easy as\n",
              "pip install path to .whl file\n",
              "\n",
              "\n",
              "My Python installation is using anaconda3\n",
              "\n",
              "That is very relevant. You should generally avoid as much as possible to use pip in your conda environment. Instead, you can find the correct conda install command for your setup(cuda version etc.) from pytroch.org, e.g. for cuda 11 it would be\n",
              "conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch\n",
              "\n",
              " <P> Change your file .py to another name, you named torch.py when you import torch it will call ur torch.py\n",
              " <P> it's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\n",
              "Try:\n",
              "\n",
              "pip uninstall torch\n",
              "pip uninstall torch\n",
              "pip uninstall torch\n",
              "pip install torch\n",
              " <P> I found solution for my problem. I have use this command\n",
              "\n",
              "C:\\Users\\Userconda install PyTorch -c PyTorch\n",
              "\n",
              "\n",
              "then I have run\n",
              "\n",
              "C:\\Users\\Userpip3 install torchvision\n",
              "\n",
              " <P> Installing from the PyTorch wheel should have worked. But, the problem turns out to be that pip is using the cached pytorch to install it as mentioned on GitHub here.\n",
              "\n",
              "Collecting pytorch\n",
              "  Using cached https://files.pythonhosted.org/packages...\n",
              "\n",
              "\n",
              "Either removing the pip's cache from %LocalAppData%\\pip\\Cache on Windows or disabling it by using --no-cache-dir would solve the issue as follows:\n",
              "\n",
              "pip3 --no-cache-dir install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-win_amd64.whl\n",
              "\n",
              " <P> After troubling shooting and a lot of restart, it seems like the issue came from when pip was trying to load a pre-downloaded file. Essentially, the first time I ran the installation command, pip downloaded files for pytorch but did not install pytorch due to some user privilege issue. The fix is to add --no-cache-dir in the pip install command. This will override the cache (pre-downloaded files) and download the files all over again.\n",
              "For me specifically, I also needed to add --user.\n",
              "In other words, the command went from\n",
              "pip install torch===1.7.0+cu110 torchvision===0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
              "\n",
              "to\n",
              "pip --no-cache-dir install torch===1.7.0+cu110 torchvision===0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html --user\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row87\" class=\"row_heading level0 row87\" >87</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col0\" class=\"data row87 col0\" >Any backward computation that generate “nan” value will what?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col1\" class=\"data row87 col1\" >any backward computation that generate “nan” value will result from the forward pass. Depending on the model, the output tensor may be either a tensor or a tuple of tensors. For the latter, it may be the responsibility of the optimizer to trace backward through the network to trace through this backward pass.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col2\" class=\"data row87 col2\" >raise an error</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row87_col3\" class=\"data row87 col3\" ><P> SGTM <P> Double-backward <P> Will take a look <P> LGTM, will test <P> friday <P> Note <P> It should eventually call into this method for the forward pass. <P> cc malfet <P> On it <P> bumping priority based on user activity</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row88\" class=\"row_heading level0 row88\" >88</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col0\" class=\"data row88 col0\" >AllenNLP Multi-Task Model: Keep encoder weights for new heads</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col1\" class=\"data row88 col1\" >Here is some code that I used to answer this question. A lot of it is specific to what I am doing, but the jist of it can be used by others who are facing the same problem I was.\n",
              "\n",
              "def stream_training(filepath, epochs=100):\n",
              "    :param filepath: File path of pkl file\n",
              "\n",
              ":param epochs: Number of epochs to run</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col2\" class=\"data row88 col2\" >you can accomplish by using a PretrainedModelInitializer. See the CopyNet model for an example of how to add this to your model.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row88_col3\" class=\"data row88 col3\" ><P> \n",
              "I expected that the generated vectors corresponding the subjects provide a strong correlation after training as it is expected from a word embedding\n",
              "\n",
              "I don't really think you'll achieve that kind of result with only 3 sentences and like 40 iterations in 10 epochs (plus most of the data in your 40 iterations is repeated).\n",
              "maybe try downloading a couple of free datasets out there, or try your own data with a proven model like a genism model.\n",
              "I'll give you the code for training a gensim model, so you can test your dataset on another model and see if the problem comes from your data or from your model.\n",
              "I've tested similar gensim models on datasets with millions of sentences and it worked like a charm, for smaller datasets you might want to change the parameters.\n",
              "from gensim.models import Word2Vec\n",
              "from multiprocessing import cpu_count\n",
              "\n",
              "\n",
              "corpus_path = 'eachLineASentence.txt'\n",
              "vecSize = 300\n",
              "winSize = 5\n",
              "numWorkers = cpu_count()-1\n",
              "epochs = 20\n",
              "minCount = 5\n",
              "skipGram = False\n",
              "modelName = f'mymodel.model'\n",
              "\n",
              "model = Word2Vec(corpus_file=corpus_path,\n",
              "                size=vecSize,\n",
              "                window=winSize,\n",
              "                min_count=minCount,\n",
              "                workers=numWorkers,\n",
              "                iter=epochs,\n",
              "                sg=skipGram)\n",
              "model.save(modelName)\n",
              "\n",
              "P.S. I don't think it's a good idea to use the keyword input as a variable in your code.\n",
              " <P> Here is some code that I used to answer this question. A lot of it is specific to what I am doing, but the jist of it can be used by others who are facing the same problem I was.\n",
              "def stream_training(filepath, epochs=100):\n",
              "    \"\"\"\n",
              "    :param filepath: file path of pkl file\n",
              "    :param epochs: number of epochs to run\n",
              "    \"\"\"\n",
              "    def training(train_dataloader, model_obj, criterion, optimizer):\n",
              "        for j, data in enumerate(train_dataloader, start=0):\n",
              "            # get the inputs; data is a list of [inputs, labels]\n",
              "            inputs, labels = data\n",
              "            inputs, labels = inputs.cuda(), labels.cuda()\n",
              "            outputs = model_obj(inputs.float())\n",
              "            outputs = torch.flatten(outputs)\n",
              "            loss = criterion(outputs, labels.float())\n",
              "            print(loss)\n",
              "            # zero the parameter gradients\n",
              "            optimizer.zero_grad()\n",
              "            loss.backward()\n",
              "            torch.nn.utils.clip_grad_norm_(model_obj.parameters(), max_norm=1)\n",
              "            optimizer.step()\n",
              "\n",
              "    tensors = []\n",
              "    expected_values = []\n",
              "    model= Model(1000, 1, 256, 1)\n",
              "    model.cuda()\n",
              "    criterion = nn.BCELoss()\n",
              "    optimizer = optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.99999), eps=1e-08, weight_decay=0.001,\n",
              "                           amsgrad=True)\n",
              "    for i in range(epochs):\n",
              "        with (open(filepath, 'rb')) as openfile:\n",
              "            while True:\n",
              "                try:\n",
              "                    data_list = pickle.load(openfile)\n",
              "                    tensors.append(data_list[0])\n",
              "                    expected_values.append(data_list[1])\n",
              "                    if len(tensors) % BATCH_SIZE == 0:\n",
              "                        tensors = torch.cat(tensors, dim=0)\n",
              "                        tensors = torch.reshape(tensors, (tensors.shape[0], tensors.shape[1], -1))\n",
              "                        train_loader = make_dataset(tensors, expected_values) # makes a dataloader for the batch that comes in\n",
              "                        training(train_loader, model, criterion, optimizer)  #Performs forward and back prop\n",
              "                        tensors = [] # washes out the batch to conserve memory on my computer.\n",
              "                        expected_values = []\n",
              "                except EOFError:\n",
              "                    print(\"This file has finished training\")\n",
              "                    break\n",
              "\n",
              "Here is the model for fun.\n",
              "class Model(nn.Module):\n",
              "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
              "        super(Model, self).__init__()\n",
              "        # dimensions\n",
              "        self.hidden_dim = hidden_dim\n",
              "        self.n_layers = n_layers\n",
              "\n",
              "        #Define the layers\n",
              "        #GRU\n",
              "        self.gru = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
              "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
              "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim)\n",
              "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
              "        self.bn2 = nn.BatchNorm1d(num_features=hidden_dim)\n",
              "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
              "        self.bn3 = nn.BatchNorm1d(num_features=hidden_dim)\n",
              "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
              "        self.bn4 = nn.BatchNorm1d(num_features=hidden_dim)\n",
              "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
              "        self.output = nn.Linear(hidden_dim, output_size)\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = x.float()\n",
              "        x = F.relu(self.gru(x)[1])\n",
              "        x = x[-1,:,:] # eliminates first dim\n",
              "        x = F.dropout(x, 0.5)\n",
              "        x = F.relu(self.bn1(self.fc1(x)))\n",
              "        x = F.dropout(x, 0.5)\n",
              "        x = F.relu(self.bn2(self.fc2(x)))\n",
              "        x = F.dropout(x, 0.5)\n",
              "        x = F.relu(self.bn3(self.fc3(x)))\n",
              "        x = F.dropout(x, 0.5)\n",
              "        x = F.relu(self.bn4(self.fc4(x)))\n",
              "        x = F.dropout(x, 0.5)\n",
              "        x = F.relu(self.fc5(x))\n",
              "        return torch.sigmoid(self.output(x))\n",
              "\n",
              "    def init_hidden(self, batch_size):\n",
              "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
              "        return hidden\n",
              "\n",
              " <P> You are asking about continual learning - this is a very active field of research, and there is no single solution/method to tackle it. You'll have to do more research to find the right approach for your specific settings.\n",
              " <P> I think you can see this https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
              "\n",
              "def get_train_valid_loader(data_dir,\n",
              "                           batch_size,\n",
              "                           augment,\n",
              "                           random_seed,\n",
              "                           valid_size=0.1,\n",
              "                           shuffle=True,\n",
              "                           show_sample=False,\n",
              "                           num_workers=4,\n",
              "                           pin_memory=False):\n",
              "    \"\"\"\n",
              "    Utility function for loading and returning train and valid\n",
              "    multi-process iterators over the CIFAR-10 dataset. A sample\n",
              "    9x9 grid of the images can be optionally displayed.\n",
              "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
              "    Params\n",
              "    ------\n",
              "    - data_dir: path directory to the dataset.\n",
              "    - batch_size: how many samples per batch to load.\n",
              "    - augment: whether to apply the data augmentation scheme\n",
              "      mentioned in the paper. Only applied on the train split.\n",
              "    - random_seed: fix seed for reproducibility.\n",
              "    - valid_size: percentage split of the training set used for\n",
              "      the validation set. Should be a float in the range [0, 1].\n",
              "    - shuffle: whether to shuffle the train/validation indices.\n",
              "    - show_sample: plot 9x9 sample grid of the dataset.\n",
              "    - num_workers: number of subprocesses to use when loading the dataset.\n",
              "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
              "      True if using GPU.\n",
              "    Returns\n",
              "    -------\n",
              "    - train_loader: training set iterator.\n",
              "    - valid_loader: validation set iterator.\n",
              "    \"\"\"\n",
              "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
              "    assert ((valid_size = 0) and (valid_size = 1)), error_msg\n",
              "\n",
              "    normalize = transforms.Normalize(\n",
              "        mean=[0.4914, 0.4822, 0.4465],\n",
              "        std=[0.2023, 0.1994, 0.2010],\n",
              "    )\n",
              "\n",
              "    # define transforms\n",
              "    valid_transform = transforms.Compose([\n",
              "            transforms.ToTensor(),\n",
              "            normalize,\n",
              "    ])\n",
              "    if augment:\n",
              "        train_transform = transforms.Compose([\n",
              "            transforms.RandomCrop(32, padding=4),\n",
              "            transforms.RandomHorizontalFlip(),\n",
              "            transforms.ToTensor(),\n",
              "            normalize,\n",
              "        ])\n",
              "    else:\n",
              "        train_transform = transforms.Compose([\n",
              "            transforms.ToTensor(),\n",
              "            normalize,\n",
              "        ])\n",
              "\n",
              "    # load the dataset\n",
              "    train_dataset = datasets.CIFAR10(\n",
              "        root=data_dir, train=True,\n",
              "        download=True, transform=train_transform,\n",
              "    )\n",
              "\n",
              "    valid_dataset = datasets.CIFAR10(\n",
              "        root=data_dir, train=True,\n",
              "        download=True, transform=valid_transform,\n",
              "    )\n",
              "\n",
              "    num_train = len(train_dataset)\n",
              "    indices = list(range(num_train))\n",
              "    split = int(np.floor(valid_size * num_train))\n",
              "\n",
              "    if shuffle:\n",
              "        np.random.seed(random_seed)\n",
              "        np.random.shuffle(indices)\n",
              "\n",
              "    train_idx, valid_idx = indices[split:], indices[:split]\n",
              "    train_sampler = SubsetRandomSampler(train_idx)\n",
              "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
              "\n",
              "    train_loader = torch.utils.data.DataLoader(\n",
              "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
              "        num_workers=num_workers, pin_memory=pin_memory,\n",
              "    )\n",
              "    valid_loader = torch.utils.data.DataLoader(\n",
              "        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
              "        num_workers=num_workers, pin_memory=pin_memory,\n",
              "    )\n",
              "\n",
              "    # visualize some images\n",
              "    if show_sample:\n",
              "        sample_loader = torch.utils.data.DataLoader(\n",
              "            train_dataset, batch_size=9, shuffle=shuffle,\n",
              "            num_workers=num_workers, pin_memory=pin_memory,\n",
              "        )\n",
              "        data_iter = iter(sample_loader)\n",
              "        images, labels = data_iter.next()\n",
              "        X = images.numpy().transpose([0, 2, 3, 1])\n",
              "        plot_images(X, labels)\n",
              "\n",
              "    return (train_loader, valid_loader)\n",
              "\n",
              "\n",
              "\n",
              "Seems that he use sampler=train_sampler to do the split.\n",
              " <P> As you can see from the documentation is possible to pass a generator to random_split\n",
              "random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
              "\n",
              " <P> Some tips to improve the network in the order of importance (and ease of implementing):\n",
              "\n",
              "1. Training data\n",
              "\n",
              "If you want your generated samples to look real, you have to give some real data to the network. Find a set of names, split those into letters and transform into indices. This step alone would give way more realistic names.\n",
              "\n",
              "2. Separate start and end tokens.\n",
              "\n",
              "I would go with SON (Start Of Name) and EON (End Of Name). In this configuration neural network can learn combinations of letters leading to EON and combinations of letters coming after SON. ATM it's trying to fit two different concepts into this one custom token.\n",
              "\n",
              "3. Unsupervised Pretaining\n",
              "\n",
              "You may want to give your letters some semantic meaning instead of one-hot encoded vectors, check word2vec for basic approach.\n",
              "\n",
              "Basically, each letter would be represented by N-dimensional vector (say 50 dimensions) and would be closer in space if the letter occurs more often next to another letter (a closer to k than x).\n",
              "\n",
              "Simple way to implement that would be taking some text dataset and trying to predict next letter at each timestep. Each letter would be represented by random vector at the beginning, through backpropagation letter representations would be updated to reflect their similarity.\n",
              "\n",
              "Check pytorch embedding tutorial for more info.\n",
              "\n",
              "4. Different architecture\n",
              "\n",
              "You may want to check Andrej Karpathy's idea for generating baby names. It is simply described here.\n",
              "\n",
              "Essentially, after training, you feed your model with random letters (say 10) and tell it to predict the next letter. \n",
              "\n",
              "You remove last letter from random seed and put the predicted one in it's place. Iterate until EON is outputted.\n",
              " <P> The model.train() and model.eval() calls will switch between the training mode (normalizing the input batch with its own stats and updating the running stats) and evaluation mode (normalizing the input batch/sample with the running stats). You don't need to apply the running stats manually. <P> As a general comment, let me just say that you have asked many different questions, which makes it difficult for someone to answer. I suggest asking just one question per StackOverflow post, even if that means making several posts. I will answer just the main question that I think you are asking: \"why is my code crashing and how to fix it?\" and hopefully that will clear up your other questions.\n",
              "Per your code, the output of your model has dimensions (128, 100, 44) = (N, D, C). Here N is the minibatch size, C is the number of classes, and D is the dimensionality of your input. The cross entropy loss you are using expects the output to have dimension (N, C, D) and the target to have dimension (N, D). To clear up the documentation that says (N, C, D1, D2, ..., Dk), remember that your input can be an arbitrary tensor of any dimensionality. In your case inputs have length 100, but nothing is to stop someone from making a model with, say, a 100x100 image as input. (In that case the loss would expect output to have dimension (N, C, 100, 100).) But in your case, your input is one dimensional, so you have just a single D=100 for the length of your input.\n",
              "Now we see the error, outputs should be (N, C, D), but yours is (N, D, C). Your targets have the correct dimensions of (N, D). You have two paths the fix the issue. First is to change the structure of your network so that its output is (N, C, D), this may or may not be easy or what you want in the context of your model. The second option is to transpose your axes at the time of loss computation using torch.transpose https://pytorch.org/docs/stable/generated/torch.transpose.html\n",
              "batch_size = 128\n",
              "sequence_length   = 100\n",
              "number_of_classes = 44\n",
              "# creates random tensor of your output shape (N, D, C)\n",
              "output = torch.rand(batch_size,sequence_length, number_of_classes)\n",
              "# transposes dimensionality to (N, C, D)\n",
              "tansposed_output = torch.transpose(output, 1, 2)\n",
              "# creates tensor with random targets\n",
              "target = torch.randint(number_of_classes, (batch_size,sequence_length)).long()\n",
              "\n",
              "# define loss function and calculate loss\n",
              "criterion = nn.CrossEntropyLoss()\n",
              "loss = criterion(transposed_output, target)\n",
              "print(loss)\n",
              "\n",
              " <P> The prediction from FasterRCNN is of the form:\n",
              "\n",
              " predictions = model([input_img_tensor])\n",
              "[{'boxes': tensor([[419.6865, 170.0683, 536.0842, 493.7452],\n",
              "          [159.0727, 180.3606, 298.8194, 434.4604],\n",
              "          [439.7836, 222.6208, 452.0138, 271.8359],\n",
              "          [444.3562, 224.4628, 456.1511, 265.5336],\n",
              "          [437.7808, 226.5965, 446.2904, 271.2691]], grad_fn=StackBackward),\n",
              "  'labels': tensor([ 1,  1, 32, 32, 32]),\n",
              "  'scores': tensor([0.9997, 0.9996, 0.5827, 0.2102, 0.0943], grad_fn=IndexBackward)}]\n",
              "\n",
              "\n",
              "where the predicted boxes are of [x1, y1, x2, y2] format, with values between 0 and H and 0 and W.\n",
              "\n",
              "You can use OpenCV's rectangle function to overlay bounding boxes on image.\n",
              "\n",
              "import cv2\n",
              "img = cv2.imread('input_iamge.png', cv2.COLOR_BGR2RGB)\n",
              "\n",
              "for i in range(len(predictions[0]['boxes'])):\n",
              "    x1, x2, x3, x4 = map(int, predictions[0]['boxes'][i].tolist())\n",
              "    print(x1, x2, x3, x4)\n",
              "    image = cv2.rectangle(img, (x1, x2), (x3, x4), (255, 0, 0), 1)\n",
              "\n",
              "cv2_imshow('img', image)\n",
              "\n",
              " <P> The GPT2-XL model is the biggest of the four architectures detailed in the paper you linked (1542M parameters). It is trained on the same data as the other three, which is the WebText you're mentioning.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row89\" class=\"row_heading level0 row89\" >89</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col0\" class=\"data row89 col0\" >How can I visualize what happens during loss.backward()?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col1\" class=\"data row89 col1\" >It should (if total.backward()). Try to print and see if they are different?Since the backward is on total i.e loss1+loss2, the computation graph would include both 1,2 inputs.\n",
              "Optimization step actually will update the params of your network. Backprop is a fancy name for PyTorch autograd system that computes the first order gradients.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col2\" class=\"data row89 col2\" >There has been been already mention of pytorchviz which lets you visualize the graph.\n",
              "\n",
              "Here is a small example that might help you to understand how pytorchviz does trace the graph using the grad_fn:\n",
              "\n",
              "import torch\n",
              "from torch import nn\n",
              "d = 5\n",
              "x = torch.rand(d, requires_grad=True)\n",
              "print('Tensor x:', x)\n",
              "y = torch.ones(d, requires_grad=True)\n",
              "print('Tensor y:', y)\n",
              "loss = torch.sum(x*y)*3\n",
              "\n",
              "del x\n",
              "print()\n",
              "print('Tracing back tensors:')\n",
              "def getBack(var_grad_fn):\n",
              "    print(var_grad_fn)\n",
              "    for n in var_grad_fn.next_functions:\n",
              "        if n[0]:\n",
              "            try:\n",
              "                tensor = getattr(n[0], 'variable')\n",
              "                print(n[0])\n",
              "                print('Tensor with grad found:', tensor)\n",
              "                print(' - gradient:', tensor.grad)\n",
              "                print()\n",
              "            except AttributeError as e:\n",
              "                getBack(n[0])\n",
              "\n",
              "loss.backward()\n",
              "getBack(loss.grad_fn)\n",
              "Output:\n",
              "\n",
              "Tensor x: tensor([0.0042, 0.5376, 0.7436, 0.2737, 0.4848], requires_grad=True)\n",
              "Tensor y: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
              "\n",
              "Tracing back tensors:\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Tensor with grad found: tensor([0.0042, 0.5376, 0.7436, 0.2737, 0.4848], requires_grad=True)\n",
              " - gradient: tensor([3., 3., 3., 3., 3.])\n",
              "\n",
              "\n",
              "Tensor with grad found: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
              " - gradient: tensor([0.0125, 1.6129, 2.2307, 0.8211, 1.4543])\n",
              "Further you should definately take a look into how autograd functions (that are used by the backward()-function) are actually work !\n",
              "Here is a tutorial from the pytorch site with an easy and short example:\n",
              "\n",
              "PyTorch: Defining New autograd Functions\n",
              "\n",
              "Hope this helps a bit!</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row89_col3\" class=\"data row89 col3\" ><P> tensor.register_hook(customHook) may work, you need to write customHook to modify grad of the tensor.but as far as I know customHook should be a function of grad only. For your case, you want to make customHook to be a function of grad and workerRank as well? <P> \n",
              "In the example that you shared, there is no validation. Usually, we perform validation to identify if the model improved from the last epoch and save a checkpoint. In this example, authors perform training and testing without any validation check.\n",
              "with torch.no_grad() basically disable the gradient computation. Disabling gradient calculation is useful for inference when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations.\n",
              "Why inside the epoch loop, they perform decoding using a random sample? This is just to see a visualization of the generated image from a random sample. Please note, the test(epoch) function gives you quantitative value which tells you how well the model performs. However, the image generation is intended for qualitative comparison, i.e., how the model improves image generation with each epoch.\n",
              "\n",
              " <P> Based on your description I understand that you are calling optimizer.step() more often (1 out of 100 steps) and calculate the gradients only 1 out of 1000 steps.In this case, the general problem could be that the optimizer updates the parameters with ‚old‚Ägradients, which might not work.To change the gradients, you could either scale the loss itself (divide with a constant) or use hooks to manipulate the .grad attributes of all parameters. <P> \n",
              "  But for me it looks like the optimization step (back-prop) is only applied on the last image of the batch. \n",
              "\n",
              "\n",
              "It should not apply only based on the last image. It should apply based on the batch size. \n",
              "If you set bs=2 and it should apply to the batch of two images.\n",
              "\n",
              "Optimization step actually will update the params of your network. Backprop is a fancy name for PyTorch autograd system that computes the first order gradients.\n",
              " <P> -- Comment on first approach removed, see other answer -- \n",
              "\n",
              "Your second approach would require that you backpropagate with retain_graph=True, which incurs heavy computational costs. Moreover, it is wrong, since you would have updated the network weights with the first optimizer step, and then your next backward() call would compute the gradients prior to the update, which means that the second step() call would insert noise into your updates. If on the other hand you performed another forward() call to backpropagate through the updated weights, you would end up having an asynchronous optimization, since the first layers would be updated once with the first step(), and then once more for each subsequent step() call (not wrong per se, but inefficient and probably not what you wanted in the first place).\n",
              "\n",
              "Long story short, the way to go is the last approach. Reduce each loss into a scalar, sum the losses and backpropagate the resulting loss. Side note; make sure your reduction scheme makes sense (e.g. if you are using reduction='sum' and the losses correspond to a multi-label classification, remember that the number of classes per objective is different, so the relative weight contributed by each loss would also be different)\n",
              " <P> Yes, it should (if total.backward()). Try to print and see if they are different?Since the backward is on total i.e loss1+loss2, the computation graph would include both 1,2 inputs.You could also refer the GAN tutorial where something similar is done <P> We usually   \n",
              "\n",
              "\n",
              "get the loss by the loss function   \n",
              "(if necessary) manipulate the loss, for example do the class weighting and etc   \n",
              "calculate the mean loss of the mini-batch    \n",
              "calculate the gradients by the loss.backward() \n",
              "(if necessary) manipulate the gradients, for example, do the gradient clipping for some RNN models to avoid gradient explosion   \n",
              "update the weights using the optimizer.step() function\n",
              "\n",
              "\n",
              "So in your case, you can first get the mean loss of the mini-batch and then calculate the gradient using the loss.backward() function and then utilize the optimizer.step() function for the weight updating.  \n",
              " <P> In pytorch, when you perform the backward step (calling loss.backward() or similar) the gradients are accumulated in-place. This means that if you call loss.backward() multiple times, the previously calculated gradients are not replaced, but in stead the new gradients get added on to the previous ones. That is why, when using pytorch, it is usually necessary to explicitly zero the gradients between minibatches (by calling optimiser.zero_grad() or similar).\n",
              "\n",
              "If your batch size is limited, you can simulate a larger batch size by breaking a large batch up into smaller pieces, and only calling optimiser.step() to update the model parameters after all the pieces have been processed.\n",
              "\n",
              "For example, suppose you are only able to do batches of size 64, but you wish to simulate a batch size of 128. If the original training loop looks like:\n",
              "\n",
              "optimiser.zero_grad()\n",
              "loss = model(batch_data) # batch_data is a batch of size 128\n",
              "loss.backward()\n",
              "optimiser.step()\n",
              "\n",
              "\n",
              "then you could change this to:\n",
              "\n",
              "optimiser.zero_grad()\n",
              "\n",
              "smaller_batches = batch_data[:64], batch_data[64:128]\n",
              "for batch in smaller_batches:\n",
              "    loss = model(batch) / 2\n",
              "    loss.backward()\n",
              "\n",
              "optimiser.step()\n",
              "\n",
              "\n",
              "and the updates to the model parameters would be the same in each case (apart maybe from some small numerical error). Note that you have to rescale the loss to make the update the same.\n",
              " <P> Probably because the gradient flow graph for NN is destroyed with the gradH step. (check HH.grad_fn vs gradH.grad_fn )\n",
              "So your pred tensor (and subsequent loss) does not contain the necessary gradient flow through the NN network.\n",
              "The loss contains gradient flow for the input X, but not for the NN.parameters().  Because the optimizer only take a step() over thoseNN.parameters(), the network NN is not being updated, and since X is neither being updated, loss does not change.\n",
              "You can check how the loss is sending it's gradients backward by checking loss.grad_fn after loss.backward()\n",
              "and here's a neat function (found on Stackoverflow) to check it:\n",
              "def getBack(var_grad_fn):\n",
              "    print(var_grad_fn)\n",
              "    for n in var_grad_fn.next_functions:\n",
              "        if n[0]:\n",
              "            try:\n",
              "                tensor = getattr(n[0], 'variable')\n",
              "                print(n[0])\n",
              "                print('Tensor with grad found:', tensor)\n",
              "                print(' - gradient:', tensor.grad)\n",
              "                print()\n",
              "            except AttributeError as e:\n",
              "                getBack(n[0])\n",
              "\n",
              "with getBack(loss.grad_fn) after loss.backward() to check it for yourself (maybe reduce size of batch N before though)\n",
              "Edit: It works by changing gradH = torch.autograd.grad(HH, X, create_graph=True)[0]\n",
              " <P>  Andrea_Rosasco: What could I do to prevent this? If you use nightly build, you can pass inputs= to .backward() to specify what you want to compute gradients for. In this case ds_loss.backward(inputs=(lr_list[j],), retrain_graph=True). Otherwise, you can do grad, = autograd.grad(ds_loss, lr_list[j], retain_graph=True); lr_list[j].grad = grad but it is not as nice </td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row90\" class=\"row_heading level0 row90\" >90</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col0\" class=\"data row90 col0\" >Parameter not registering if .to(device) is used</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col1\" class=\"data row90 col1\" >dataParallel requires every input tensor be provided on the first device in its device_ids list.\n",
              "\n",
              "It basically uses that device as a staging area before scattering to the other GPUs and it's the device where final outputs are gathered before returning from forward. If you want device 2 to be the primary device then you just need to put it at the front of the list as follows\n",
              "\n",
              "model = nn.DataParallel(model, deviceids = [2, 0, 1, 3])\n",
              "model.to(f'cuda:{model.device_ids[0]}')\n",
              "After which all</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col2\" class=\"data row90 col2\" >You need to use:\n",
              "\n",
              "self.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row90_col3\" class=\"data row90 col3\" ><P> It's supposed to be `torch.device` <P> your input is on gpu 1 but your net work is on gpu 0. <P> As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream. <P> DataParallel requires every input tensor be provided on the first device in its device_ids list.\n",
              "\n",
              "It basically uses that device as a staging area before scattering to the other GPUs and it's the device where final outputs are gathered before returning from forward. If you want device 2 to be the primary device then you just need to put it at the front of the list as follows\n",
              "\n",
              "model = nn.DataParallel(model, device_ids = [2, 0, 1, 3])\n",
              "model.to(f'cuda:{model.device_ids[0]}')\n",
              "\n",
              "\n",
              "After which all tensors provided to model should be on the first device as well.\n",
              "\n",
              "x = ... # input tensor\n",
              "x = x.to(f'cuda:{model.device_ids[0]}')\n",
              "y = model(x)\n",
              "\n",
              " <P> According to the documentation for torch.cuda.device\n",
              "\n",
              "device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.\n",
              "\n",
              "Based on that we could use something like\n",
              "with torch.cuda.device(self.device if self.device.type == 'cuda' else None):\n",
              "    # do a bunch of stuff\n",
              "\n",
              "which would simply be a no-op when self.device isn't a CUDA device.\n",
              " <P> tensor.to() does not modify the tensor inplace. It returns a new tensor that's stored \n",
              "in the specified device.\n",
              "\n",
              "Use the following instead.\n",
              "\n",
              " images = images.to(device)\n",
              " labels = labels.to(device)\n",
              "\n",
              " <P> Issue is still present on `pytorch==1.3.1`\n",
              "\n",
              "To fix, replace the following:\n",
              "`X = X.to(device)`\n",
              "With this:\n",
              "`X = X.to(device=device)`\n",
              "Provided that \"X\" is a packed sequence. <P> This is so by design. \n",
              "\n",
              "Only the tensors which are a part of the model will move with model.cuda() or model.to(\"cuda\").\n",
              "\n",
              "These tensors are registered with register_parameter or register_buffer. This also includes child modules, parameters and buffers registered with the aforementioned functions. \n",
              "\n",
              "Even though self.a=torch.zeros(1) is actually a part of the class itself, by design it will not be moved to CUDA, instead you would need to do a.to(\"cuda\"), if you haven't used the register* methods.\n",
              " <P> In all likelihood your `device` value is nonsense. Print it out and see what the problem is. <P> Don't use defaults like that in `def __init__(self, x, test1 = Test1()):`.\n",
              "\n",
              "This creates a single instance that will be passed to future constructor calls as default. This is a well-known trap when using default parameters.\n",
              "\n",
              "Try:\n",
              "\n",
              "t = Test2(torch.ones(5), Test1()).cuda()\n",
              "t = Test2(torch.ones(5), Test1()).cuda()\n",
              "\n",
              "and your bug will vanish.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row91\" class=\"row_heading level0 row91\" >91</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col0\" class=\"data row91 col0\" >torch.meshgrid has no docstring if typing.TYPE_CHECKING is True</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col1\" class=\"data row91 col1\" >I solve this issue with this.\n",
              "TensorFlow Backend for ONNX.\n",
              "Let me know if you have any issue.\n",
              "Change from tensorflow 2.0 to 1.14 solves the problem. I believe one was to be created, but I don't know if it has been yet. But if it isnot, `Module: numpy` is what we have</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col2\" class=\"data row91 col2\" >This is fixed in 1.8.0 and in master</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row91_col3\" class=\"data row91 col3\" ><P> Probably can fix this by removing all of the inline methods from TensorIterator. Not sure how to fix this at a deeper level though. <P> I believe one was to be created, but I don't know if it has been yet. But if it isnot, `module: numpy` is what we have <P> I solve this issue with this.\n",
              "Tensorflow Backend for ONNX.\n",
              "Let me know if you have any issue.\n",
              "Change from tensorflow 2.0 to 1.14.Maybe solve the problem.\n",
              " <P> Yes specifying the `dtype` works <P> [solved] The problem was that somewhere in the code `torch.cat` was applied to a single tensor only instead of a list of tensors, i.e., `torch.cat([tensor])`. While this worked fine for normal runs, the JIT tracer threw the above mentioned error without any hint on where it occured. @ailzhang <P> the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576 <P> @oneway234 - Several updates have been added to ONNX exporter in PyTorch since PyTorch 1.3.1. Could you please try with the latest version of PyTorch (1.7) and see if this issue is resolved. <P> Hello @Sakulaki ,\n",
              " \n",
              " \n",
              " \n",
              " IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method.\n",
              " \n",
              " So for the case of tuple it should be used smth like this:\n",
              " \n",
              " \n",
              " \n",
              "  final IValue output = mModule.forward(IValue.from(mInputTensor));\n",
              " \n",
              "  IValue[] outputTuple = output.toTuple();\n",
              " \n",
              "  for (int i = 0; i < outputTuple.length; i++) {\n",
              " \n",
              "  IValue tupleElement = outputTuple[i];\n",
              " \n",
              "  }\n",
              " \n",
              " \n",
              " \n",
              " Or did not I get your problem right? <P> It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions. <P> We cant assign it to you in the UI (github doesnt' allow), but just assume you've been assigned.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row92\" class=\"row_heading level0 row92\" >92</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col0\" class=\"data row92 col0\" >CNN weights at input image location</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col1\" class=\"data row92 col1\" >The model used in that example returns a tensor of logits of shape (batch size, classes). Assuming what you mean by \"accuracy value\" is the predicted probability of the class with the largest probability, what you need to do is first compute your probabilities by taking the SoftMax of the output from the model, which gives the predicted probabilities for each image in your batch.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col2\" class=\"data row92 col2\" >Using adaptive pooling, with adaptive pooling, one can reduce any feature map size.\n",
              "\n",
              "Adaptive max pooling</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row92_col3\" class=\"data row92 col3\" ><P> The model used in that example returns a tensor of logits of shape (batch size, classes). Assuming what you mean by \"accuracy value\" is the predicted probability of the class with the largest probability, what you need to do is first compute your probabilities by taking the SoftMax of the output from the model, which gives the predicted probabilities for each image in your batch. Their visualize_model function would look something like the following, though I haven't tested it.\n",
              "\n",
              "def visualize_model(model, num_images=6):\n",
              "    was_training = model.training\n",
              "    model.eval()\n",
              "    images_handeled = 0\n",
              "    fig = plt.figure()\n",
              "\n",
              "    with torch.no_grad():\n",
              "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
              "            inputs = inputs.to(device)\n",
              "            labels = labels.to(device)\n",
              "\n",
              "            outputs = model(inputs)\n",
              "            probabilities = nn.functional.softmax(outputs, dim=-1) # compute probabilities\n",
              "            _, preds = torch.max(outputs, 1)\n",
              "\n",
              "            for j in range(inputs.size()[0]):\n",
              "                images_handeled += 1\n",
              "                ax = plt.subplot(num_images//2, 2, images_handeled)\n",
              "                ax.axis('off')\n",
              "                ax.set_title('predicted: {}, probability: {}'.format(class_names[preds[j]], probabilities[preds[j]])) # add predicted class probability\n",
              "                imshow(inputs.cpu().data[j])\n",
              "\n",
              "                if images_handeled == num_images:\n",
              "                    model.train(mode=was_training)\n",
              "                    return\n",
              "        model.train(mode=was_training)\n",
              "\n",
              "\n",
              "Or do you mean overall classification accuracy?\n",
              " <P> If you take a look at the dataloader documentation, you'll see a drop_last parameter, which explains that sometimes when the dataset size is not divisible by the batch  size, then you get a last batch of different size. So basically the answer is yes, it is possible, it happens often and it does not affect (too much) the training of a neural network.\n",
              "However you must a bit careful, some pytorch layers deal poorly with very small batch sizes. For example if you happen to have Batchnorm layers, and if you get a batch of size 1, you'll get errors due to the fact that batchnorm at some point divides by len(batch)-1. More generally, training a network that has batchnorms generally require batches of significant sizes, say at least 16 (literature generally aims for 32 or 64). So if you happen to have variable size batches, take the time to check whether your layers have requirement in terms of batch size for optimal training and convergence. But except in particular cases, your network will train anyway, no worries.\n",
              "As for how to make your batches with custom sizes, I suggest you look at and take inspiration from the pytorch implementation of dataloader and sampler. You may want to implement something similar to BatchSampler and use the batch_sampler argument of Dataloader\n",
              " <P> The weights and biases of your neural network layer are not specified in terms of batch size.\n",
              "\n",
              "eg: w1 = torch.randn(784,256) :\n",
              "This is a 2D matrix you're going to use for a matrix multiply.\n",
              "784 is the dimension of your input image without considering batch size. (I'm guessing this is for mnist? it looks like you're flattening the 2d images to a 1d vector so 28*28=784).\n",
              "\n",
              "256 is your output dimension of your output (how many logits you're using)\n",
              "\n",
              "Similarly, b1 = torch.randn(256):\n",
              "This is an 1D vector you're just adding to the logits.\n",
              "\n",
              "256 is the dimension of the logits\n",
              "\n",
              "Pytorch automaticallly broadcasts (repeats) these over the batch dimension for all your operations, so it doesn't matter what the batch size was.\n",
              "\n",
              "Eg. eg in adding, b1 is automatically repeated over the first dimension, so it's actual shape for the add is (batch_size, 256).\n",
              "\n",
              "By convention, pytorch \"aligns\" dimensions from right to left.\n",
              "\n",
              "\n",
              "if any are missing, it then repeats the tensor over the missing dimension\n",
              "If any dimension is 1, it repeats the tensor over that dimension to match the other operand.\n",
              "Eg (copied from the docs on broadcasting)\n",
              "\n",
              "\n",
              " x=torch.empty(5,7,3)\n",
              " y=torch.empty(5,7,3)\n",
              "# same shapes are always broadcastable (i.e. the above rules always hold)\n",
              "\n",
              " x=torch.empty((0,))\n",
              " y=torch.empty(2,2)\n",
              "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
              "\n",
              "# can line up trailing dimensions\n",
              " x=torch.empty(5,3,4,1)\n",
              " y=torch.empty(  3,1,1)\n",
              "# x and y are broadcastable.\n",
              "# 1st trailing dimension: both have size 1\n",
              "# 2nd trailing dimension: y has size 1\n",
              "# 3rd trailing dimension: x size == y size\n",
              "# 4th trailing dimension: y dimension doesn't exist\n",
              "\n",
              "# but:\n",
              " x=torch.empty(5,2,4,1)\n",
              " y=torch.empty(  3,1,1)\n",
              "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n",
              "\n",
              "\n",
              "This is really convenient because it means you don't have to redefine your neural net every time you want to use a different batch_size\n",
              "\n",
              "Here's a link if you want to learn more about broadcasting in pytorch\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  Also what is the difference between tensor (256), (256,) and (256,1)\n",
              "\n",
              "\n",
              "the first two are exactly the same; python generally allows for trailing commas in tuple expressions. You are creating a 1D vector of 256 elements.\n",
              "\n",
              "The last one is different; you are creating a 2D tensor where the first dimension is 256 and the second dimension is 1. The underlying data is the same, and it doesn't matter as long as you're consistent about which you're using, but if you mix them, it often leads to undesired behavior:\n",
              "\n",
              "Eg: \n",
              "\n",
              "a = torch.randn(256)\n",
              "b = torch.randn(256)\n",
              "c = a + b\n",
              "c.shape\n",
              " torch.Size([256])\n",
              "\n",
              "\n",
              "Simple: they just add element-wise.\n",
              "\n",
              "But notice what happens when one of them is shaped (-1,1):\n",
              "\n",
              "b = b.view(-1,1) # -1 here means torch will infer the shape of this dimension based on the known size of the tensor and all other specified dimensions\n",
              "b.shape\n",
              " torch.Size([256, 1])\n",
              "\n",
              "c = a + b\n",
              "\n",
              "\n",
              "Now because of broadcasting rules\n",
              "\n",
              "\n",
              "a is repeated over the first dimension so it has the same number of dimensions as b, so it automatically interpretes a as tensor(256,256)\n",
              "b is repeated so it's last dimension (1) now matches the dimension of a (256) \n",
              "\n",
              "\n",
              "so:\n",
              "\n",
              "c.shape\n",
              " torch.Size([256, 256])\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "Hint: The broadcasting rules can be hard to remember, and are often the source of bugs. When in doubt about tensor shapes, it's worth running your code in an interpreter line by line with dummy data and just checking what the shape of each tensor is eg print(torch.mm(input,w1).shape)\n",
              " <P> Why would you need a library to do that?\n",
              "\n",
              "Simply pass the same examples through all your neural networks and get the predictions (either logits or probabilities or labels).\n",
              "\n",
              "\n",
              "Hard voting choose the label predicted most often by classifiers. \n",
              "Soft voting, average probabilities predicted by classifiers and choose the label having the highest.\n",
              "Weighted voting - either of the above can be weighted. Just assign weights to each classifier and multiply their predictions by them. Weights are usually normalized to (0, 1] range.\n",
              "\n",
              "\n",
              "In principle you could also sum logits and choose the label with highest.\n",
              "\n",
              "Oh, and weight averaging is different technique and requires you to have the same model and usually is done for the same initialization but at different training timesteps. You can read about it in this blog post.\n",
              " <P> In the tutorials you would see on the internet, people mostly do multi-class classification, for which they use cross-entropy loss which doesn't require a user defined activation function at the output. It applies the softmax activation itself (actually applying an activation function before the cross-entropy is one of the most common mistakes in PyTorch). However, in your case you have a binary classification problem, for which you need to use binary cross-entropy loss, which doesn't apply any activation function by itself unlike the other one. So you will need to apply sigmoid activation (or any kind of activation that maps the real numbers to the range (0, 1) yourself.\n",
              " <P> You data has the following shape [batch_size, c=1, h=28, w=28]. batch_size equals 64 for train and 1000 for test set, but that doesn't make any difference, we shouldn't deal with the first dim.\n",
              "To use F.cross_entropy, you must provide a tensor of size [batch_size, nb_classes], here nb_classes is 10. So the last layer of your model should have a total of 10 neurons.\n",
              "As a side note, when using this criterion you shouldn't use F.log_softmax on the model's output (see here).\n",
              "\n",
              "This criterion combines log_softmax and nll_loss in a single function.\n",
              "\n",
              "This is not the issue though. The problem is your model doesn't ouput [batch_size, 10] tensors. The problem is your use of view: the tensor goes from torch.Size([64, 128, 6, 6]) to torch.Size([32, 9216]). You've basically said \"squash everything to a total of 9216 (128*6*6*64/2) on dim=1 and let the rest (32) stay on dim=0\". This is not desired since your messing up the batches.\n",
              "It's easier to use a Flatten layer in this particular instance after your CNN layers. This will flatten all values from each channels. Make sure to preserve the first dimension though with start_dim=1.\n",
              "Here's an example, meant to show, layers are random but the code runs. You should tweak the kernel sizes, number of channels etc... to your liking!\n",
              "class Net(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Net, self).__init__()\n",
              "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4)\n",
              "        self.conv2 = nn.Conv2d(32, 32, kernel_size=8)\n",
              "        self.fc1 = nn.Linear(128, 100)\n",
              "        self.fc2 = nn.Linear(100, 10)\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
              "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
              "        x = torch.flatten(x, start_dim=1)\n",
              "        x = F.relu(self.fc1(x))\n",
              "        x = self.fc2(x)\n",
              "        return x\n",
              "\n",
              " <P> You seem to be thinking of the binary classification as a multi-class classification with two classes, but that is not quite correct when using the binary cross-entropy approach. Let's start by clarifying the goal of the binary classification before looking at any implementation details.\n",
              "\n",
              "Technically, there are two classes, 0 and 1, but instead of considering them as two separate classes, you can see them as opposites of each other. For example, you want to classify whether a StackOverflow answer was helpful or not. The two classes would be \"helpful\" and \"not helpful\". Naturally, you would simply ask \"Was the answer helpful?\", the negative aspect is left off, and if that wasn't the case, you could deduce that it was \"not helpful\". (Remember, it's a binary case, there is no middle ground).\n",
              "\n",
              "Therefore, your model only needs to predict a single class, but to avoid confusion with the actual two classes, that can be expressed as: The model predicts the probability that the positive case occurs. In context of the previous example: What is the probability that the StackOverflow answer was helpful?\n",
              "\n",
              "Sigmoid gives you values in the range [0, 1], which are the probabilities. Now you need to decide when the model is confident enough for it to be positive by defining a threshold. To make it balanced, the threshold is 0.5, therefore as long as the probability is greater than 0.5 it is positive (class 1: \"helpful\") otherwise it's negative (class 0: \"not helpful\"), which is achieved by rounding (i.e. torch.round(torch.sigmoid(pred))).\n",
              "\n",
              "\n",
              "  After that the choice of Loss function is loss_fn=BCEWithLogitsLoss() (which is numerically stable than using the softmax first and then calculating loss) which will apply Softmax function to the output of last layer to give us a probability.\n",
              "  \n",
              "  Isn't it better to use the sigmoid once after the last layer within the network rather using a softmax and a sigmoid at 2 different places given it's a binary classification??\n",
              "\n",
              "\n",
              "BCEWithLogitsLoss applies Sigmoid not Softmax, there is no Softmax involved at all. From the nn.BCEWithLogitsLoss documentation:\n",
              "\n",
              "\n",
              "  This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n",
              "\n",
              "\n",
              "By not applying Sigmoid in the model you get a more numerically stable version of the binary cross-entropy, but that means you have to apply the Sigmoid manually if you want to make an actual prediction outside of training.\n",
              "\n",
              "\n",
              "  [...] and use the argmax() for checking accuracy??\n",
              "\n",
              "\n",
              "Again, you're thinking of the multi-class scenario. You only have a single output class, i.e. output has size [batch_size, 1]. Taking argmax of that, will always give you 0, because that is the only available class.\n",
              " <P> The important concept is not so much the batch size; it's the quantity of epochs you train.  Can you double the batch size, giving you the same cluster batch size?  If so, that will compensate directly for the problem.  If not, double the quantity of iterations, so you're training for the same quantity of epochs.  The model will quickly overcome the effects of the early-batch bias.\n",
              "\n",
              "However, if you are comfortable digging into the training code, myrtlecat gave you an answer that will eliminate the batch-size difference quite nicely.\n",
              " <P> To me, this looks more like linear regression than logistic regression. You are trying to fit a linear model onto your data. It's different to a binary classification task where you would need to use a special kind of activation function (a sigmoid for instance) so that the output is either 0 or 1.\n",
              "In this particular instance you want to solve a 2D linear problem given input x of shape (batch, x1, x2) (where x1 is trade_quantity and x2 is trade_value) and target (batch, y) (y being the stock_price).\n",
              "So the objective is to find the best w and b matrices (weight matrix and bias column) so that x@w + b is the closest to y as possible, according to your criterion, the mean square error.\n",
              "\n",
              "I would recommend normalizing your data so it stays in a [0, 1] range. You can do so by measuring the mean and standard deviation of inputs and targets.\n",
              "inputs_min, inputs_max = inputs.min(axis=0).values, inputs.max(axis=0).values\n",
              "targets_min, targets_max = targets.min(axis=0).values, targets.max(axis=0).values\n",
              "\n",
              "Then applying the transformation:\n",
              "x = (inputs - inputs_min)/(inputs_max - inputs_min)\n",
              "y = (targets - targets_min)/(targets_max - targets_min)\n",
              "\n",
              "Try changing your learning rate and have it run for multiple epochs.\n",
              "lr = 1e-2\n",
              "for epochs in range(100):\n",
              "    preds = model(x)\n",
              "    loss = mse(preds, y)\n",
              "    loss.backward()\n",
              "    with torch.no_grad():\n",
              "        w -= lr*w.grad\n",
              "        b -= lr*b.grad\n",
              "        w.grad.zero_()\n",
              "        b.grad.zero_()\n",
              "\n",
              "I use a (1, 2) randomly initialized matrix for w (and a (1,) matrix for b):\n",
              "w = torch.rand(1, 2)\n",
              "w.requires_grad = True\n",
              "b = torch.rand(1)\n",
              "b.requires_grad = True\n",
              "\n",
              "And got the following train loss over 100 epochs:\n",
              "\n",
              "To find the right hyperparameters, it's better to have a validation set. This set will get normalized with the mean and std from the train set. It will be used to evaluate the performances at the end of each epoch on data that is 'unknown' to the model. Same goes for your test set, if you have one.\n",
              " <P> I might be wrong but I am trying to answer based on your code in the question.\n",
              "\n",
              "You are using BCEwithlogitsloss which means that the model is expected to output logits. logits are the output just before using the sigmoid activation. Recall, sigmoid activation is used to convert outputs to probability (essentially to be between 0 and 1). Logits can be any real number. \n",
              "\n",
              "Based on this, I think you should pass the output of the model through a sigmoid activation i.e. F.sigmoid(phi(x_batch)). Or you can also just check if your model's output is greater than 0 or less than 0. If greater than 0, the label should be 1. \n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row93\" class=\"row_heading level0 row93\" >93</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col0\" class=\"data row93 col0\" >torch: minimally pad tensor such that num elements divisible by x</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col1\" class=\"data row93 col1\" >use.squeeze() and a negative index.\n",
              "\n",
              "a = torch.array([[[[1.0, 1.1]]], [[[2.1, 2.0]]]])\n",
              "np.argmax(a, axis = -1).squeez()\n",
              "\n",
              "array([1, 0], dtype=int32)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col2\" class=\"data row93 col2\" >simply adding one to the largest dimension until numel is divisible by x doesn't work in all cases. For example if the shape of t is (3, 2) and x = 9 then we would want to pad t to be (3, 3), not (9, 2)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row93_col3\" class=\"data row93 col3\" ><P> Indexing in PyTorch is almost similar to numpy.\n",
              "\n",
              "a = torch.randn(2, 2, 3)\n",
              "b = torch.eye(2, 2, dtype=torch.long)\n",
              "c = torch.eye(2, 2, dtype=torch.long)\n",
              "\n",
              "print(a)\n",
              "print(a[b, c, :])\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "tensor([[[ 1.2471,  1.6571, -2.0504],\n",
              "         [-1.7502,  0.5747, -0.3451]],\n",
              "\n",
              "        [[-0.4389,  0.4482,  0.7294],\n",
              "         [-1.3051,  0.6606, -0.6960]]])\n",
              "tensor([[[-1.3051,  0.6606, -0.6960],\n",
              "         [ 1.2471,  1.6571, -2.0504]],\n",
              "\n",
              "        [[ 1.2471,  1.6571, -2.0504],\n",
              "         [-1.3051,  0.6606, -0.6960]]])\n",
              "\n",
              " <P> Use .squeeze() and a negative index.\n",
              "\n",
              "a = np.array([[[[1.0, 1.1]]], [[[2.1, 2.0]]]])\n",
              "np.argmax(a, axis = -1).squeeze()\n",
              "\n",
              "array([1, 0], dtype=int32)\n",
              "\n",
              " <P> You can use range; and squeeze to get proper idx dimension like\n",
              "x[range(x.size(0)), idx.squeeze()]\n",
              "tensor([10., 43.])\n",
              "\n",
              "# or\n",
              "x[range(x.size(0)), idx.squeeze()].unsqueeze(1)\n",
              "tensor([[10.],\n",
              "        [43.]])\n",
              "\n",
              " <P> Simply do:\n",
              "res = x.argmax(axis = 2)\n",
              "\n",
              " <P> \n",
              "Excuse me, if you have a tensor like this:\n",
              "tensor([[[ 1.1937, -0.7235, -0.1802, -0.5610],\n",
              "         [-0.7524,  1.3047, -1.5577,  1.8352],\n",
              "         [ 1.1573, -1.8952,  0.4175, -0.2085]],\n",
              "\n",
              "        [[ 0.4069, -0.1069, -0.3838, -0.2991],\n",
              "         [-0.5824, -0.4965, -0.1542,  1.1482],\n",
              "         [ 0.5182,  0.5445,  1.1730, -0.2523]]])\n",
              "\n",
              "and lengths = torch.tensor([1, 2])\n",
              "do you want to have sth like this?\n",
              "tensor([[[ 1.1937, -0.7235, -0.1802, -0.5610],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "         [ 1.1573, -1.8952,  0.4175, -0.2085]],\n",
              "\n",
              "        [[ 0.4069, -0.1069, -0.3838, -0.2991],\n",
              "         [-0.5824, -0.4965, -0.1542,  1.1482],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n",
              "\n",
              "or sth like this:\n",
              "tensor([[[ 1.4429, -0.8365, -0.2565,  0.2319],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 1.7247,  0.2831,  0.0687,  0.6557],\n",
              "         [ 0.5110,  1.7607,  0.8107,  0.3624],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n",
              "\n",
              "Thanks\n",
              "I want second option :\n",
              "tensor([[[ 1.4429, -0.8365, -0.2565,  0.2319],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 1.7247,  0.2831,  0.0687,  0.6557],\n",
              "         [ 0.5110,  1.7607,  0.8107,  0.3624],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n",
              "\n",
              "This works:\n",
              ">>> import torch\n",
              ">>> a = torch.rand(2, 3, 4)\n",
              ">>> print(a)\n",
              "tensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n",
              "         [0.4995, 0.1689, 0.6175, 0.7917],\n",
              "         [0.7996, 0.0225, 0.1145, 0.4249]],\n",
              "\n",
              "        [[0.0975, 0.2995, 0.5857, 0.0806],\n",
              "         [0.4922, 0.4778, 0.9133, 0.1418],\n",
              "         [0.6594, 0.4907, 0.3268, 0.1211]]])\n",
              ">>> l = torch.randint(3, (2, ))\n",
              ">>> print(l)\n",
              "tensor([1, 2])\n",
              ">>> l += torch.arange(2) * 3\n",
              ">>> b = torch.arange(6).view(2, 3)\n",
              ">>> b = b < l.view(2, 1)\n",
              ">>> a * b.view(2, 3, 1).float()\n",
              "tensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.0975, 0.2995, 0.5857, 0.0806],\n",
              "         [0.4922, 0.4778, 0.9133, 0.1418],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
              ">>> \n",
              "\n",
              "I hope it helps !\n",
              "\n",
              "\n",
              "\n",
              " LeviViana:\n",
              "\n",
              "b = b < l.view(2, 1)\n",
              "\n",
              "\n",
              "What does this command do?\n",
              "It creates a binary tensor whose values are calculated with respect to the comparison of l and b. The .view(2, 1) is for broadcasting purposes.\n",
              ">>> import torch\n",
              ">>> l = torch.randint(3, (2, ))\n",
              ">>> l # the original form of the lengths\n",
              "tensor([2, 1])\n",
              ">>> l += torch.arange(2) * 3\n",
              ">>> l # the modified form of the lengths for the comparison to work\n",
              "tensor([2, 4])\n",
              ">>> b = torch.arange(6).view(2, 3)\n",
              ">>> b\n",
              "tensor([[0, 1, 2],\n",
              "        [3, 4, 5]])\n",
              ">>> b < l.view(2, 1)\n",
              "tensor([[1, 1, 0],\n",
              "        [1, 0, 0]], dtype=torch.uint8)\n",
              "\n",
              "The trick is to use torch.arange and then compare it to the indices you want to create the effect of zeroing starting from a given point. I hope it is clear !\n",
              "Thanks LeviViana with link \"https://discuss.pytorch.org/u/LeviViana\"\n",
              "Nice solution\n",
              "I also found a bit different approach here:\n",
              "\n",
              "\n",
              "stackoverflow.com with link \"https://stackoverflow.com/questions/57548180/filling-torch-tensor-with-zeros-after-certain-index/57550753#57550753\"\n",
              "\n",
              "\n",
              " with link \"https://stackoverflow.com/users/1714410/shai\"\n",
              "\n",
              "Filling torch tensor with zeros after certain index with link \"https://stackoverflow.com/questions/57548180/filling-torch-tensor-with-zeros-after-certain-index/57550753#57550753\"\n",
              "\n",
              "\n",
              "python, nlp, pytorch\n",
              "\n",
              "\n",
              "  \n",
              "  answered by\n",
              "  Shai with link \"https://stackoverflow.com/users/1714410/shai\"\n",
              "  on 05:07AM - 19 Aug 19 UTC with link \"https://stackoverflow.com/questions/57548180/filling-torch-tensor-with-zeros-after-certain-index/57550753#57550753\"\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              " <P> Numpy has the function put_along_axis that can answer this question, I haven't found the torch equivalent for this function.\n",
              "import numpy as np\n",
              "\n",
              "input = torch.randn(3, 10)\n",
              "result = np.zeros(input.size())\n",
              "np_argmax = torch.argmax(input, dim=0, keepdim=True).numpy()\n",
              "result = np.put_along_axis(result, np_argmax, 1, axis=0)\n",
              "# go back to torch:\n",
              "result = torch.from_numpy(result)\n",
              "\n",
              " <P> You can also use np.indices. If the row and column indexes % 2 are equal, you are on an \"even\" diagonal. If they are not equal you are on an \"odd\" diagonal.\n",
              "arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
              "rows, cols = np.indices(arr.shape)\n",
              "\n",
              "Set the even indexes to 0\n",
              " arr[(rows%2)==(cols%2)] = 0\n",
              " arr\n",
              "... array([[0, 2, 0],\n",
              "           [4, 0, 6],\n",
              "           [0, 8, 0]])\n",
              "\n",
              "Set the odd indexes to 0\n",
              " arr[(rows%2)!=(cols%2)] = 0\n",
              " arr\n",
              "... array([[1, 0, 3],\n",
              "           [0, 5, 0],\n",
              "           [7, 0, 9]])\n",
              "\n",
              " <P> arr[[0,1,2,3], [0,2,1,2]]\n",
              "\n",
              "or if you prefer np.arange(4) for the 1st indexing array.\n",
              " <P> You are looking for numpy.argmax\n",
              " <P> answered here: torch.lstsq returns wrong tensor size ¬∑ Issue #56833 ¬∑ pytorch/pytorch ¬∑ GitHub</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row94\" class=\"row_heading level0 row94\" >94</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col0\" class=\"data row94 col0\" >Memory Error in pip install of torch 1.2.0 on Linux</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col1\" class=\"data row94 col1\" >I just reported another issue with this API in #42695, please take a look. Apparently, it produces so many kinds of error message with abort, and even segfault. Thank you!\n",
              "\n",
              "Replaceacing the existing numpy with `numpy?1.14.5+mkl?cp36;cp36M?win_amd64.whl` from  resolves the issue for me (but not before wasting hours in try-this-and-that).\n",
              "\n",
              "It would've been nice if this issue was mentioned on the PyTorch installation page.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col2\" class=\"data row94 col2\" >`pip install --no-cache-dir install torchvision` seems to have gotten around the issue. I guess there is a typo in your command. The command which worked for me: `pip --no-cache-dir install torchvision`</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row94_col3\" class=\"data row94 col3\" ><P> Installing netcdf4 via pip solved the problem.\n",
              " <P> Thanks Sam I have fix for it. Will submit PR shortly. <P> I just reported another issue with this API in #42695, please take a look. Apparently, it produces so many kinds of error message with abort, and even segfault. Thank you! <P> Replacing the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl` from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy resolved the issue for me (but not before wasting hours in try-this-and-that).\n",
              " It would've been nice if this issue was mentioned on the PyTorch installation page. <P> @qbx2 This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again? <P> `sudo rm -rf ~/.nv` works. <P> @shoukang by any chance you can try installing new Pytorch, we have done a lot in terms of threading recently. And your problem might be related. <P> Got it, I tried on a mac and I can reproduce this. We will look into it, thank you for the report <P> Can you please check if this fix to SobolEngineOps.cpp fixes it for you? \n",
              " \n",
              " https://github.com/pytorch/pytorch/pull/36711 <P> I found the bug, I'll send a patch soon.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row95\" class=\"row_heading level0 row95\" >95</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col0\" class=\"data row95 col0\" >does pytorch has geometric deep learning support?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col1\" class=\"data row95 col1\" >PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as Geometric Deep Learning, from a variety of published papers. In addition, it consists of an easy-to-use mini-batch loader for many small and single giant graphs, multi GPU-support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D clouds.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col2\" class=\"data row95 col2\" > PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row95_col3\" class=\"data row95 col3\" ><P> PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers. In addition, it consists of an easy-to-use mini-batch loader for many small and single giant graphs, multi gpu-support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds. <P> \n",
              "\"Kernel\" here is for computation kernels: https://en.wikipedia.org/wiki/Compute_kernel\n",
              "Operations like convolution are often implemented using compute kernels for better efficiency. Compute kernels can be written using C, CUDA, OpenCL or even assembly for maximum efficiency. It is therefore not surprizing that \"a Python-only build\" does not support...\n",
              "\"Fusing\" means commonalization of computation steps. Basically, it's an implementation trick to run code more efficiently by combining similar operations in a single hardware (GPU, CPU or TPU) operation. Therefore, a \"fusedLayer\" is a layer where operations benefit from a \"fused\" implementation.\n",
              "\n",
              " <P> The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t. <P> Currently, it's not possible to use Cloud TPU with PyTorch since it's designed specifically for Tensorflow. \n",
              "\n",
              "But, according to this product news posted three days ago in the Google Cloud blog, \"engineers on Google’s TPU team are actively collaborating with core PyTorch developers to connect PyTorch to Cloud TPUs\".\n",
              " <P> As of PyTorch v1.6.0, features in torch.distributed can be categorized into three main components:Distributed Data-Parallel Training (DDP) is a widely adopted single-program multiple-data training paradigm. With DDP, the model is replicated on every process, and every model replica will be fed with a different set of input data samples. DDP takes care of gradient communications to keep model replicas synchronized and overlaps it with the gradient computations to speed up training.RPC-Based Distributed Training (RPC) is developed to support general training structures that cannot fit into data-parallel training, such as distributed pipeline parallelism, parameter server paradigm, and combination of DDP with other training paradigms. It helps manage remote object lifetime and extend autograd engine to beyond machine boundaries.Collective Communication (c10d) library support sending tensors across processes within a group. It offers both collective communication APIs (e.g., all_reduce and all_gather) and P2P communication APIs (e.g., send and isend). DDP and RPC (ProcessGroup Backend) are built on c10d as of v1.6.0, where the former uses collective communications and the latter uses P2P communications. Usually, developers do not need to directly use this raw communication API, as DDP and RPC features above can serve many distributed training scenarios. However, there are use cases where this API is still helpful. One example would be distributed parameter averaging, where applications would like to compute the average values of all model parameters after the backward pass instead of using DDP to communicate gradients. This can decouple communications from computations and allow finer-grain control over what to communicate, but on the other hand, it also gives up the performance optimizations offered by DDP. The Writing Distributed Applications with PyTorch shows examples of using c10d communication APIs.Most of the existing documents are written for either DDP or RPC, the remainder of this page will elaborate materials for these two components. <P> That's a good question. I just asked the same question in Pyro's dedicated forum. Here's the answer of one of their core developers: \"There are many cool stuffs in Pyro that do not appear in NumPyro, for example, see Contributed code section in Pyro docs. For me, while developing, it is much easier to debug PyTorch code than Jax code (though Jax team has put much effort to help debugging in recent releases). Hence to implement a new inference algorithm, it is easier for me to work in Pyro.\"\n",
              " <P> Few options depending on the API in TF you're using:\n",
              "\n",
              "\n",
              "tf.concat - most similar to torch.cat:\n",
              "\n",
              "tf.concat(values, axis, name='concat')\n",
              "\n",
              "tf.keras.layers.concatenate - if you're using Keras sequential API:\n",
              "\n",
              "tf.keras.layers.concatenate(values, axis=-1, **kwargs)\n",
              "\n",
              "tf.keras.layers.Concatenate - if you're using Keras functional API:\n",
              "\n",
              "x = tf.keras.layers.Concatenate(axis=-1, **kwargs)(values)\n",
              "\n",
              "\n",
              "\n",
              "If you're using the Keras API, this answer is informative for understanding the differences between all the Keras concatenation functions.\n",
              " <P> More information and testing done by xymeng in github could be seen in the given link\n",
              "\n",
              "Referencing xymeng's words : \n",
              "\n",
              "\n",
              "  PyTorch has its own cuda kernels. From my measurement the cuda runtime allocates ~1GB memory for them. If you compile pytorch with cudnn enabled the total memory usage is 1GB + 750M + others = 2GB+\n",
              "  Note that this is just my speculation as there is no official documentation about this. What puzzles me is that the cuda runtime allocates much more memory than the actual code size (they are approx. linearly correlated. If I remove half of pytorch's kernels the memory usage is also reduced by half). I suspect either the kernel binaries have been compressed or they have to be post-processed by the runtime.\n",
              "\n",
              "\n",
              "Seems it suits your situation.\n",
              " <P> Feature Pyramid Networks(FPN) for Object Detection is not an RPN. \n",
              "\n",
              "FPN is just a better way to do feature extraction. It incorporates features from several stages together which gives better features for the rest of the object detection pipeline (specifically because it incorporates features from the first stages which gives better features for detection of small/medium size objects).\n",
              "\n",
              "As the original paper states: \"Our goal is to leverage a ConvNet’s pyramidal feature\n",
              "hierarchy, which has semantics from low to high levels, and\n",
              "build a feature pyramid with high-level semantics throughout. The resulting Feature Pyramid Network is general purpose and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and\n",
              "region-based detectors (Fast R-CNN)\"\n",
              "\n",
              "So they use it to check \"Two stage\" object detection pipeline. The first stage is the RPN and this is what they check in section 5.1 and then they check it for the classification stage in section 5.2.\n",
              "\n",
              "Fast R-CNN Faster R-CNN etc.. are region based object detectors and not sliding window detectors. They get a fixed set of regions from the RPN to classify and thats it.\n",
              "\n",
              "A good explanation on the differences you can see at https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9. \n",
              " <P> As described in the tutorial, the next step after writing the extension is to write a custom CUDA kernel and reduce the kernel launch overheads A definite method of speeding things up is therefore to rewrite parts in C++ (or CUDA) and  fuse particular groups of operations. Fusing means combining the implementations of many functions into a single functions, which profits from fewer kernel launches as well as other optimizations we can perform with increased visibility of the global flow of data</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row96\" class=\"row_heading level0 row96\" >96</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col0\" class=\"data row96 col0\" >In Pytorch, quantity.backward() computes the gradient of quantity wrt which of the parameters?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col1\" class=\"data row96 col1\" >The argument grad_output is the gradient with respect to the output given by the autograd engine.\n",
              "grad_input is the tensor where the output is the sum of the gradients in each of its uses.\n",
              "In other words, if your matrix element x has positive gradient when first applied and negative when used the second time, it may be that the net effects will cancel out and it will stay as it is (or change just a bit). If both applications call for x to be higher, it will raise more than if it was used just once, etc.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col2\" class=\"data row96 col2\" >Computing quantity requires constructing a 2-sorted graph with nodes being either tensors or differentiable operations on tensors (a so-called computational graph). Under the hood, pytorch keeps track of this graph for you. When you call quantity.backward(), you're asking pytorch to perform an inverse traversal of the graph, from the output to the inputs, using the derivative of each operation encountered rather the operation itself. Leaf tensors that are flagged as requiring gradients accumulate the gradients computed by backward.\n",
              "\n",
              "An optimizer is a different story: it simply implements an optimization strategy on a set of parameters, hence it needs to know which parameters you want it to be optimizing. So quantity.backward() computes gradients, optim.step() uses these gradients to perform on a optimization step, updating the parameters contained in model.\n",
              "\n",
              "As for efficiency, I don't see any argument in favor of specifying parameters in the backward pass (what would the semantics of that be?). If what you'd want is to avoid traversal of parts of the graph in backward mode, pytorch will do it automagically for you if you remember:\n",
              "\n",
              "* you can mark leaf tensors as not requiring grad\n",
              "* a non-leaf tensor -- the output of some operation f(x1,...xN) -- requires grad if at least one of x1...xN requires grad\n",
              "* a tensor that doesn't require grad blocks backward traversal, ensuring no unnecessary computation</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row96_col3\" class=\"data row96 col3\" ><P> When you call backward (either as the function or a method on a tensor) the gradients of operands with requires_grad == True are calculated with respect to the tensor you called backward on. These gradients are accumulated in the .grad property of these operands. If the same operand A appears multiple times in the expression, you can conceptually treat them as separate entities A1, A2... for the backpropagation algorithm and just at the end sum their gradients so that A.grad = A1.grad + A2.grad + ....\n",
              "\n",
              "Now, strictly speaking, the answer to your question\n",
              "\n",
              "\n",
              "  I want to know what happens to middle_linear weight at each backward\n",
              "\n",
              "\n",
              "is: nothing. backward does not change weights, only calculates the gradient. To change the weights you have to do an optimization step, perhaps using one of the optimizers in torch.optim. The weights are then updated according to their .grad property, so if your operand was used multiple times, it will be updated accordingly to the sum of the gradients in each of its uses.\n",
              "\n",
              "In other words, if your matrix element x has positive gradient when first applied and negative when used the second time, it may be that the net effects will cancel out and it will stay as it is (or change just a bit). If both applications call for x to be higher, it will raise more than if it was used just once, etc.\n",
              " <P> net.zero_grad() sets the gradients of all its parameters (including parameters of submodules) to zero. If you call optim.zero_grad() that will do the same, but for all parameters that have been specified to be optimised. If you are using only net.parameters() in your optimiser, e.g. optim = Adam(net.parameters(), lr=1e-3), then both are equivalent, since they contain the exact same parameters.\n",
              "\n",
              "You could have other parameters that are being optimised by the same optimiser, which are not part of net, in which case you would either have to manually set their gradients to zero and therefore keep track of all the parameters, or you can simply call optim.zero_grad() to ensure that all parameters that are being optimised, had their gradients set to zero.\n",
              "\n",
              "\n",
              "  Moreover, what happens if I do both?\n",
              "\n",
              "\n",
              "Nothing, the gradients would just be set to zero again, but since they were already zero, it makes absolutely no difference.\n",
              "\n",
              "\n",
              "  If I do none, then the gradients get accumulated, but what does that exactly mean? do they get added?\n",
              "\n",
              "\n",
              "Yes, they are being added to the existing gradients. In the backward pass the gradients in respect to every parameter are calculated, and then the gradient is added to the parameters' gradient (param.grad). That allows you to have multiple backward passes, that affect the same parameters, which would not be possible if the gradients were overwritten instead of being added.\n",
              "\n",
              "For example, you could accumulate the gradients over multiple batches, if you need bigger batches for training stability but don't have enough memory to increase the batch size. This is trivial to achieve in PyTorch, which is essentially leaving off optim.zero_grad() and delaying optim.step() until you have gathered enough steps, as shown in HuggingFace - Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU  Distributed setups.\n",
              "\n",
              "That flexibility comes at the cost of having to manually set the gradients to zero. Frankly, one line is a very small cost to pay, even though many users won't make use of it and especially beginners might find it confusing.\n",
              " <P> \n",
              "The argument grad_output is the gradient with respect to the output given by the autograd engine.\n",
              "grad_input is the tensor where the result should be written.\n",
              "grad_columns is a temporary buffer given here for efficiency.\n",
              "The backward functions always take grad_output and return grad_input.\n",
              "Thx a lot! Could i regard the grad_input as δi,l defined below? \n",
              "the superscript l reprensents the layer , W is conv2d’s weight kernel and σ(z) has a universal format like\n",
              "\n",
              "Yes, if you’re in the backward of the layer l,\n",
              "grad_input = di,l\n",
              "grad_output = di,l+1\n",
              "I am confused that why     gradColumns = weights’ * gradOutput_n ?\n",
              "Could any help be provided? <P> The condition to the if statement relies on the value of x.sum(), which relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by binding concrete values to arguments during symbolic tracing: In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example: The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API: <P> The condition to the if statement relies on the value of x.sum(), which relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing. <P> As the torch.autograd.grad mentioned, torch.autograd.grad computes and returns the sum of gradients of outputs w.r.t. the inputs. Since your c and d are not scalar values, grad_outputs are required.\n",
              "\n",
              "import torch\n",
              "\n",
              "a = torch.rand(2,requires_grad=True)\n",
              "b = torch.rand(2, requires_grad=True)\n",
              "\n",
              "a\n",
              "# tensor([0.2308, 0.2388], requires_grad=True)\n",
              "\n",
              "b\n",
              "# tensor([0.6314, 0.7867], requires_grad=True)\n",
              "\n",
              "c = a*a + b*b\n",
              "d = 2*a+4*b\n",
              "\n",
              "torch.autograd.grad([c,d], inputs=[a,b], grad_outputs=[torch.Tensor([1.,1.]), torch.Tensor([1.,1.])])\n",
              "# (tensor([2.4616, 2.4776]), tensor([5.2628, 5.5734]))\n",
              "\n",
              "\n",
              "Explanation:\n",
              "dc/da = 2*a = [0.2308*2, 0.2388*2]\n",
              "dd/da = [2.,2.]\n",
              "So the first output is dc/da*grad_outputs[0]+dd/da*grad_outputs[1] = [2.4616, 2.4776]. Same calculation for the second output. \n",
              "\n",
              "If you just want to get the gradient of c and d w.r.t. the inputs, probably you can do this:\n",
              "\n",
              "a = torch.rand(2,requires_grad=True)\n",
              "b = torch.rand(2, requires_grad=True)\n",
              "\n",
              "a\n",
              "# tensor([0.9566, 0.6066], requires_grad=True)\n",
              "b\n",
              "# tensor([0.5248, 0.4833], requires_grad=True)\n",
              "\n",
              "c = a*a + b*b\n",
              "d = 2*a+4*b\n",
              "\n",
              "[torch.autograd.grad(t, inputs=[a,b], grad_outputs=[torch.Tensor([1.,1.])]) for t in [c,d]]\n",
              "# [(tensor([1.9133, 1.2132]), tensor([1.0496, 0.9666])),\n",
              "# (tensor([2., 2.]), tensor([4., 4.]))]\n",
              "\n",
              " <P> The parameters() only gives the module parameters i.e. weights and biases.\n",
              "\n",
              "\n",
              "  Returns an iterator over module parameters.\n",
              "\n",
              "\n",
              "You can check the list of the parameters as follows:\n",
              "\n",
              "for name, param in model.named_parameters():\n",
              "    if param.requires_grad:\n",
              "        print(name)\n",
              "\n",
              "\n",
              "On the other hand, state_dict  returns a dictionary containing a whole state of the module. Check its source code that contains not just the call to parameters but also buffers, etc.\n",
              "\n",
              "\n",
              "  Both parameters and persistent buffers (e.g. running averages) are included. Keys are the corresponding parameter and buffer names.\n",
              "\n",
              "\n",
              "Check all keys that state_dict contains using:\n",
              "\n",
              "model.state_dict().keys()\n",
              "\n",
              "\n",
              "For example, in state_dict, you'll find entries like bn1.running_mean and running_var, which are not present in .parameters().\n",
              "\n",
              "\n",
              "\n",
              "If you only want to access parameters, you can simply use .parameters(), while for purposes like saving and loading model as in transfer learning, you'll need to save state_dict not just parameters.\n",
              " <P> When you call loss.backward(), all it does is compute gradient of loss w.r.t all the parameters in loss that have requires_grad = True and store them in parameter.grad attribute for every parameter.\n",
              "\n",
              "optimizer.step() updates all the parameters based on parameter.grad\n",
              " <P> See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed. torch.autograd.grad() is not supported. functions – A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially. segments – Number of chunks to create in the model input – A Tensor that is input to functions <P> y.backward() computes dy/dz where z are all the leaf nodes in the computation graph. And it stores dy/dz in z.grad.\n",
              "\n",
              "For example: In the above case, leaf nodes are x.\n",
              "\n",
              "y.backward() works when y is a scalar which is the case for most of the deep-learning. When y is a vector you have to pass another vector (v in the above case). You can see this as computing d(v^Ty)/dx. \n",
              "\n",
              "\n",
              "\n",
              "To answer how we got x.grad note that you raise x by the power of 2 unless norm exceeds 1000, so x.grad will be v*k*x**(k-1) where k is 2**i and i is the number of times the loop was executed. \n",
              "\n",
              "To have a less complicated example, consider this:\n",
              "\n",
              "\n",
              "x = torch.randn(3,requires_grad=True)                                         \n",
              "print(x)                                                                            \n",
              "Out: tensor([-0.0952, -0.4544, -0.7430], requires_grad=True)\n",
              "\n",
              "y = x**2   \n",
              "v = torch.tensor([1.0,0.1,0.01])                                                                   \n",
              "y.backward(v) \n",
              "\n",
              "print(x.grad)                                                                        \n",
              "Out[15]: tensor([-0.1903, -0.0909, -0.0149])\n",
              "\n",
              "print(2*v*x)                                              \n",
              "Out: tensor([-0.1903, -0.0909, -0.0149], grad_fn=MulBackward0)\n",
              "\n",
              "\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row97\" class=\"row_heading level0 row97\" >97</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col0\" class=\"data row97 col0\" >What does the stashing logic save and restore for the current device?</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col1\" class=\"data row97 col1\" >the stashing logic save and restore for the current device. This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won’t be accurate. Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.)</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col2\" class=\"data row97 col2\" >the RNG state</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row97_col3\" class=\"data row97 col3\" ><P> Double-backward <P> bumping priority based on user activity <P> friday <P> SGTM <P> Note <P> cc malfet <P> trace-based means that it operates by executing your model once, and exporting the operators which were actually run during this run.  This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won’t be accurate.  Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.)  We recommend examining the model trace and making sure the traced operators look reasonable.  If your model contains control flows like for loops and if conditions, trace-based exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run.  If you want to export your model with dynamic control flows, you will need to use the script-based exporter. <P> On it <P> Will take a look <P> The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which were actually run during this run.  This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won’t be accurate.  Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.)  We recommend examining the model trace and making sure the traced operators look reasonable.  If your model contains control flows like for loops and if conditions, trace-based exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run.  If you want to export your model with dynamic control flows, you will need to use the script-based exporter.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row98\" class=\"row_heading level0 row98\" >98</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col0\" class=\"data row98 col0\" >Computational graph vs (computer algebra) symbolic expression</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col1\" class=\"data row98 col1\" >There is currently no way to propagate gradients from Tensorflow to PyTorch or vice-versa. Maybe in the future there will be some kind of massive update to both frameworks that lets them inter-operate, but I doubt that. It's best to use them both separately.\n",
              "So, in short, you can't convert placeholder tensors between two frameworks. You have to stick to one of the libraries or use concrete tensors + numpy mediator to communicate in-between frameworks.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col2\" class=\"data row98 col2\" >This is a nice question, which gets at some fundamental differences in AD and also some fundamental design differences between big ML libraries like PyTorch and TensorFlow. In particular, I think understanding the difference between define-by-run and define-and-run AD is confusing and takes some time to appreciate.\n",
              "\n",
              "Backpropagation versus Reverse-Mode AD?\n",
              "\n",
              "You can see a stack overflow question here, and my answer to it. Basically, the difference is whether you want the gradient of a scalar-valued function R^n -> R or the vector-Jacobian product of a vector-valued function R^n -> R^m. Backpropagation assumes you want the gradient of a scalar loss function, and is a term most commonly used in the machine learning community to talk about neural network training.\n",
              "\n",
              "Reverse-mode AD is therefore more general than backpropagation.\n",
              "\n",
              "How is symbolic differentiation different from AD?\n",
              "\n",
              "Symbolic differentiation acts on symbols which represent inputs, while AD computes a numerical value of the derivative for a given input.\n",
              "\n",
              "For example: suppose I have the function y = x^2. If I were to compute the symbolic derivative of y, I would get the value 2x as the symbolic derivative. Now, for any value of x, I immediate know the value of the derivative at that x. But if I were to perform automatic differentiation, I would first set the value of x, say x=5, my AD tool would tell me the derivative is 2*5, but it wouldn't know anything about the derivative at x=4 since it only computed the derivative at x=5, not a symbolic expression for the derivative.\n",
              "\n",
              "Difference between define-and-run / static computational graph and define-by-run / dynamic computational graph?\n",
              "\n",
              "As you point out, TF1 and Theano are define-and-run, while Pytorch, Autograd, and TF2 are define-by-run. What is the difference?\n",
              "\n",
              "In TensorFlow 1, you told TensorFlow what you were going to do, and then TensorFlow prepared to perform those computations on some data by building the static computational graph, and then finally you received the data and performed the calculations. So step 1 was telling TensorFlow what you were going to do, and step 2 was performing that calculation once TensorFlow got some data.\n",
              "\n",
              "In Autograd, you don't tell it what you are going to do before you do it. Autograd, unlike TF1, finds out what you are going to do to your data after it receives the data. If it receives a vector, it has no idea what computations are going to be performed on the vector, because it has no static computational graph ahead of time. It \"builds the graph\" by recording operations on each variable as the code executes, and then at the end of your computation you have a list of the operations which were performed which you can traverse backwards. This allows you to easily include control flow like if statements. Handling control flow in a define-and-run framework is much more difficult.\n",
              "\n",
              "Why does Theano and TF1 not provide general purpose reverse-mode AD?\n",
              "\n",
              "Theano and TF1 do not provide general-purpose AD because they don't allow for control flow. Actually, TF1 did, but it was a mess.\n",
              "\n",
              "Difference between differentiable programming and computational graph?\n",
              "\n",
              "From Wikipedia:\n",
              "\n",
              "\"Differentiable programming is a programming paradigm in which the programs can be differentiated throughout, usually via automatic differentiation.\"\n",
              "\n",
              "So differentiable programming is a paradigm for designing programs. A computational graph, on the other hand, is an abstraction used in the AD field for understanding the computations performed by a differentiable computer program. One is a programming paradigm, one is a programming abstraction.</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row98_col3\" class=\"data row98 col3\" ><P> PyTorch-Geometric treats all the graphs in a batch as a single huge graph, with the individual graphs disconnected from each other. The node indices correspond to nodes in this big graph. This means there is no need for a batch dimension in x or edge_index.\n",
              " <P> Operations you do to Tensorflow tensors are \"remembered\" in order to calculate and back-propagate gradients. Same is true for PyTorch tensors. All this is ultimately required to train the model in both frameworks. This also is the reason why you can't convert tensors between the two frameworks: They have different ops and gradient calculation systems. They are incapable of capturing any operation that happens beyond their framework. For example, you can't (as of Jan 2021) have python for loops in custom loss functions. It has to be implemented into the framework in order to work. Similarly, there is no implementation of converting pytorch operations to Tensorflow operations.\n",
              "This answer shows how it's done when your tensor is well-defined (not a placeholder). But there is currently no way to propagate gradients from Tensorflow to PyTorch or vice-versa. Maybe in the future there will be some kind of massive update to both frameworks that lets them inter-operate, but I doubt that. It's best to use them both separately.\n",
              "So, in short, you can't convert placeholder tensors between two frameworks. You have to stick to one of the libraries or use concrete tensors + numpy mediator to communicate in-between frameworks.\n",
              " <P> It can't be implemented in the static graph mode of TensorFlow without significant tradeoffs because the topology of the neural networks in the population changes. Static graphs are suited for models whose architecture doesn't change during training. However, it can be done in TensorFlow Eager or PyTorch because they support dynamic computation graphs. \n",
              "\n",
              "Check this implementation in TensorFlow Eager: https://github.com/crisbodnar/TensorFlow-NEAT\n",
              " <P> As far as I understand, you are essentially asking if this operation can be vectorized. The answer is no, at least not fully, because svd implementation in PyTorch is not vectorized.\n",
              "\n",
              "If you showed the tensorflow implementation, it would help in understanding your starting point. I don't know what you mean by finding the rotation matrix of the vertex, but I would guess this can be vectorized. This would mean that svd is the only non-vectorized operation and you could perhaps get away with writing just a single custom OP, that is the vectorized svd - which is likely quite easy, because it would amount to calling some library routines in a loop in C++.\n",
              "\n",
              "Two possible sources of problems I see are\n",
              "\n",
              "\n",
              "if the neighborhoods of N(i) in equation 7 can be of significantly different sizes (which would mean that the covariance matrices are of different sizes and vectorization would require some dirty tricks)\n",
              "the general problem of dealing with meshes and neighborhoods could be difficult. This is an innate property of irregular meshes, but PyTorch has support for sparse matrices and a dedicated package torch_geometry, which at least helps.\n",
              "\n",
              " <P> When you do m(input), the __call__ (what is __call__?) method is called, which internally calls forward method and does other stuff. This logic is written in the base class: nn.Module. For simplicity, assume for now, that doing m(input) is equivalent to m.forward(input).\n",
              "And what's the input to forward? A tensor.\n",
              "def forward(self, input: Tensor) - Tensor\n",
              "\n",
              " <P> I think it is quite simple or I didn't get your query correctly. \n",
              "\n",
              "x, t are your input variables.\n",
              "\n",
              "Now let us define a network M that will take input t and output theta.\n",
              "\n",
              "M = nn.Sequential(....) # declare network here\n",
              "\n",
              "\n",
              "Next, we define a network Y. This here might be tricky as you want to use theta as parameters. It might be easier and intuitive to work with functional counterparts of the modules declared in nn (see https://pytorch.org/docs/stable/nn.functional.html). I will try to give an example of this assuming theta are params of a linear module. \n",
              "\n",
              "class Y(nn.Module):\n",
              "    def __init__(self):\n",
              "        # declare any modules here\n",
              "\n",
              "    def forward(self, theta, x):\n",
              "        return nn.functional.linear(input=x, weight=theta, bias=None)\n",
              "\n",
              "\n",
              "The overall forward pass would be \n",
              "\n",
              "def forward(t, x, M, Y):\n",
              "    theta = M(t)\n",
              "    output = Y(theta, x)\n",
              "    return output\n",
              "\n",
              " <P> The tensorflow equivalent would be tf.stop_gradient\n",
              "Also don't forget, that Keras does not compute gradients when using predict (or just calling the model via __call__).\n",
              " <P> Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation. <P> What you want is possible, although I'm not sure what you exactly want to do with this. Basically you seem to want to include the \"index\" in the intermediate tensor for some purpose, and again discard it when passing to the next layer.\n",
              "class Model(nn.Module):\n",
              "    def __init__(self):\n",
              "        super(Model, self).__init__()\n",
              "        self.fc1 = nn.Linear(1, 5)\n",
              "        self.fc2 = nn.Linear(5, 10)\n",
              "        self.fc3 = nn.Linear(10, 1)\n",
              "\n",
              "    def forward(self, x):\n",
              "        x = self.fc1(x)\n",
              "        idx_tensor = (torch.arange(x.shape[1]) + 1).unsqueeze(0).repeat_interleave(repeats=x.shape[0], dim=0)\n",
              "        x = torch.cat([x.unsqueeze(2), idx_tensor.unsqueeze(2)], dim=2)\n",
              "        print(x)\n",
              "        \n",
              "        x = torch.relu(x[:, :, 0])    \n",
              "        x = torch.relu(self.fc2(x))\n",
              "        x = self.fc3(x)\n",
              "        return x\n",
              "\n",
              "Running this gives:\n",
              "net = Model()\n",
              "\n",
              "opt = optim.Adam(net.parameters())\n",
              "features = torch.rand((3,1))\n",
              "net(features)\n",
              "\n",
              "tensor([[[ 0.0817,  1.0000],\n",
              "         [ 0.8084,  2.0000],\n",
              "         [ 1.6118,  3.0000],\n",
              "         [ 0.8658,  4.0000],\n",
              "         [-0.1583,  5.0000]],\n",
              "\n",
              "        [[ 0.2881,  1.0000],\n",
              "         [ 0.6946,  2.0000],\n",
              "         [ 1.3760,  3.0000],\n",
              "         [ 0.6098,  4.0000],\n",
              "         [-0.1240,  5.0000]],\n",
              "\n",
              "        [[ 0.1919,  1.0000],\n",
              "         [ 0.7476,  2.0000],\n",
              "         [ 1.4859,  3.0000],\n",
              "         [ 0.7291,  4.0000],\n",
              "         [-0.1400,  5.0000]]], grad_fn=CatBackward)\n",
              "\n",
              "tensor([[-0.2841],\n",
              "        [-0.2191],\n",
              "        [-0.2495]], grad_fn=AddmmBackward)\n",
              "\n",
              "Note that a typical tensor cannot be both integer and float at the same time, so the 1, 2, 3 will be stored as float 1.000.., 2.000... etc.\n",
              "I suggest if your purpose is something like a fancy printing, then maybe look into torch's hook functions? For example, you can do something like:\n",
              "import pandas as pd\n",
              "\n",
              "def fc_hook_fn(module, input, output):\n",
              "    print(\"\\n\" + \"#\" * 60)\n",
              "    print(f\"In layer {module}\")\n",
              "    print(\"#\" * 60 + \"\\n\")\n",
              "    cols = [f\"Neuron-{i + 1}\" for i in range(output.shape[1])]\n",
              "    idx = [f\"Input-{i + 1}\" for i in range(output.shape[0])]\n",
              "    neuron_activations = pd.DataFrame(output.detach().numpy(), columns=cols, index=idx)\n",
              "    print(neuron_activations)\n",
              "\n",
              "net.fc.register_forward_hook(fc_hook_fn)\n",
              "\n",
              "Now each time something passes through fc1, the function above will be triggered. You don't need to put your print(x) in the forward method.\n",
              "\n",
              "############################################################\n",
              "In layer Linear(in_features=1, out_features=5, bias=True)\n",
              "############################################################\n",
              "\n",
              "         Neuron-1  Neuron-2  Neuron-3  Neuron-4  Neuron-5\n",
              "Input-1 -0.948735 -0.901034 -0.290353 -0.082616 -0.405337\n",
              "Input-2 -0.725904 -0.801648 -0.302922 -0.045514 -0.580485\n",
              "Input-3 -0.829738 -0.847960 -0.297065 -0.062802 -0.498870\n",
              "\n",
              " <P> Generally speaking, in python, when \"calling\" an object, you are invoking its __call__ method. That is,\n",
              "  self(x)\n",
              "\n",
              "is equivalent to\n",
              "  self.__call__(x)\n",
              "\n",
              "For pytorch nn.Module (and all derivative classes) __call__ wraps around the module's forward function, therefore, from your perspective self(x) is basically forwarding x through the module self.\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002level0_row99\" class=\"row_heading level0 row99\" >99</th>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col0\" class=\"data row99 col0\" >PyTorch 1.5.0 CUDA 10.2 installation via pip always installs CUDA 9.2</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col1\" class=\"data row99 col1\" >upgrade your NVIDIA driver to latest, and that will likely fix the issue. CUDAToolkit 10.1 requires much newer driver than 10.0 <P> @mszhanyi I think we should just say that we support cuda >= 10. 1 and vs >= 2019.0. Do you already have the CUDA90 feature installed, for some reason...\n",
              "Try first doing:\n",
              "\n",
              "`conda uninstall cuda90`</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col2\" class=\"data row99 col2\" >You can always manually download and install the .whl files:\n",
              "\n",
              "PyTorch 1.5.0 (cu102, py3.6, linux)\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "TorchVision 0.6.0 (cu102, py3.6, linux)\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "You can check these links on  Scroll down to cu102/ section. You'll find other versions there, if you need.\n",
              "</td>\n",
              "                        <td id=\"T_2bfea3ac_1910_11ec_8326_0242ac1c0002row99_col3\" class=\"data row99 col3\" ><P> @darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it. <P> Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`. <P> @LLNLanLeN upgrade your NVIDIA driver to latest, and that will likely fix the issue. cudatoolkit 10.1 requires much newer driver than 10.0 <P> @mszhanyi I think we should just say that we support cuda >= 10.1 and vs >= 2019. <P> do you already have the cuda90 feature installed, for some reason...\n",
              "\n",
              "Try first doing:\n",
              "\n",
              "`conda uninstall cuda90` <P> I downloaded the nighly build instead using CUDA 10.2 and that worked fine. The nightly build with CUDA 11.1 had the same problem that all CUDA based tests failed. <P> Does this failure reproduces if PyTorch-1.7 or later is used? (Please note, that CUDA-11 was not yet available when PyTorch-1.3 was released) <P> closing this issue, because PyTorch now works and ships with CUDA10. <P> I noticed that I had installed a potentially incorrect version of 'magma-cuda'.\n",
              " \n",
              " \n",
              " \n",
              " I'd installed magma-cuda92 rather than magma-cuda90.\n",
              " \n",
              " \n",
              " \n",
              " Might be worth checking.\n",
              " \n",
              " \n",
              " \n",
              " @fwillo This appears to have resolved this for me! <P> Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me.</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb5f31bb7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjeszl_Cnsq",
        "outputId": "946e17e9-6c67-4cf4-964d-8f384db53129"
      },
      "source": [
        "predicted = []\n",
        "reference = []\n",
        "\n",
        "# Generate answers for the full test set\n",
        "for i in range(len(test)):\n",
        "    # create support document with the dense index\n",
        "    question = test[i]['x']\n",
        "\n",
        "    doc, res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer,\n",
        "        passage_snippets, doc_gpu_index, device='cuda'\n",
        "    )\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=96,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    predicted += [answer]\n",
        "    reference += [test[i]['y']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rB_gYtFLMag"
      },
      "source": [
        "## Generator model evaluation \n",
        "\n",
        "The last thing we'll do is see how we can get a quantitative evaluation of the model performance. Here, we'll use the ROUGE implementation provided in the nlp library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60xEtU5NHCaq",
        "outputId": "890a7a66-0e81-4556-cd13-517b39b3e06e"
      },
      "source": [
        "pip install rouge_score rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "RuQazT_ADNq7",
        "outputId": "f4f46ff3-ad3c-43d8-a56c-a013b019d2bb"
      },
      "source": [
        "# Compare each generation to the  answer from the dataset\n",
        "nlp_rouge = nlp.load_metric('rouge')\n",
        "\n",
        "scores = nlp_rouge.compute(\n",
        "    predicted, reference,\n",
        "    rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
        "    use_agregator=True, use_stemmer=False\n",
        ")\n",
        "df = pd.DataFrame({\n",
        "    'rouge1': [scores['rouge1'].mid.precision, scores['rouge1'].mid.recall, scores['rouge1'].mid.fmeasure],\n",
        "    'rouge2': [scores['rouge2'].mid.precision, scores['rouge2'].mid.recall, scores['rouge2'].mid.fmeasure],\n",
        "    'rougeL': [scores['rougeL'].mid.precision, scores['rougeL'].mid.recall, scores['rougeL'].mid.fmeasure],\n",
        "}, index=[ 'P', 'R', 'F'])\n",
        "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rouge1</th>        <th class=\"col_heading level0 col1\" >rouge2</th>        <th class=\"col_heading level0 col2\" >rougeL</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >P</th>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.1382</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.0231</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.0868</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >R</th>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.2680</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.0591</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.2083</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >F</th>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.1313</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.0226</td>\n",
              "                        <td id=\"T_40e5fc78_1921_11ec_8326_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.0864</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb6e581c650>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}